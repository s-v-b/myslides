---
title: "Product spaces and product distributions"
subtitle: "⚔<br/>Probabilités Master I, MIDS"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/11/05 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["header-footer.css", "default", "rutgers-fonts", "hygge"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: layout-general
layout: true
class: left, middle
background-image: url('./img/Universite_Paris_logo_horizontal.jpg')
background-size: 10%
background-position: 97% 3%

<style>
.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: red;
}
</style>

---
class: middle, center, inverse
background-size: 4%
background-position: 97% 3%



# Product distributions

### `r Sys.Date()`

#### Probabilités Master I MIDS


[<img src="img/Universite_Paris_logo_horizontal.jpg" align="left" width="300px" style="vertical-align:bottom">](https://www.u-paris.fr)

---
class: inverse, center, middle

## Motivation

## `r fontawesome::fa("grin", fill = "white")`


---

### `r fontawesome::fa("chart-line")`

Description of _random walks_ over $\mathbb{Z}^d$ :

> at each step, we chose a random neighbour of the current position and  move to that neighbour.

--

An elementary move is an element of $\{0, \pm 1\}^d$ where one component is non-zero

--

Picking an elementary move uniformly at random is easy

--

Picking finitely many independent moves is easy too

--

Enough to have an .red[infinite] supply of independent move-valued random variables

---

### `r fontawesome::fa("tools")`

We need a building device that allows us to glue probability spaces together
so as to get stochastically independent components in the resulting probability space

We need tools to perform computations in the resulting probability space

---
class: center, middle, inverse

## Product of two probability spaces

---

## Product $\sigma$-algebra

Given: two measured spaces

$$(\mathcal{X}, \mathcal{F}, \mu) \quad \text{ and } \quad (\mathcal{Y}, \mathcal{G}, \nu)$$

### Goal

Build a measure space $(\mathcal{X}\times \mathcal{Y}, \mathcal{H}, \rho)$
and two measurable functions $X : \mathcal{X}\times \mathcal{Y} \to \mathcal{X}$
and  $Y : \mathcal{X}\times \mathcal{Y} \to \mathcal{Y}$ with

- $\mu = \rho \circ X^{-1}$ and $\nu = \rho \circ Y^{-1}$

- $\rho(A \times B) =  \mu(A) \times \nu(B) \qquad \forall A \in \mathcal{F}, B\in \mathcal{G} \,.$

--

`r fontawesome::fa("tools")` We have to define a convenient $\sigma$-algebra $\mathcal{H}$ of subsets
of $\mathcal{X} \times \mathcal{Y}$: the _product $\sigma$-algebra_

---

.content-box-gray[
### Definition: Product sigma-algebra

Let $(\mathcal{X}, \mathcal{F})$  and $(\mathcal{Y}, \mathcal{G})$ be two measurable spaces.

The product $\sigma$-algebra $\mathcal{F} \otimes \mathcal{G}$ is the $\sigma$-algebra
of subsets of $2^{\mathcal{X} \times \mathcal{Y}}$ that is generated by the so-called _rectangles_:

$$\Big\{ A \times B : A \in \mathcal{F}, B \in \mathcal{G}\Big\} \, .$$
]

--

`r fontawesome::fa("exclamation")` The product $\sigma$-algebra makes the functions $X$ and $Y$ (sometimes called _coordinate projections_) measurable.


---

#### Example

If $\mathcal{F} = \mathcal{G}  =\mathcal{B}(\mathbb{R})$

$$\mathcal{F} \otimes \mathcal{G} = \sigma\left(A \times B : A, B \in \mathcal{B}(\mathbb{R})\right)$$

--

$$\mathcal{B}(\mathbb{R}) \otimes \mathcal{B}(\mathbb{R}) = \mathcal{B}\left(\mathbb{R}^2 \right)$$


---

### More generally

If $\mathcal{F} = \sigma(\mathcal{A})$ (resp. $\mathcal{G} = \sigma(\mathcal{B})$ )
with $\mathcal{A}$  (resp. $\mathcal{B}$ ) a $\pi$ -class

Then
$$\mathcal{F} \otimes \mathcal{G} = \sigma\left(\mathcal{A}\right) \otimes \sigma\left(\mathcal{B}\right) = \sigma\left(\mathcal{A} \times \mathcal{B}\right)$$


---

### Recall `r emo::ji("syringe")`

.content-box-gray[
### Definition:

A measure $\mu$ on $(\Omega, \mathcal{F})$ is $\sigma$-finite iff

there exists $(A_n)_n$ with

- $\Omega \subseteq \cup_n A_n$

- $\mu(A_n) < \infty$ for each $n$.

]

--

- Finite measures (this encompasses  probability measures) are $\sigma$-finite `r fontawesome::fa("smile")`

- Lebesgue measure is $\sigma$-finite `r fontawesome::fa("smile")`

- The counting measure on $\mathbb{R}$ is __not__ $\sigma$-finite `r fontawesome::fa("frown", fill="gray")`

---

.content-box-gray[

### Product-measure Theorem

Let $(\mathcal{X}, \mathcal{F}, \mu)$ and $(\mathcal{Y}, \mathcal{G}, \nu)$ be
two measured spaces where $\mu,\nu$ are $\sigma$-finite.

Then there exists a .red[unique]
$\sigma$-finite measure $\alpha$ on $\mathcal{X} \times \mathcal{Y}$ endowed with the product $\sigma$-algebra
$\mathcal{F} \otimes \mathcal{G} = \sigma(\mathcal{F} \times \mathcal{G})$ that satisfies

$$\alpha (A \times B) = \mu(A) \times \nu(B)\qquad \forall A \in \mathcal{F}, B \in \mathcal{G} \, .$$

... (to be continued)
]

---

.content-box-gray[

### Theorem  (continued)

Moreover, for all $E \in \mathcal{F} \otimes \mathcal{G}$,

1. for each $x \in \mathcal{X}$, $y \mapsto \mathbb{I}_E(x,y)$ is $\mathcal{G}$-measurable;

1. $x \mapsto \int_{\mathcal{Y}} \mathbb{I}_E(x,y) \, \mathrm{d} \nu(y)$
is $\mathcal{F}$-measurable;

1. for each $y \in \mathcal{Y}$, $x \mapsto \mathbb{I}_E(x,y)$ is $\mathcal{F}$-measurable;

1. $y \mapsto \int_{\mathcal{X}}  \mathbb{I}_E(x,y) \,  \mathrm{d}\mu(x)$
is $\mathcal{G}$-measurable,

and the following holds:

$$\int_{\mathcal{X}\times \mathcal{Y}} \mathbb{I}_E \, \mathrm{d}\alpha
= \int_{\mathcal{X}} \Big(\int_{\mathcal{Y}} \mathbb{I}_E(x,y) \, \mathrm{d} \nu(y)\Big) \, \mathrm{d}\mu(x)
= \int_{\mathcal{Y}} \Big( \int_{\mathcal{X}}  \mathbb{I}_E(x,y) \,  \mathrm{d}\mu(x)\Big) \,\mathrm{d} \nu(y)$$

where the three integrals are either finite or infinite.

]

.blue[
Measure $α$ is called a _product measure_, denoted by $\mu \otimes \nu$.
]


???


---



### `r fontawesome::fa("skull-crossbones")`

### Assuming that both $μ$ and $ν$ are .red[ $σ$ -finite] is essential!

--

Choose

- $\mu$ as the counting measure on $[0,1]$ and

- $\nu$ as the Lebesgue measure on $[0,1]$.


Consider the diagonal $E = \{(x,x) : x \in [0,1]\}$.

--

The set $E$ belongs to $\mathcal{B}(\mathbb{R}) \otimes \mathcal{B}(\mathbb{R}) = \mathcal{B}(\mathbb{R}^2)$

$$μ ⊗ ν(E) = ???$$

---

### `r fontawesome::fa("skull-crossbones")`

.content-box-red[

Interchanging the order of integration leads to different results:

\begin{align*}
1      & = \int_{[0,1]} \Big(\underbrace{\int_{[0,1]} \mathbb{I}_E (x,y) \, \mathrm{d}\mu(x)}_{=1}\Big) \, \mathrm{d}\nu(y) \\
0      & = \int_{[0,1]} \Big(\underbrace{\int_{[0,1]} \mathbb{I}_E (x,y) \, \mathrm{d}\nu(y)}_{=0}\Big) \, \mathrm{d}\mu(x)
\end{align*}

]

---

#### The product-measure theorem contains three statements:

- existence of a  measure
over $(\mathcal{X} \times \mathcal{Y}, \mathcal{F} \otimes \mathcal{G})$ that
satisfies the product property over rectangles

- uniqueness of this measure

- the possibility of computing the measure of  $E \in \mathcal{F} \otimes \mathcal{G}$
by iterated integration in arbitrary order.


---



- The first statement is proved using an extension theorem

--

- The second statement follows from a monotone class argument (rectangles form a generating $\pi$-class)
  - the case where both $\mu$ and $\nu$ are finite measure is settled
  - If either $\mu$ or $\nu$ is just $\sigma$-finite, consider restrictions to rectangles with finite measure, and
proceed by approximation.

--

- The third statement trivially holds for rectangles.


---


---
class: center, middle, inverse

## Tonelli-Fubini theorem

---

`r fontawesome::fa("exclamation")` We consider product measures that are built from $\sigma$-finite measures

--

> The Tonelli-Fubini Theorem shows that (under mild conditions) integration with respect to a product measure reduces to iterated integration over the component measures

---

.content-box-gray[
### Theorem Tonelli-Fubini

Let  $( \mathcal{X}, \mathcal{A})$ and $(\mathcal{Y}, \mathcal{B})$ ne two measurable spaces,  $\mu$ and $\nu$ two
$\sigma$-finite measures on these spaces, $\mu \otimes \nu$ the product measure,
and $f$ a  $\mathcal{A} \otimes \mathcal{B}$-measurable real function
such as $\int |f| \mathrm{d} \mu \otimes \nu < 0$.

The following properties are satisfied:

i. $\forall x \in \mathcal{X}, \hspace{1em} y \mapsto f (x, y)$ is
$\mathcal{B}$-measurable.

i. The function  $x \mapsto \int_{\mathcal{Y}} f (x, y) \mathrm{d}\nu(y)$ is $\mathcal{A}$-measurable, finite $\mu$- almost everywhere and
$$\int_{\mathcal{X} \times \mathcal{Y}} f \mathrm{d} \mu \otimes \nu
= \int_{\mathcal{X}} \left[ \int_{\mathcal{Y}} f (x, y) \mathrm{d} \nu (y)
\right]  \mathrm{d} \mu (x)$$


]


---

#### Proof



---

#### A simple consequence of the Tonelli-Fubini Theorem.


.content-box-gray[
### Proposition "IPP formula"

Let $X$ be a non-negative real-valued random variable, then

$$\mathbb{E}X = \int_0^\infty  P\{ X > t \} \mathrm{d}t$$


]

---

#### Proof

\begin{align*}
\mathbb{E}X
  & = \int_{\Omega}  X(\omega) \, \mathrm{d}P(\omega) \\
  & = \int_{\Omega}  \Big( \int_{[0,\infty)} \mathbb{I}_{X(\omega)> t} \mathrm{d}t \Big)\, \mathrm{d}P(\omega) \\
  & =  \int_{[0,\infty)}  \Big( \int_{\Omega} \mathbb{I}_{X(\omega)> t} \,  \mathrm{d}P(\omega) \Big) \mathrm{d}t \\
  & =  \int_{[0,\infty)}  \Big( P\{ \omega : X(\omega) > t \} \Big) \mathrm{d}t
\end{align*}

---
class: center, middle, inverse

## Independence and product distributions

---

### Two random variables

Let the two random variables $X, Y$ map $(\Omega, \mathcal{F})$ to $(\mathcal{X}, \mathcal{G})$
and $(\mathcal{Y}, \mathcal{H})$.

Equip $(\Omega, \mathcal{F})$ with probability distribution $P$.

Let $Q_X = P \circ X^{-1}$ and $Q_Y =  P \circ Y^{-1}$  be the two image distributions (called the
marginal distributions).

Let $Q$  be the joint distribution of $(X,Y)$ under $P$, that is the probability distribution
over $\mathcal{X} \times \mathcal{Y}$ that is uniquely defined by

$$Q( A \times B) = P\Big\{ \omega: X(\omega) \in A, Y(\omega) \in B \Big\}$$

--

.content-box-gray[
Then
$$X \perp\!\!\!\perp Y \text{ under } P \Longleftrightarrow  Q = Q_X \otimes Q_Y \, ,$$


In words,  $X$ and $Y$ are independent iff their joint distribution is the product
of their marginal distributions.
]

---

###  `r emo::ji("syringe")` Independence of finitely many $\sigma$-algebras

Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $\mathcal{G_1}, \ldots, \mathcal{G}_n$
be a  collection of sub- $\sigma$ -algebras

.content-box-gray[

### Definition

This collection is independent with respect to $P$ if

$∀ A_1 ∈ \mathcal{G}_1, …, A_n ∈ \mathcal{G}_n$
$$P (A_1 ∩ … ∩ A_n) = P(A_1) \times \ldots \times P(A_n)$$

]

---

###  Independence of countably many $\sigma$-algebras

In many applications, independence between  two $\sigma$-algebras or a finite collection
of $\sigma$-algebras is not enough.

This is the case when deriving or using laws of large numbers.

We have to deal with a _countable collection of independent random variables_. In words,  we have to work with a countable collection of $\sigma$-algebras and we need to elaborate a notion of a countable collection of independent $\sigma$-algebras.

---

.content-box-gray[

### Definition

Let $(\Omega, \mathcal{F}, P)$ be a probability space. Let $\mathcal{G_1}, \ldots, \mathcal{G}_n, \ldots$
be a countable collection of sub $\sigma$ algebras.

The collection $\mathcal{G_1}, \ldots, \mathcal{G}_n, \ldots$ is said to be independent under $P$
if every finite sub-collection is independent under $P$.

]

---

### Example

Consider the uniform probability distribution over $[0,1]$, define $X_1, X_2, \ldots$ by

$$X_n(\omega) = \operatorname{sign}\Big(\sin\big(2^{n+1} \pi \omega \big)\Big)$$

then $X_1, \ldots, X_n, \ldots$ form a countable independent collection of random variables.


---
class: center, middle, inverse

## Infinite product measures

---

.content-box-gray[

### Definition Cylindrical $\sigma$-algebra


Let $(\Omega_n, \mathcal{F}_n)_n$ be a countable collection of measurable spaces, the cylinder $\sigma$-algebra
is the $\sigma$-algebra of subsets of $\prod_{n=1}^\infty \Omega_n$  that is generated by subsets of the form:

$$\prod_{n=1}^m A_n \times \prod_{n=m+1}^\infty \Omega_n \qquad\text{with } A_n \in \mathcal{F}_n \text{ for } n \leq m$$

where $m$ is any integer.

The subsets are called _finite-dimensional rectangles_ or _cylinders_.

]

`r fontawesome::fa("eye")`:  cylinders form a $\pi$-class


---



If each  $(\Omega_n, \mathcal{F}_n)$ is endowed with a probability distribution, assigning a probability
to cylinders looks straightforward:

$$\mathbb{P} \left( \prod_{n=1}^m A_n \times \prod_{n=m+1}^\infty \Omega_n \right) = \prod_{n=1}^m P_n(A_n) \times \prod_{n=m+1}^\infty P_n(\Omega_n) = \prod_{n=1}^m P_n(A_n)$$

The question is:

> does $\mathbb{P}$ extends to the cylinder $\sigma$-algebra? If an extension exists, is it unique?

The answer is yes! `r emo::ji("champagne")`

---

.content-box-gray[

### Kolmogorov's extension theorem
Let $(\Omega_n, \mathcal{F}_n, P_n)_n$ be a countable collection of probability spaces. Then there exists a unique probability
distribution $\mathbb{P}$ on the cylindrical $\sigma$-algebra that satisfy:

$$\mathbb{P} \left( \prod_{n=1}^m A_n \times \prod_{n=m+1}^\infty \Omega_n \right) =
 \prod_{n=1}^m P_n(A_n)$$

for every finite sequence $A_1, \ldots, A_m$ in $\mathcal{F}_1 \times \ldots \times \mathcal{F}_m$.

]

---
exclude: true
class: center, middle, inverse

## Infinite independent collection of  events


---
exclude: true
class: center, middle, inverse

## Infinite independent collection of  $\sigma$-algebras


---
class: center, middle, inverse

## Second Borel-Cantelli Lemma

---

.content-box-gray[

### Lemma

Let $A_1, …, A_n, …$ be a countable independent collection of events under $(Ω, \mathcal{F}, P)$.

If $∑_n P(A_n) = ∞$ then $P \left(\cap_n \cup_{m\geq n} A_m\right) = 1$



]


This is a partial converse to the first Borel-Cantelli Lemma

---

#### Proof

$$P\left(\overline{\cap_n \cup_{m\geq n} A_m}\right) = P\left(\cup_n \overline{\cup_{m\geq n} A_m}\right) = P\left(\cup_n \cap_{m\geq n} \overline{A_m}\right)$$

For some $n$:

$$P\left(\cap_{m\geq n} \overline{A_m}\right) = P \left(\lim_{k \uparrow \infty} \cap_{n \leq m \leq k} \overline{A_m}\right)
= \lim_{k \uparrow \infty} P \left(\cap_{n \leq m \leq k} \overline{A_m}\right)$$

$$P \left(\cap_{n \leq m \leq k} \overline{A_m}\right) = \prod_{m=n}^k P\left( \overline{A_m} \right)
= \prod_{m=n}^k \left( 1 - P\left( {A_m} \right)\right) \leq \mathrm{e}^{- ∑_{m=n}^k P(A_m)}$$

$$\lim_{k ↑ ∞} \mathrm{e}^{- ∑_{m=n}^k P(A_m)} = 0$$

--

Hence:

$$P\left(\cap_{m\geq n} \overline{A_m}\right) = 0$$

$$P\left(\overline{\cap_n \cup_{m\geq n} A_m}\right)  \leq ∑_n P\left(\cap_{m\geq n} \overline{A_m}\right) =0$$

---
class: center, middle, inverse

## Absolutely continuous distributions

```{r loads, echo=FALSE, message=FALSE, warning=FALSE}
# %%
require(tidyverse)
require(kableExtra)
old_theme <-theme_set(theme_bw(base_size=9, base_family = "Helvetica"))
# %%
```

---

### Densities and absolute continuity


Beyond discrete distributions, the simplest probability distributions are
defined by a density function with respect to a ( $\sigma$ -finite) measure

This encompasses  the distributions of the so-called _continuous random variables_.

--

.content-box-gray[

### Definition Absolute continuity

Let   $\mu, \nu$ be
two $\sigma$-additive measures on measurable space $(\Omega, \mathcal{F})$,

$\mu$ is said to be _absolutely continuous_ with respect to $\nu$ (denoted by
$\mu \trianglelefteq \nu$) iff

$\forall A \in \mathcal{F}$  $\nu(A)=0 \Rightarrow \mu(A)=0$.

]

--


If $\mu, \nu$ are two probability distributions, and  $\mu \trianglelefteq \nu$,
then any event which is impossible under $\nu$ is also impossible under $\mu$.


---


### `r fontawesome::fa("puzzle")`

- Is the counting measure on $\mathbb{R}$ absolutely continuous with respect
to Lebesgue measure?

- Is the converse true?

- Check that absolute continuity is a transitive relationship.

---




### Radon-Nikodym Theorem

Let   $\mu, \nu$ be two $\sigma$ -additive measures on measurable space $(\Omega, \mathcal{F})$

Assume  $\nu$ is $\sigma$ -finite

If $\mu \trianglelefteq \nu$, then  there exists a measurable function $f$ from $\Omega$ to $[0, \infty)$ such that


$$\forall A \in \mathcal{F} \qquad \mu(A) =  \int_A f(\omega) \mathrm{d}\nu(\omega) =  \int_{\Omega} \mathbb{I}_A f \mathrm{d}\nu$$





---

- The function $f$ is called _a  version_ of the density of $\mu$ with respect to $\nu$

- The density is also called the Radon-Nikodym derivative of $\mu$ with respect to $\nu$

- The density is sometimes denoted by $\frac{\mathrm{d}\mu}{\mathrm{d}\nu}$

---

`r fontawesome::fa("skull-crossbones")` The sigma-finiteness assumption is crucial.

If we choose $\mu$ as Lebesgue measure and
$\nu$ as the counting measure, $\nu$ is not $\sigma$-finite, $\mu(A)>0$ implies $\nu(A)=\infty$
which we may consider as larger than $0$. Nevertheless, Lebesgue measure has no density
with respect to the counting measure.

---


.content-box-gray[

###  Chain rule

If $\rho \trianglelefteq \mu \trianglelefteq  \nu$ , $f$ is a density of $\rho$ with respect to $\mu$
while $g$ is a density of $\mu$ with respect to $\nu$, then $fg$ is a density of $\rho$
with respect to $\nu$

]


---

### Exponential distribution

The exponential distribution shows up in several areas of probability
and statistics

- In _reliability theory_, its _memoryless property_
make it a borderline case.

- In the theory of _point processes_, the exponential distribution is connected with _Poisson Point Processes_

- It is also important in _extreme value theory_

---



### Definition

The exponential distribution with intensity parameter $\lambda>0$ is defined by its density with respect
to Lebesgue measure on $[0,\infty)$

$$x \mapsto \lambda \mathrm{e}^{-\lambda x}$$

The reciprocal of the intensity parameter is called the scale parameter.


---


- Geometric and exponential distributions are connected: if $X$
is exponentially distributed, then $\lceil X\rceil$ is geometrically distributed. For $k\geq 1$:

$$P \Big\{ \lceil X \rceil \geq k \Big\} = P \Big\{  X  > k - 1 \Big\}
= \mathrm{e}^{- \lambda (k-1)}$$

- Check that $x \mapsto \lambda \mathrm{e}^{-\lambda x}$ is a density probability
over $[0, \infty)$.

- Compute the tail function and the cumulative distribution function of the exponential distribution
function with parameter $\lambda$.

---


- Let $X_1, \ldots, X_n$ be i.i.d. exponentially distributed. Characterize the
distribution of $\min(X_1, \ldots, X_n)$.


- If $X$ is exponentially distributed  with scale parameter $\sigma$, what is the
distribution of $a X$?

---
exclude: true

### Exponential densities with different parameters: scales $1, 2, 1/2$ or equivalently intensities $1, 1/2, 2$. Expectation equals scale,  variance equals squared scale.


```{r witgetexponential, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="(ref:witgetexponential)"}
# %%

p <- ggplot2::ggplot(data = tibble::tibble(x=seq(0, 5,by=0.01)),
                mapping= ggplot2::aes(x=x)) +
  ggplot2::scale_linetype_manual(values = c("solid", "dashed", "dotted"),
                                 guide = ggplot2::guide_legend(override.aes = list(
                        linetype = c("solid", "dashed", "dotted"),
                        title = "Parameters"))) +
  ggplot2::labs(linetype="Parameters") +
  ggplot2::xlab("x") +
  ggplot2::ylab("density") +
  ggplot2::xlim(xlim = c(0, 5)) +
  ggplot2::stat_function(fun=dexp, args = c(1),
                         ggplot2::aes(linetype="a. standard")) +
  ggplot2::stat_function(fun=dexp, args = c(2),  ggplot2::aes(linetype="b. scale=1/2")) +
  ggplot2::stat_function(fun=dexp, args = c(.5),
                         ggplot2::aes(linetype="c. scale=2"))


p
# %%
```


---
class: inverse, middle, center

## Gamma distributions

---

- Sums of independent exponentially distributed random variables are not exponentially distributed `r fontawesome::fa("frown")`

- The family of _Gamma distributions_ encompasses the family of exponential distributions

- The family of _Gamma distributions_ with the same intensity parameter is stable under addition `r fontawesome::fa("smile")`

---

`r emo::ji("syringe")`  Euler's Gamma function:

$$\Gamma(t) = \int_0^\infty x^{t-1}\mathrm{e}^{-x} \mathrm{d}x \qquad \text{for } t>0$$



### Definition

The Gamma distribution with _shape_ parameter $p>0$ and _intensity_ parameter $\lambda>0$ is defined by its density with respect to Lebesgue measure on $[0,\infty)$:

$$x \mapsto \lambda^p \frac{x^{p-1}}{\Gamma(p)} \mathrm{e}^{-\lambda x}$$

The reciprocal of the intensity parameter is called the _scale_ parameter.



---

### `r fontawesome::fa("puzzle")`

- Check that $x \mapsto \lambda^p \frac{x^{p-1}}{\Gamma(p)} \mathrm{e}^{-\lambda x}$ is a density probability
over $[0, \infty)$.

- If $X$ is Gamma distributed  with shape parameter $p$ and scale parameter $\sigma$, what is the
distribution of $a X$?


---

###  Gamma densities with different parameters:

Scales $1, 1, 1/3, 1, 2$ and Shapes  $1, 2, 3, 5, 5/2$.

Expectation equals shape times scale

Variance equals shape times squared scale

---


```{r witgetgamma, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="(ref:witgetgamma)"}
# %%

p <- ggplot2::ggplot(data = tibble::tibble(x=seq(0, 10, by=0.01)),
                mapping= ggplot2::aes(x=x)) +
  ggplot2::scale_linetype_manual(values = c("solid", "dashed", "dotted", "dotdash", "81"),
                                 guide = ggplot2::guide_legend(override.aes = list(
                        linetype = c("solid", "dashed", "dotted", "dotdash", "81"),
                        title = "Parameters"))) +
  ggplot2::labs(linetype="Parameters") +
  ggplot2::xlab("x") +
  ggplot2::ylab("density") +
  ggplot2::xlim(xlim = c(0, 10)) +
  ggplot2::stat_function(fun=dgamma, args = c(1, 1),
                         ggplot2::aes(linetype="a. standard")) +
  ggplot2::stat_function(fun=dgamma, args = c(2, 1),  ggplot2::aes(linetype="b. shape=2, scale=1")) +
  ggplot2::stat_function(fun=dgamma, args = c(3, 3),
                         ggplot2::aes(linetype="c. shape=3 scale=1/3")) +
  ggplot2::stat_function(fun=dgamma, args = c(5, 1),
                                                ggplot2::aes(linetype="c. shape=5 scale=1")) +
  ggplot2::stat_function(fun=dgamma, args = c(2.5,.5),
                         ggplot2::aes(linetype="d. shape=5/2 scale=2"))

p
# %%
```

---

### Univariate Gaussian distributions

Gaussian distributions play a central role in Probability theory, Statistics, Information theory, and Analysis

.content-box-gray[

### Definition

The Gaussian or normal distribution with mean $\mu \in \mathbb{R}$ and variance $\sigma^2, \sigma>0$ has density

$$x \mapsto \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{- \frac{(x-\mu)^2}{2 \sigma^2}} \qquad\text{for } x \in \mathbb{R}$$

The standard Gaussian density is defined by $\mu=0, \sigma=1$.

]


---

`r fontawesome::fa("puzzle-piece")`

- Check that $x \mapsto \frac{\mathrm{e}^{-x^2/2}}{\sqrt{2\pi}}$ is a probability density over $\mathbb{R}$.

- If $X$ is distributed according to a standard Gaussian density, what is the distribution of $\mu + \sigma X$?

- If $X$ is distributed according to a standard Gaussian density, show that

$$\Pr \{ X > t \} \leq \frac{1}{t} \frac{\mathrm{e}^{-t^2/2}}{\sqrt{2\pi}} \qquad\text{for } t>0\,.$$

---

### Gaussian densities parameters

- The _location parameter_ $\mu$ coincides with the mean and the median.

- The _scale parameter_ $σ$ is the standard deviation

- The Inter-Quartile-Range (IQR) is proportional to the standard deviation.

- If $\Phi^{\leftarrow}$ denotes the quantile function of $\mathcal{N}(0,1)$ then the interquartile range of $\mathcal{N}(\mu, \sigma^2)$ is $\sigma \Big(\Phi^{\leftarrow}(3/4) - \Phi^{\leftarrow}(1/4)\Big)=2 \sigma \Phi^{\leftarrow}(3/4)$.

---

```{r witgetgauss, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="(ref:witgetgauss)"}
# %%

p <- ggplot2::ggplot(data = tibble::tibble(x=seq(-5, 5,by=0.01)),
                mapping=ggplot2::aes(x=x)) +
  ggplot2::scale_linetype_manual(values = c("solid", "dashed", "dotted"),
                                 guide = ggplot2::guide_legend(override.aes = list(
                        linetype = c("solid", "dashed", "dotted"),
                        title = "Parameters"))) +
  ggplot2::labs(linetype="Parameters") +
  ggplot2::xlab("x") +
  ggplot2::ylab("density") +
  ggplot2::xlim(xlim = c(-2, 2)) +
  ggplot2::stat_function(fun=dnorm, args = c(0, 1),
                         ggplot2::aes(linetype="a. standard")) +
  ggplot2::stat_function(fun=dnorm, args = c(1, 1),
                          ggplot2::aes(linetype="b. mean=1")) +
  ggplot2::stat_function(fun=dnorm, args = c(0, sqrt(1/2)),
                         ggplot2::aes(linetype="c. var=.5")) +
  ggtitle("Gaussian densities")

p
# %%
```

---
class: middle, center, inverse

## Computing the density of an image probability distribution

---

### Problem

Assume

- we know the (a) density $f$ of the distribution of some vector valued
random variable $X$, that $f$ is positive on some open set $U$.

- we have a _smooth_ function $\phi$ that maps $U$ to $V$

`r fontawesome::fa("meh-rolling-eyes")` Does the distribution of $Y= \phi(X)$ have a density?  If yes, can we (easily)
compute that density?

---


`r emo::ji("syringe")` Recall the _change of variable formula_ in elementary calculus.

If $\phi$ is monotone increasing and différentiable from open $A \subseteq \mathbb{R}$ to $B$
and $f$ is Riemann integrable over $B$, then

$$\int_B f(y) \, \mathrm{d}y = \int_A f(\phi(x)) \, \phi^{\prime}(x) \, \mathrm{d}x$$

---

### A multi-dimensional generalization of this elementary formula.

This extension is then used to establish an off-the-shelf formula for computing
the density of an image distribution

--

Let us start with a uni-dimensional warm-up `r fontawesome::fa("walking")`

When starting from the uniform distribution on $[0,1]$ and
applying a monotone differentiable transformation,  the density of the image measure
is easily computed.

- Let $\phi$ be differentiable and increasing on $[0,1]$, and
let $P$ be the uniform distribution on $[0,1]$.

Check  that $P \circ \phi^{-1}$
has density $\frac{1}{\phi'\circ \phi^\leftarrow}$  on $\phi([0,1])$.

---

.content-box-gray[

### Proposition

If the real valued random variable $X$  is distributed according to $P$
with density $f$, and $\phi$ is monotone increasing and differentiable
over $\operatorname{supp}(P)$, then the probability distribution of
$Y = \phi(X)$ has density

$$g = \frac{f \circ \phi^{\leftarrow}}{\phi^{\prime}\circ \phi^{\leftarrow}}$$

over $\phi\big(\operatorname{supp}(P)\big)$.

]

---

### Proof

By the fundamental theorem of calculus, the density $f$ is a.e. the derivative of the cumulative distribution function $F$
of $P$.

The cumulative distribution function of $Y=\phi(X)$ satisfies:

\begin{align*}
P \Big\{ Y \leq y \Big\}
  & = P \Big\{ \phi(X) \leq y \Big\} \\
  & = P \Big\{ X \leq \phi^{\leftarrow} (y) \Big\} \\
  & = F \circ \phi^{\leftarrow}(y)
\end{align*}

Almost everywhere, $F \circ \phi^{\leftarrow}$ is differentiable, and has derivative $\frac{f \circ \phi^{\leftarrow}}{\phi' \circ  \phi^{\leftarrow}}$ in $\phi(\text{supp}(P))$, $0$ elsewhere.and

$$P \Big\{ Y \leq y \Big\} = \int_{(-\infty, y] \cap \phi(\text{supp}(P))} \frac{f \circ \phi^{\leftarrow}(u)}{\phi' \circ  \phi^{\leftarrow}(u)} \mathrm{d}u$$

---

.content-box-gray[

### Corollary

If the distribution of the real valued random variable $X$ has density $f$ then the distribution of  $\sigma X + \mu$ has density $\frac{1}{\sigma}f\Big(\frac{\cdot -\mu}{\sigma}\Big)$

]

---

- In univariate calculus, it is easy to establish that if a function is  continuous and increasing over
an open set, it is invertible and its inverse is continuous and increasing

- If the function is differentiable with positive derivative, its inverse is also differentiable

- The differential and the differential of the inverse are related in transparent way

---

The Global Inversion Theorem extends the preceding  observation to the multivariate setting `r fontawesome::fa("running")`

.content-box-gray[

### Global Inversion Theorem

Let $U$ and $V$ be two non-empty open subsets of $\mathbb{R}^d$. Let $\phi$ be a continuous bijective
from $U$ to $V$. Assume furthermore that $\phi$ is continuously differentiable, and that
$D\phi_x$ is non-singular at every $x \in U$.

Then, the inverse function $\phi^{\leftarrow}$ is also continuously differentiable on $V$ and at every
$y \in V$:

$$D\phi^{\leftarrow}_y = \Big(D\phi_{\phi^{\leftarrow}(y)} \Big)^{-1}$$

]

--

The Jacobian determinant of $\phi$ is the determinant of the matrix that represents the differential.
It is denoted by $J_\phi$. Recall that:

$$J_{\phi^{\leftarrow}}(y) = \Big(J_{\phi}(\phi^{\leftarrow}(y)) \Big)^{-1}$$

---

The multidimensional version of the change of variable formula
is stated under the same assumptions as the Global Inversion Theorem.
We admit the next Theorem.

.content-box-gray[

### Geometric change of variable formula

Let $U$ and $V$ be two non-empty open subsets of $\mathbb{R}^d$. Let $\phi$ be a continuous bijective
from $U$ to $V$. Assume furthermore that $\phi$ is continuously differentiable, and that
$D\phi_x$ is non-singular at every $x \in U$.

Let $\ell$ denote the Lebesgue measure on $\mathbb{R}^d$.

For any  a non-negative  Borel-measurable function $f$:

$$\int_U f(x) \mathrm{d}\ell(x)   = \int  f(\phi^\leftarrow(y)) \Big|J_{\phi^\leftarrow}(y) \Big| \mathrm{d}\ell(y)$$

]

--



Moving from cartesian coordinates to polar/spherical coordinates
is easy thanks to an non-trivial application of the Geometricchange of variable formula

---

The Image density formula is a corollary of the geometric change of variable formula.

.content-box-gray[

### Image density formula

Let $P$ have  density $f$ over open $U \subseteq  \mathbb{R}^d$.

Let $\phi$ be bijective fron $U$ to $\phi(U)$ and $\phi$ be  continuously differentiable over $U$ with non-singular differential.

The density $g$ of the image distribution $P \circ \phi^{-1}$ over $\phi(U)$ is given by

$$g(y) = f\big(\phi^\leftarrow(y)\big) \times \big|J_{\phi^\leftarrow}(y)\big|  =  f\big(\phi^\leftarrow(y)\big) \times \Big|J_{\phi}(\phi^\leftarrow(y))\Big|^{-1}$$

]


---

The proof of Image density formula from Geometric change of variable formula is a routine
application of the transfer formula.


Let $B$ be a Borelian subset of $\phi(U)$.

By the transfer formula:
\begin{align*}
P\Big\{ Y \in B \Big\}
  & =  P\Big\{ \phi(X) \in B \Big\} \\
  & = \int_U \mathbb{I}_B(\phi(x)) f(x) \mathrm{d}\ell(x) \,.
\end{align*}

--

Now, we invoke Geometric change of variable formula:

\begin{align*}
\int_U \mathbb{I}_B(\phi(x)) f(x) \mathrm{d}\ell(x)
 & = \int_{\phi(U)} \mathbb{I}_B(\phi(\phi^\leftarrow(y))) f(\phi^\leftarrow(y)) \Big|J_{\phi^\leftarrow}(y)\Big| \mathrm{d}\ell(y) \\
 & = \int_{\phi(U)} \mathbb{I}_B(y) f(\phi^\leftarrow(y)) \Big|J_{\phi^\leftarrow}(y)\Big| \mathrm{d}\ell(y) \, .
\end{align*}

This suffices to conclude that $f\circ \phi^\leftarrow \Big|J_{\phi^\leftarrow}\Big|$ is a version of
the density of $P \circ \phi^{-1}$ with respect to Lebesgue measure over $\phi(U)$.


---

## Application: Gamma-Beta calculus

The image density formula is applied to show a remarkable connexion between
Gamma  and Beta distributions.

.content-box-gray[

### Proposition

Let $X, Y$ be independent random variables distributed according to
$\Gamma(p, \lambda)$ and $\Gamma(q, \lambda)$ (the intensity parameter are _equal_).

Let $U = X+Y$ and  $V= X/(X+Y)$.

- $U \perp \!\!\! \perp V$
- $U \sim \Gamma(p+q, \lambda)$
- $V \sim \operatorname{Beta}(p, q)$.

]

---

### Proof

The mapping $f: ]0, \infty)^2 \to ]0, \infty) \times ]0,1[$ defined by

$$f(x,y) =  \Big(x+y, \frac{x}{x+y} \Big)$$

is one-to-one with inverse $f^{\leftarrow}(u,v) = \Big(uv,u(1-v)\Big)$.

--

The Jacobian matrix of $f^{\leftarrow}$ at $(u,v)$ is

$$\begin{pmatrix}
  v & u \\
  (1-v) & -u
\end{pmatrix}$$

with determinant $-uv -u +uv=-u$.

---

The joint image density at $(u,v) \in ]0,\infty) \times ]0,1[$ is

\begin{align*}
& = \lambda^{p+q}\frac{(uv)^{p-1}}{\Gamma(p)} \frac{(u(1-v))^{q-1}}{\Gamma(q)}
\mathrm{e}^{-\lambda (uv + u(1-v))} u \\
& = \Big(\lambda^{p+q} \frac{u^{p+q-1}}{\Gamma(p+q)} \mathrm{e}^{\lambda u}\Big)
\times \Big(\frac{\Gamma(p+q)}{\Gamma(q)\Gamma(p)} v^{p-1} (1-v)^{q-1}\Big) \,.
\end{align*}

---

The factorization of the joint density proves that  $U \perp \!\!\! \perp V$

We recognize that the density of (the distribution of) $U$
is the Gamma density with shape parameter $p+q$, intensity parameter $\lambda$.

The density of the distribution of $V$ is the Beta density with parameters
$p$ and $q$.

---

### `r fontawesome::fa("puzzle")`

Assume $X_1, X_2, \ldots, X_n$ form an  independent family with each $X_i$
distributed according to $\Gamma(p_i, \lambda)$.

Determine  the joint distribution of

$$\sum_{i=1}^n X_i, \frac{X_1}{\sum_{i=1}^n X_i}, \frac{X_2}{\sum_{i=1}^n X_i}, \ldots, \frac{X_{n-1}}{\sum_{i=1}^n X_i}$$


---
class: middle, center, inverse


# `r fontawesome::fa("coffee", fill="white")`
