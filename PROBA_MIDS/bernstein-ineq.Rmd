template: inter-slide
name: bernstein-ineq

## Bennett and Bernstein's inequality


---
  
### Theorem Bennett's inequality

  
.bg-light-gray.b--light-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Let $X_1,\ldots,X_n$ be independent  random variables with finite
variance such that $X_i\le b$ for some $b>0$
almost surely for all $i\leq n$.

Let
$$S=\sum_{i=1}^n \left(  X_i-\mathbb{E} X_i\right)$$
and $v=\sum_{i=1}^n \mathbb{E}\left[X_i^2\right]$.
Let $\phi(u)=e^u-u-1$ for $u\in \mathbb{R}$.


$$\forall \lambda >0, \qquad \log \mathbb{E} e^{\lambda S}  \leq \frac{v}{b^2} \phi(b\lambda)$$

$$\forall t>0, \qquad P\{  S\geq t\}  \leq \exp\left(  -\frac{v}{b^2}h\left(\frac{bt}{v}\right) \right)$$
  
where $h(u)=\phi^*(u) = (1+u)\log(1+u) -u$ for $u>0$.

]

---
  
`r fontawesome::fa("triangle-exclamation")`

Bennett's inequality provides us with improved tail bounds for  the
binomial random variable with parameters $n$ and $\mu/n$.

This binomial random variable is distributed like the sum $n$ independent
Bernoulli random variables with parameter $\mu/n$.

This fits in the scope of Bennett's inequality, we can choose $b=1$ and $v=\mu.$
  
`r fontawesome::fa("triangle-exclamation")` The obtained upper bound on the logarithmic moment generating function coincides with logarithmic moment generating function of a centered Poisson random variable with parameter $\mu$
  
  
---
  
### Proof
  
The proof combines the Cramer-Chernoff technique with an _ad hoc_ upper bound
on $\log \mathbb{E} \mathrm{e}^{\lambda (X_i - \mathbb{E}X_i)}$.

By homogeneity, we may assume $b=1$.

Note that $\phi(\lambda)/\lambda^2$ is non-decreasing over $\mathbb{R}$. For
$x\leq 1, \lambda \geq 0$, $\phi(\lambda x)\leq x^2 \phi(\lambda)$.

$$\begin{array}{rl}
\log \mathbb{E} \mathrm{e}^{\lambda (X_i - \mathbb{E}X_i)}
& = \log \mathbb{E} \mathrm{e}^{\lambda X_i}  - \lambda \mathbb{E}X_i \\
& \leq \mathbb{E} \mathrm{e}^{\lambda X_i} - 1 - \lambda \mathbb{E}X_i \\
& =  \mathbb{E} \phi(\lambda X_i) \\
& = \mathbb{E}X_i^2 \phi(\lambda)\end{array}$$
  
`r fontawesome::fa("square")`

---
  
Whereas Bennett's bound works well for Poisson-like random variables,
our last bound  is geared towards Gamma-like random variables. It is
one of the pillars of statistical learning theory.


---

### Theorem Bernstein's inequality


.bg-light-gray.b--light-gray.ba.bw1.br3.shadow-5.ph4.mt5[
  
Let $X_1,\ldots,X_n$ be independent
real-valued random variables.
Assume that there exist positive numbers
$v$ and $c$ such that $\sum_{i=1}^n \mathbb{E}\left[X_i^2\right]  \leq v$ and

$$\sum_{i=1}^n \mathbb{E}\left[  \left(X_i\right)_+^q \right] \leq\frac{q!}{2}vc^{q-2}\quad \text{for all integers }  q \geq 3$$
  
Let $S=\sum_{i=1}^n \left(X_i-\mathbb{E} X_i \right)$
  
Then for all $\lambda\in (0,1/c)$
  
$$\log \mathbb{E} \mathrm{e}^{\lambda (S- \mathbb{E}S)} \leq \frac{v\lambda^2}{2(1-c\lambda)}$$
  
For $t>0$,

$$P \big\{ S > t \big\} \leq \exp\Big( - \frac{v}{c^2} h_1\big(\frac{ct}{v}\big)\Big)$$
  
with $h_1(x)= 1+x - \sqrt{1+2x}$
  
]  

---
  
### Proof
  
The proof combines again the Cramer-Chernoff technique with an _ad hoc_ upper bound
on $\log \mathbb{E} \mathrm{e}^{\lambda (S - \mathbb{E}S)}$.


Let again $\phi(u)=e^u-u-1$ for $u\in \mathbb{R}$.

For $\lambda>0$,

$$\begin{array}{rl}\phi(\lambda X_i)  & = \sum_{k=2}^\infty \frac{\lambda^k X_i^k}{\lambda^k} \\ & \leq \frac{\lambda^2 X_i^2}{2!} + \sum_{k=3}^\infty \frac{\lambda^k (X_i)_+^k}{\lambda^k}\end{array}$$
  
---
  
### Proof (continued)
  
For $c> \lambda>0$,

$$\begin{array}{rl} \log \mathbb{E} \mathrm{e}^{\lambda S}  & = \sum_{i=1}^n \log \mathbb{E} \mathrm{e}^{\lambda (X_i - \mathbb{E}X_i)} \\  & \leq \sum_{i=1}^n \mathbb{E} \phi(\lambda X_i) \\
& \leq \frac{\lambda^2 \sum_{i=1}^n  \mathbb{E} X_i^2}{2!} + \sum_{k=3}^\infty \frac{\lambda^k \sum_{i=1}^n \mathbb{E}(X_i)_+^k}{k!} \\  & \leq \frac{\lambda^2 v}{2} + \sum_{k=3}^\infty \frac{\lambda^k v c^{k-2}}{2} \\  & =  \frac{\lambda^2 v}{2 (1 - c \lambda)}\end{array}$$
  
The tail bound follows by maximizing

$$\sup_{\lambda \in [0,1/c)}  \lambda t - \frac{\lambda^2 v}{2 (1 - c \lambda)} = \frac{v}{c^2} \sup_{\eta \in [0,1)} \eta \frac{ct}{v} - \frac{\eta^2}{2(1-\eta)}$$
  
`r fontawesome::fa("square")`

---
exclude: true

### Applications of Bernstein's inequality : unbalanced binomials



---
exclude: true

### Applications of Bernstein's inequality : Gamma distributions
  
  
  
  




  
  