template: inter-slide
name: quantization

## $k$-means and _quantization_

---

Quantization plays an important role in signal processing and information theory (lossy coding with quadratic distortion)

Given a probability distribution $P$ over
a metric space $(\mathcal{X},d)$, a $k$-quantizer is defined by a $k$-element subset of
$\mathcal{X}$, $\mathbf{c} :=  \{x_1,\ldots, x_k\}$ called a codebook.

The codebook defines a quantization by mapping every $x \in \mathcal{X}$ to its nearest neighbor in codebook $\mathbf{c}$

The quality of a codebook is assessed by its mean distortion measured as the mean quadratic distance to the nearest neighbor:

$$\mathsf{R}(\mathbf{c}) := \mathbb{E}\left[\min_{x \in \mathbf{c}}d(X,x)^2\right]$$

where $X \sim P$

---

When the  $P$ is known,  designing an optimal codebook  may  be a difficult optimization problem

When $P$ is unknown, if the statistician is left with an i.i.d. sample $X_1,\ldots,X_n \sim P$, the first reasonable thing to do is minimizing the empirical distortion, the $k$-means cost:

$$\mathsf{R}_n(\mathbf{c}) :=  \frac{1}{n} \sum_{i=1}^n \min_{x \in \mathbf{c}} d(X_i,x)^2$$



### Theorem NP-hardness of k-means cost minimization

.bg-light-gray.b--light-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Mininizing the $k$-means cost (sum of within clusters sum of squares) is $\mathsf{NP}$ -hard.

]

???

Sanjoy Dasgupta proved that if $\mathsf{P} \neq \mathsf{NP}$, minimizing
the $k$-means cost (sum of within clusters sum of squares) is __computationally intractable__

This is one result in a long series of negative results going back
to the late seventies

---

### Statistical/information-theoretical issues

Even though minimizing the $k$-means cost is hard,
one may investigate the _statistical_ problem raised by minimizing the $k$-means cost.

$k$-means served as a showcase for empirical process theory during the early 1980's

Significant progress during recent years
The $k$-means cost provides a concrete illustration of a recurrent situation.

If the sampling distribution is square integrable and has a density with respect to Lebesgue measure, the mapping  $\mathsf{R}(\mathbf{c})$
is differentiable, its gradient can be explicitly computed


???

`Cite(myBib, "Pol84")`  `Cite(myBib, "MR3080408")`, `Cite(myBib, "MR3316191")`

---

### Smoothness of _k_-means cost

Whereas the local behavior of the $k$-means cost is simple, the global behavior remains elusive.

Bounding the number of global minima, local minima, local extrema and saddlepoints   is difficult.

Observation: under fairly general assumptions,
the $k$-means cost function is twice differentiable in the neighborhood of optimal codebooks

Even though the $k$-means cost function is not convex, recent advances tell us that if sample size tends to infinity,  an empirical cost functions will also share local minima, local maxima, and local saddlepoints with the  theoretical population cost function

???

`Cite(myBib, "Pol84")`

`Cite(myBib, "MR3851754")`

---

### Pollard's regularity conditions

- The sampling distribution is absolutely continuous with respect to Lebesgue measure on $\mathbb{R}^p$.

- The Hessian matrix of the mapping $\mathbf{c} \mapsto \mathsf{R}(\mathbf{c})$ is positive definite for all optimal codebooks

--

Under Pollard's regularity conditions, let $\mathbf{c}^*$ denote the optimal codebook, and $\widehat{\mathbf{c}}_n$ denote the optimal empirical codebook

Large sample  behavior of the empirically optimal codebook:

- $\sqrt{n}\|\widehat{\mathbf{c}}_n - \mathbf{c}^*\|$ is asymptotically normal

and

- $n \left( \mathsf{R}(\widehat{\mathbf{c}}_n) - \mathsf{R}(\mathbf{c}^*)\right)$ is stochastically bounded

---

### Key observation


Pollard's condition entails

that

for some constant $\kappa_0>0$,

$$\mathsf{R}(\mathbf{c}) - \mathsf{R}(\mathbf{c}^*) \geq \kappa_0 \|\mathbf{c}- \mathbf{c}^*\|^2$$

