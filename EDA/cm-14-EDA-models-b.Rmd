---
title: "EDA XIV: Tidy models II"
subtitle: "Analyse de Données Master I, MFA et MIDS"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2021/12/21 (updated: `r Sys.Date()`)"
params:
  title: "Introduction to tidy modeling II"
  curriculum: "Master I MIDS & MFA"
  coursetitle: "Analyse Exploratoire de Données"
  lecturer: "Stéphane Boucheron"
  homepage: "http://stephane-v-boucheron.fr/courses/eda/"
  curriculum_homepage: "https://master.math.univ-paris-diderot.fr/annee/m1-mi/"
  institution: "Université de Paris"
output:
  xaringan::moon_reader:
    css: ["header-footer.css", "xaringan-themer.css"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
bibliography: mon_chapeau.bib
---
name: inter-slide
class: left, middle, inverse

{{ content }}

---
name: layout-general
layout: true
class: left, middle

```{r setup, child="loaders_fixers.Rmd", echo=FALSE, message=FALSE, warning=FALSE}

```


---


```{r child="title_slide.Rmd"}

```


---
template: inter-slide

## `r fontawesome::fa("map", fill="white")`

### [Starter](#starter)

### [....](#diy)

### [...](#books)

---
template: inter-slide

## Preprocess data with recipes


???


---

```{r}
knitr::include_url("https://www.tidymodels.org/books/moderndive/")
```

---

### `recipes`,

`recipes` is designed to help you preprocess your data before training your model.

Recipes are built as a series of preprocessing steps, such as:

- converting qualitative predictors/categorical variables to indicator variables (also known as dummy variables),
- transforming data to be on a different scale (e.g., taking the logarithm of a variable),
- transforming whole groups of predictors together,
- extracting key features from raw variables (e.g., getting the day of the week out of a date variable),
- ...

???

If you are familiar with `R`’s formula interface, a lot of this might sound familiar and like what a `formula` already does.

Recipes can be used to do many of the same things, but they have a much wider range of possibilities.

---

```{r}
library(tidymodels)      # for the recipes package, along with the rest of tidymodels

# Helper packages
pacman::p_load("nycflights13")    # for flight data
pacman::p_load(skimr)
```

---

```{r}
set.seed(123)

flight_data <-
  flights %>%
  mutate(
    # Convert the arrival delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    # We will use the date (not date-time) in the recipe below
    date = lubridate::as_date(time_hour)
  ) %>%
  # Include the weather data
  inner_join(weather, by = c("origin", "time_hour")) %>%
  # Only retain the specific columns we will use
  select(dep_time, flight, origin, dest, air_time, distance,
         carrier, date, arr_delay, time_hour) %>%
  # Exclude missing data
  na.omit() %>%
  # For creating models, it is better to have qualitative columns
  # encoded as factors (instead of character strings)
  mutate(across(where(is.character), as.factor))

```

---

```{r}
flight_data %>%
  group_by(arr_delay) %>%
  summarise(percent= 100*n()/nrow(flight_data)) %>%
  ggplot() +
  aes(x=arr_delay, y=percent) +
  geom_col()
```

---

```{r}
flight_data %>%
  skimr::skim(dest, carrier)
```

???

> We will perform logistic regression

> The variables `dest` and `carrier` will be converted to dummy variables.

> Some of these values do not occur very frequently and this could complicate our analysis.


---

### Data splitting


```{r}
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(flight_data, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)

# train_data |> glimpse()
```

???

> Let’s split this single dataset into two:

> - a training set and
> - a testing set.

> We’ll keep most of the rows in the original dataset (subset chosen randomly) in the training set.

> The training data will be used to _fit the model_, and the testing set will be used to _measure model performance_.

> To do this, we can use the `rsample` package to create an object that contains the
information on how to split the data, and then two more rsample functions
to create data frames for the training and testing sets:

---

### `recipe()`


```{}
flights_rec <-
    recipe(arr_delay ~ ., data = train_data)  #<<

flights_rec |> glimpse()
```

---

### `r fontawesome::fa("layer-group")` Building an instance of `recipe`

.fl.w-50.pa2.f6[

```{r update_role, echo=TRUE, eval=FALSE}
flights_rec <-
  recipe(arr_delay ~ .,  #<<
         data = train_data) %>%   #<<
  update_role(flight, time_hour, #<<
              new_role = "ID")  #<<

summary(flights_rec)
```
]

.fl.w-50.pa2.f6[


```{r update_role-out, ref.label="update_role", echo=FALSE}

```

]


???

> The `recipe()` function as we used it here has two arguments:

> A `formula`. Any variable on the left-hand side of the tilde `(~)` is considered the model _outcome_ (here `arr_delay`).

> On the right-hand side of the tilde are the _predictors_ (covariates, explanatory variables).  Variables may be listed by name, or you can use the `dot (.)`
> to indicate all other variables as predictors.

> The `data`. A recipe is associated with the data set used to create the model.
> This will typically be the training set, so `data = train_data` here.

> Naming a data set doesn’t actually change the data itself;
> it is only used to catalog the names of the variables and their types, like `factors`, `integers`, `dates`, etc.


`r fontawesome::fa("hand-point-right")` Two dialects in data science:

- Statistics: explanatory variables/covariates, response variable
- Predictive modelling/Machine learning: predictors, outcome

Function `recipe()` serves as a constructor

---

### Class `recipe`


.fl.w-50.pa2[

```{r recipe_nuts, eval=FALSE, echo=TRUE}
class(flights_rec)

attributes(flights_rec)

names(flight_rec)
```


]


.fl.w-50.pa2[

```{r recipe_nuts-out, ref.label="recipe_nuts", echo=FALSE, eval=TRUE}

```

]



???

Differences between

- `term_info`
- `var_info`

Meaning  of

- `levels`
- `steps`
- `template`


What are the possible values of `role`?

Should the set of variables labelled as `ID` form a _primary key_?


---

### Feature engineering

> How should the date be encoded into the model? The date column has an R date object so including that column “as is” will mean that the model will convert it to a numeric format equal to the number of days after a reference date:

```{r}
flight_data %>%
  distinct(date) %>%
  mutate(numeric_date = as.numeric(date))
```

???

Making time series analysis possible

Detecting _trends_ and _seasonalities_

> It’s possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable.

> However, it might be better to add model _terms_ _derived_ from the date that have a better potential to be important to the model.

> For example, we could derive the following meaningful features from the single date variable:

- the day of the week,
- the month, and
- whether or not the date corresponds to a holiday.

---

### Feature engineering and `steps`

```{r}
flights_rec <-
  recipe(arr_delay ~ ., data = train_data) %>%
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>%
  step_holiday(date,
               holidays = timeDate::listHolidays("US"),
               keep_original_cols = FALSE)
```

???

`step_date`  and `step_holiday` insert two items in the `steps` attribute of `flights_rec`

Attributes `var_info`  and `term_info` look unchanged

Attribute `template` looks unchanged

We are building a `pipeline`

---

###

```{r}
flights_rec$template
```

---

### Creating dummy variables (one hot encoding)



???

The idea


---

### Dummy encoding of categorical variables

```{r}
flights_rec <- flights_rec %>%
  step_dummy(all_nominal_predictors()) #<<  tidy selection


```

???


> Selectors `all_numeric_predictors()` and `all_nominal_predictors()`  select on role and type

How does it fit with tidy selection?



---

### Handling rare values


> When the recipe is applied to the training set, a column is made for `LEX` because the factor levels
come from `flight_data` (not the _training set_), but this column will contain all zeros.

> This is a _zero-variance predictor_ that has no information within the column.

> While some `R` functions will not produce an error for such predictors,
it usually causes warnings and other issues. `step_zv()`
will remove columns from the data when the training set data have a single value,
so it is added to the recipe after `step_dummy()`:


---

```{r}
flights_rec <- flights_rec %>%
  step_zv(all_predictors())
```


---

### And now?

> Now we’ve created a specification of what should be done with the data.

> How do we use the recipe we made?


---

###


```{r}
lr_mod <-
  logistic_reg() %>%
  set_engine("glm")
```

---


### Process the recipe using the training set:

> This involves any estimation or calculations based on the training set.

> The training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.

---

### Apply the recipe to the training set

> We create the final predictor set on the training set.

---

### Apply the recipe to the test set

> We create the final predictor set on the test set.

> Nothing is recomputed and no information from the test set is used here;

> The dummy variable and zero-variance results from the training set are applied to the test set.


---

.fl.w-50.pa2[

```{r fl_workflow, echo=TRUE, eval=FALSE}
flights_wflow <-
  workflow() %>%
  add_model(lr_mod) %>%
  add_recipe(flights_rec)

flights_wflow
```

]

.fl.w-50.pa2[

```{r fl_workflow-out, ref.label="fl_workflow", echo=FALSE}

```
}
]

---

###

.fl.w-50.pa2[

```{r flights_fit, eval=FALSE, echo=TRUE}
flights_fit <-
  flights_wflow %>%
  fit(data = train_data)
```


]


.fl.w-50.pa2[

```{r flights_fit-out, ref.label="flights_fit", echo=FALSE}

```


]

---

###


```{r}
flights_fit %>%
  extract_fit_parsnip() %>%
  tidy()
```

---

### Prediction


> Our goal was to predict whether a plane arrives more than 30 minutes late.

```{r}
```


???

We have just:

Built the model (`lr_mod`),

Created a preprocessing recipe (`flights_rec`),

Bundled the model and recipe (`flights_wflow`), and

Trained our workflow using a single call to `fit()`.

The next step is to use the trained workflow (`flights_fit`) to predict
with the unseen test data, which we will do with a single call to `predict()`.

The `predict()` method

- applies the recipe to the new data, then
- passes them to the fitted model.


---


```{r}
predict(flights_fit, test_data)
   %>% head()
```


```{r}
flights_aug <-
  augment(flights_fit, test_data)

# The data look like:
flights_aug %>%
  select(arr_delay, .pred_class) %>%
  table()

#> # A tibble: 81,455 × 5
#>   arr_delay time_hour           flight .pred_class .pred_on_time
#>   <fct>     <dttm>               <int> <fct>               <dbl>
#> 1 on_time   2013-01-01 05:00:00   1545 on_time             0.945
#> 2 on_time   2013-01-01 05:00:00   1714 on_time             0.949
#> 3 on_time   2013-01-01 06:00:00    507 on_time             0.964
#> 4 on_time   2013-01-01 06:00:00   5708 on_time             0.961
#> 5 on_time   2013-01-01 06:00:00     71 on_time             0.962
#> # … with 81,450 more rows

```

---

### Evaluating


Now that we have a tibble with our predicted class probabilities,
how will we evaluate the performance of our workflow?

We can see from these first few rows that our model predicted these 5 on time flights correctly
because the values of `.pred_on_time` are `p > .50`.

But we also know that we have `81,455` rows total to predict.

We would like to calculate a metric that tells how well our model predicted late arrivals,
compared to the true status of our outcome variable, `arr_delay`.

Let’s use the area under the ROC curve as our metric, computed using `roc_curve()` and `roc_auc()`
from the `yardstick` package.

To generate a ROC curve, we need the predicted class probabilities
for `late` and `on_time`, which we just calculated in the code chunk above.
We can create the ROC curve with these values, using `roc_curve()` and then piping to the `autoplot()` method:

---

```{r}
flights_aug %>%
  roc_curve(truth = arr_delay, .pred_late) %>%
  autoplot()
```

---

### Area under the ROC curve

```{r}
flights_aug %>%
  roc_auc(truth = arr_delay, .pred_late)
```


---

### ... 

---

```{r child='closing_slide.Rmd'}

```
