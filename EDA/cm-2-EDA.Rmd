---
title: "Univariate statistics"
subtitle: "EDA Master I, MIDS & MFA"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/11/20 (updated: `r Sys.Date()`)"
params:
  title: "Univariate statistics"
  curriculum: "Master I ISIFAR"
  coursetitle: "Analyse des Données"
  lecturer: "Stéphane Boucheron"
  homepage: "http://stephane-v-boucheron.fr/courses/isidata/"
  curriculum_homepage: "https://master.math.univ-paris-diderot.fr/annee/m1-isifar/"
output:
  xaringan::moon_reader:
    css: ["header-footer.css",  "xaringan-themer.css", "custom-callout.css"]
    lib_dir: libs
    seal: false
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inter-slide
class: left, middle, inverse

{{ content }}

---
name: layout-general
layout: true
class: left, middle

```{r setup, child="loaders_fixers.Rmd", echo=FALSE, message=FALSE, warning=FALSE}

```

---

```{r child='title_slide.Rmd'}

```

---
exclude: true
template: inter-slide

# Univariate Statistics

### `r Sys.Date()`

#### [`r params$curriculum`](`r params$homepage_curriculum`)

#### [`r params$coursetitle`](`r params$homepage`)

#### [`r params$lecturer`](http://stephane-v-boucheron.fr)


---
template: inter-slide

## `r fontawesome::fa("map", fill="white")`

### [Samples, observations an variables ](#samples)

### [Summary statistics](#numerical-summaries)

### [Robust summary statistics](#robust)

### [Boxplots](#boxplots)

### [Histograms](#histograms)

### [Cumulative distribution function](#ecdf)

### [Quantile function](#quantile-function)


---


---
template: inter-slide
name: samples

## Samples, observations an variables 

???

- Start with lexicon 

- We use a dataset (`flights` from `nycflights13`) to illustrate definitions 


---

### `r fontawesome::fa("plane-departure")` `flights`dataset from `nycflights13` package

.fl.w-50.pa2[
### A dataframe

```{r }
pacman::p_load("nycflights13")  #<<
# install.packages("nycflights13")  
# require("nycflights13")
```

> On-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013.

- `flights` is a multivariate _sample_ 

- Each `row` corresponds to an _observation/individual_

- Each column/variable corresponds to a _measurement_ 

]

.fl.w-50.pa2.f6[

### A glimpse 

```
Columns: 19
$ year           <int> 2013,…
$ month          <int> 1, 1,…
$ day            <int> 1, 1,…
$ dep_time       <int> 517, …
$ sched_dep_time <int> 515, …
$ dep_delay      <dbl> 2, 4,…
$ arr_time       <int> 830, …
$ sched_arr_time <int> 819, …
$ arr_delay      <dbl> 11, 2…
$ carrier        <chr> "UA",…
$ flight         <int> 1545,…
$ tailnum        <chr> "N142…
$ origin         <chr> "EWR"…
$ dest           <chr> "IAH"…
$ air_time       <dbl> 227, …
$ distance       <dbl> 1400,…
$ hour           <dbl> 5, 5,…
$ minute         <dbl> 15, 2…
$ time_hour      <dttm> 2013…
```

]

???

In `r fontawesome::fa("r-project")` education and training, dataset `nycflights` is used to teach 
table wrangling 

We use table `flights` from  package `nycflights`  as a collection of univariate samples



---
### Definition(s)

A _sample_ is a sequence of items of the same type

In `r fontawesome::fa("r-project")`, if the type matches a basetype, a sample is typically represented using a `vector`

A _numerical_ (quantitative) sample is just a sequence of numbers (either integers
or floats), it is represented by a `numeric` vector

A _categorical_ (qualitative)  sample is a sequence of values  from  some predefined finite set

In `r fontawesome::fa("r-project")`, categorical samples are handled using a special class: `factor`

---

In this lesson, we are concerned with _univariate statistics_, that  is either with

- _numerical samples_,

- _categorical samples_ taken from some finite (or seldom countable) set


`r fontawesome::fa("exclamation-triangle")` The boundary between qualitative and quantitative is not always straightforward


---

### Table `flights`  from package `nycflights`

- Each column from the dataset `flights` may be considered as a univariate sample 

- There are `336776` rows in the table: we (potentially) have `19` univariate samples of length $n=336776$ 

-  `r fontawesome::fa("exclamation-triangle")` By considering the columns as univariate samples we _disregard_ possible relations between the columns 

- Some columns are irrelevant: columns `year, month, day`   and others can be recovered from other columns like `time_hour`


???

Examples of functional dependance:

```{r}
sum((flights$sched_dep_time  %/% 100) != (flights$hour))
sum((flights$sched_dep_time  %% 100) != (flights$minute))
```

---

### Focusing on (more) interesting columns

```{r}
flights %>% 
  select(origin, dest, distance, flight, tailnum, carrier, 
         air_time, time_hour, dep_delay, arr_delay, sched_dep_time, sched_arr_time) %>% 
  glimpse(width=80)
```


???

Table is not tidy, it is messy 




---



- Columns `carrier`, `dest`, `origin` are considered categorical

- Possible values for `origin` correspond to the three airports in the NY city area:

```{r}
levels(factor(flights$origin))
```

- Columns `arr_delay`, `dep_delay`, distance are numerical

- Column `time_hour`, ... contains `timestamps` 



---


```{r, echo=FALSE, message=FALSE, fig.align='center'}
m <- leaflet::leaflet(width=900, height=600) |>
  leaflet::setView(lat=40.6397,
                   lng=-74.0118, zoom=10) |>
  leaflet::addTiles() |>  # Add default OpenStreetMap map tiles
  leaflet::addMarkers(lat=40.6446,
                      lng=-73.7823,  
                      popup="JFK") |>
  leaflet::addMarkers(lat=40.7747, 
                      lng=-73.8729,
                      popup="LGA") |>
  leaflet::addMarkers(lat=40.6905, 
                      lng=-74.177,
                      popup="EWR")  

m
```



---


We will learn about:

- numerical summaries (mean, standard deviation and so on)

- graphical displays for numerical summaries

for univariate samples

--

Then we move to:

- sufficient/exhaustive descriptions of numerical samples (CDF, Quantiles, histogrames, density estimators)

- graphical representation of sufficient/exhaustive descriptions 


---
template: inter-slide
name: numerical-summaries

## Summary statistics


???

We start with a review of the most basic summary statistics that mean and standard deviation.
I should say the empirical mean and the empirical standard deviation. Both statistics are easily defined 
and computed. 

---

### Numerical summaries for quantitative variables

- __Mean__:

$$\overline{X}_n = \frac{1}{n}\sum_{i=1}^n x_i \qquad  \textbf{(location)}$$

- **Standard deviation**:

$$s^2 = \frac{1}{n-1} \sum_{i=1}^n \big(x_i - \overline{X}_n\big)^2 \quad \text{or}\quad s^2 = \underbrace{\frac{1}{n} \sum_{i=1}^n \big(x_i - \overline{X}_n\big)^2}_{\text{Empirical variance}}\qquad\textbf{(dispersion)}$$

???

Computing the mean and standard deviation just involves something over the whole sample. 
This requires a linear time with respect to the size of the sample. We cannot expect to do better.

The mean tells us where the sample points lie on the realign on average. 
The standard deviation tells us how widely spread the sample points are. 

We give two slightly different definitions of the squared standard deviation, that is of the variance.
The first one is what `r fontawesome::fa("r-project")` will deliver.  The second one is 
what Mathematicians called the variance of the empirical distributions or the empirical variance. 

The rational for choosing the first definition comes from mathematical statistics:
it delivers an unbiased estimate of the so-called population variance if the sample has been obtained by i.i.d.
 sampling from some population distribution. 

We review some important properties of the mean and standard deviation

---

### Properties of mean and standard deviation

`r fontawesome::fa("hand-point-right")` If we translate a sample by some fixed value $t$, we translate the mean by $t$. 
The mean is *location* estimator  

If $y_i = x_i + t$ for some $t \in \mathbb{R}$ and all $i \leq n$, 
$$\overline{Y}_n =  \sum_{i=1}^n \frac{1}{n} y_i = \overline{X}_n + t$$


`r fontawesome::fa("hand-point-right")`  If we translate a sample by some fixed value $t$, *we leave the standard deviation invariant*. 

`r fontawesome::fa("hand-point-right")` If we multiply  a sample by a fixed quantitiy $\sigma$, we multiply the sample standard by $\sigma$. The standard deviation is a *scale* estimator

$$\frac{1}{n}\sum_{i=1}^n (y_i - \overline{Y}_n)^2 = \frac{1}{n} \sum_{i=1}^n (x_i  +t - \overline{X}_n -t)^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{X}_n)^2$$

The mean  is not the only location estimator 

The standard deviation is not the only scale estimator

???

Now, we turn to 2 other indices that are useful in descriptive statistics for summarising numerical samples.

These indices are called the skewness and the kurtosis. 


---

### Beyond location et dispersion

- **Skewness**

$$\frac{\frac{1}{n} \sum_{i=1}^n \big(x_i - \overline{X}_n\big)^3}{s^3}$$

- **Kurtosis**

$$\kappa = \frac{\frac{1}{n} \sum_{i=1}^n \big(x_i -\overline{X}_n\big)^4}{s^4} - 2$$

???

Note that the two indices are dimension-free. If we scale the sample, we leave them both invariant.  
If we translate or rescale the sample, we leave them both invariant. The two indices tell us something about the shape of the empirical distribution.
We'll see more about this topic later. 

---

### Properties of skewness and kurtosis


`r fontawesome::fa("hand-point-right")`  Skewness and kurtosis are _both_ translation and scale invariant

???

Now, let's see how we can compute the mean, the standard deviation and the other indices using the `R` project.

---

### Numerical summaries at work

Base `R` provides us with two functions: `mean()` and `sd()`

We illustrate the two functions on simulated data.

```{r}
s <- rnorm(1000)   # a random sample from the standard normal distribution of size 1000

n <-  length(s)

mean(s) - sum(s)/n  #<<

sd(s) - sqrt(sum((s-mean(s))^2)/(n-1))   #<<
```

The two functions have optional arguments that are useful when handling real data

???

Let's try now to summarise the arrival delay column of the `flights` dataframe

---

### Summarizing `arr_delay` (step by step)


```{r}
mean(flights$arr_delay)

mean(flights$arr_delay, na.rm=T) #<<

sd(flights$arr_delay)

sd(flights$arr_delay, na.rm=T) #<<      
```

???

If we do not use the optional argument,  the result of the `mean()` function is not very useful. 
This is due to the fact that the arrival delay column contains `NA` (not available) entries
This  happens with real data. The result is consistent with the principles that underlie the design of the `R` project. 
If some entries are not available, we have no idea about the way to sum them up. 
We have no idea about the way to compute the mean and the  standard deviation. Some action is required. 

The most obvious action consist in discarding the non-available entries this is what we do 
when we use the optional argument `na.rm` and and set it to `TRUE`. 
Using the optional argument allows us to obtain a meaningful result,  even if we have to take it with a grain of salt. 



---

### Summarizing `arr_delay` (all inclusive)

If we want to obtain a useful summary of some numerical sample, the `R` project offers a generic function called `summary`:

```{r}
summary(flights$arr_delay)
```

--

`summary` delivers the empirical mean (`Men`) and the number of `NA's`  in the sample. It also 
returns the maximum and the minimum value in the sample, as well as three other quantities (The median and the 
two other quartiles)


---

### Summarizing `arr_delay` (all inclusive, continued)

It is often useful to obtain the result as a one-row table (for inclusion in a report for example)

`skim()` from package `skimr` does it  `r fontawesome::fa("glass-cheers")`


```{r, warning=FALSE}
flights$arr_delay %>%   
  skimr::skim() %>%    #<<
  select(1:6, -starts_with('skim')) %>% 
  knitr::kable(digits = 2,
  caption = "Summary statistics for nycflights13 arrival delay")
```

`r fontawesome::fa("hand-point-right")` `NA's` matter!


`r fontawesome::fa("hand-point-right")` We use the pipe `%>%` to *chain* a sequence of 
simple processing steps: `skim(), select(), kable()`

???

- We could call `skim` on a dataframe. It would generate a summary for every column

- In database parlance, `select`  from package `dplyr` (not from base `R`) performs a *projection* on a subset 
of columns (a selection of columns). Here we first project on the first six columns, then discard columns 
whose name starts with pattern `skim`.   This is an instance of *tidy selection*

- `kable()` from package `knitr` provides a kind of pretty print for tables and dataframes


---

### A first glimpse at the pipe `%>%` from package `magrittr`

`r fontawesome::fa("hand-point-right")` The pipe `%>%` allows us to chain (many) processing steps in a readable way

Assume we have some object $x$ and we want to compute  $h \circ g \circ f(x)$

Expression  `y <- h(g(f(x)))` is cluttered with parentheses. It is not readable `r fontawesome::fa("frown")`

```
tmp <- f(x)
tmp <- g(tmp)
y <- h(tmp)
```

is slightly more readable, but using _temporary_ objects makes things more complicated than they should be

`r fontawesome::fa("smile")` The pipe `%>%` or the standard pipe `|>` (for `R 4.x`) alleviate our lives :

```{r, eval=FALSE}
y <- x %>%   #<< feed x in the pipe 
  f() %>%    #<< feed f(x)  in the pipe
  g() %>%    #<< feeed g(f(x))  in the pipe
  h()    #<<  return h(g(f(x))) and assign the result to y
```

???

This is our first contact with the pipe operator

The pipe operator was introduced with package `magrittr`  around 2010. 
Since then it became it has become an indispensable tool for all `R` practitioners

The pipe operator can do much more than what we presented here

---
exclude: true 

### With `dplyr::summarise`

```{r}
flights %>% 
  select(- year, -month, -day, -hour, -minute , -time_hour, -ends_with("time"), -flight) %>% 
  summarise(across(where(is.numeric),   #<<
                   .fns=list("mean"=mean, "median"=median , "IQR"=IQR), #<<
                   na.rm=T, #<<
                   .names="{.col}_{.fn}")) %>%  #<<
  pivot_longer(cols = everything()) %>% 
  knitr::kable(digits=2)
```


---
exclude: true


### With `dplyr::summarise` (result)

---
exclude: true

### What if some data are missing?

```{r}
s <- rnorm(10)
s[c(2, 3, 7)] = NA_real_
s
```

```{r}
mean(s)   #<<
mean(s, na.rm = TRUE)  #<<
mean(s[-c(2, 3, 7)])
```

---
template: inter-slide

## More on missing values ([`NA's`]())

---

Real datasets are plagued missing values 

Missing values are a great annoyance to anyone who wants to compute statistics

Through the ages, statisticians have developped techniques to cope with missing values

The crudest technique consists in throwing away all observations with missing values (`dropna()`)

This is not always the smartest thing to do 

---

### Missing values and trimming

The code for `mean.default` goes (far) beyond computing the weighted mean

.f6[

```{r, eval=FALSE}
function (x, trim = 0, na.rm = FALSE, ...)
{
    # ...
    if (na.rm)    #<<
        x <- x[!is.na(x)]    #<<
    if (!is.numeric(trim) || length(trim) != 1L)
        stop("'trim' must be numeric of length one")
    n <- length(x)
    if (trim > 0 && n) {
        if (is.complex(x))
            stop("trimmed means are not defined for complex data")
        if (anyNA(x))
            return(NA_real_)
        if (trim >= 0.5)
            return(stats::median(x, na.rm = FALSE))
        lo <- floor(n * trim) + 1      #<<
        hi <- n + 1 - lo               #<<
        x <- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]  #<<
    }
    .Internal(mean(x))  #<< 
}

```
]

???

There are several optional arguments 

- `trim`
- `na.rm`
- The ellipsis `...`

If `na.rm` is not `FALSE`,  `x <- x[!is.na(x)]` masks out the `NA` entries in argument `x`

Argument `trim` has a *default* value (0)

---

### `r fontawesome::fa("brain")` The code raises questions

- What happens when `na.rm` is `TRUE`?

- What are the possible values of argument `trim`?

- If `trim` belongs to $(0, 1/2)$,  what happens?

- Which lines actually compute the (uniformly) weighted mean?

???


- The first `if (...) {...}` is an example of defensive programming

- `if (na.rm) { ... }` handles missing data when the default behavior is overriden.
`NA` are filtered out

- When positive, parameter  `trim` allows us to compute _trimmed means_

- If `0 < trim < 0.5`, the function removes the `trim * n` smallest  and `trim * n` largest values in 
the sample, and return the mean of the remaining values 

- Depending on optional arguments, fucntion `mean` allows to compute either arithmetic means, or trimmed (arithmetic) means

- The hard work is performed by  `.Internal(mean(x))`  which calls code written in `Fortran` or `C`


---

### `r fontawesome::fa("hand-point-right")`

- In a _tidy_ sample, _missing values_ are explicitely pointed as 
missing information using special objects `NA`, `NA_integer_`, `NA_real_`, ...

- To compute summary statistics when facing missing values,  argument `na.rm` has to be `TRUE`

- What is the class of `NA`, `NA_integer_`, `NA_real_`?


- In messy tables, missing values are not spotted as `NA's`. Part of **data wrangling** consists in turning missing values 
(possibly indicated as `0`  or negative numbers) into explicit `NA's`

---

### `r fontawesome::fa("brain")`

Why is `var(sample)` defined by 

$$\sum_{i=1}^n \frac{1}{n-1} (x_i - \overline{X}_n)^2$$

when $\texttt{sample}=x_1, \ldots, x_n$?

---

### Sensitivity

Computing the mean and the standard deviation is less trivial than it looks like

Handling missing values is not the only issue

Another important issue is the high **sensitivity** of the mean (and of the standard deviation)
to the perturbation (modification) of a single sample point

If $x_i$ is replaced by $x_i + t$ (all other sample points remaining what they were), the empirical mean 
is shifted by $t/n$

If we take $t$ to infinity, we move the empirical mean to infinity

`r fontawesome::fa("skull")` Corruption of a single sample point may 
have a dramatic impact on the sample mean (and on the sample standard deviation)



---
template: inter-slide
name: robust

## Robust summary statistics

---

During the 1960s, [Peter Huber]() and colleagues developed alternatives to the  mean and  the standard deviation 
for assessing the location and the dospersion of a sample. These alternatives are collectively known as 
**robust estimators** for location and scale. 

Robust estimators of location retain nice properties of the mean,
however they are less sensitive to corruption of a couple of observations

Robust estimators of dispersion retain the shift invariance of the standard deviation, 
but prove less sensitive to data corruption


---

### Building robust (univariate) statistics

Constructions rely on **sample sorting**

**Order statistics** are obtained by  sorting the sample:

$$x_{1:n} \leq x_{2:n} \leq \ldots \leq x_{n:n}$$

Using an efficient comparison sorting method  (mergesort, quicksort, ...), sorting a collection of $n$ items require 
$O(n \log n)$ operations. This is slightly superlinear with respect to sample size $n$

???

$x_{i:n}$ reads as the $i^{\text{th}}$ item (in increasing order) in sample $x_1, x_2, \ldots, x_n$

---

### A robust location estimator : the **median**


.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

$$\text{Empirical median} = \begin{cases} x_{\lfloor n/2\rfloor+1:n} & \text{if } n\text{ is odd,}\\ 
(x_{n/2:n}+ x_{n/2+1:n})/2 & \text{if } n \text{ is even}\end{cases}$$

]

---

### A robust dispersion estimator : the Inter-Quartile Range (IQR)


- **Quartiles** 

  - First quartile $x_{\lfloor n/4\rfloor:n}$ 
  - Second quartile: the median! 
  - Third quartile: $x_{\lfloor 3 n/4\rfloor:n}$

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

- **IQR** : 

$$x_{\lfloor 3 n/4\rfloor:n} - x_{\lfloor n/4\rfloor:n}$$

]

???

The IQR is a robust dispersion index (why?)

Assuming that the median can be computing using a linear number of operations, check that it is possible to compute 
the three quartiles in linear time

---

### Property 

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

The median minimizes the average absolute deviation

]


Let $M$ be a median of some numerical sample $x_1, \ldots, x_n$ then 

$$\forall a \in \mathbb{R}, \qquad \frac{1}{n}\sum_{i=1}^n |x_i -M| \leq \frac{1}{n} \sum_{i=1}^n |x_i -a|$$

`r fontawesome::fa("brain")` check that!

???

Recall that the mean minimizes the average quadratic error 

---

### Connections between robust and ordinary statistics 


**Lévy's inequality**

Let $M$ denote the empirical median, $\overline{X}_n$ the empirical mean and $s_n$ the empirical standard deviation  

Then

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

$$|M - \overline{X}_n| \leq s_n$$

]

`r fontawesome::fa("brain")` check that!


---

### Other distinguished quantiles 

- **Quintiles** $x_{\lfloor k n/5\rfloor:n}$ pour $k \in 1, \ldots, 4$

- **Deciles** $x_{\lfloor k n/10\rfloor:n}$ pour $k \in 1, \ldots, 9$

- **Percentiles**

- ...




---

### `r fontawesome::fa("brain")`

Compute the derivative of 

- the mean

- the median 

- the trimmed mean 

with respect to $x_{n:n}$


???

`r fontawesome::fa("lightbulb")`

- the mean: $1/n$ with respect to $x_{n:n}$. Recall that $\overline{X}_n = \frac{1}{n} \sum_{i=1}^n x_{i:n}$

- the median: $0$ with respect to $x_{n:n}$. 

- the trimmed mean: $0$ with respect to $x_{n:n}$


---
exclude: true


Let $x_1, \ldots,  x_n$ be a numerical sample.

```{r}
rapportools::kurtosis
```

`r fontawesome::fa("hand-point-right")` Check that the kurtosis and the skewness are translation invariant

---

### Robust numerical summaries in `R`

Function `summary()` delivers robust statistics 

So does `skim()`

.f6[
```{r, warning=FALSE}
flights$arr_delay %>%   
  skimr::skim() %>% 
  select(1:10, -starts_with('skim')) %>% 
  knitr::kable(digits = 2,
  caption = "Summary statistics for nycflights13 arrival delay")
```
]


???

`numeric.p0, p25, ... ` denote the sample quartiles 

Notice that the median (`numeric.p50`) differs from the mean




---
template: inter-slide
name: boxplots

## Boxplots

---

Robust numerical summaries are complemented by plots

[Tukey](https://en.wikipedia.org/wiki/John_Tukey) introduced **boxplots** in the 1960's


---

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

In descriptive statistics, a box plot or boxplot is a method for graphically depicting groups of numerical data through their _quantiles_ (more specifically _quartiles_)

Box plots also have lines extending from the boxes (__whiskers__) indicating variability outside the upper and lower quartiles, hence the terms _box-and-whisker plot_ and box-and-whisker diagram

_Outliers_ may be plotted as individual points

Box plots are _non-parametric_: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length)

The spacings between the different parts of the box indicate the degree of _dispersion_ (spread) and _skewness_ in the data, and show _outliers_


.tr[
[Wikipedia](https://en.wikipedia.org/wiki/Box_plot)
]

]

---

> Tukey's boxplot assumes symmetry for the whiskers and normality for their length

What does this mean?

Assume that we _boxplot_ the $\mathcal{N}(\mu, \sigma^2) \sim \mu + \sigma \mathcal{N}(0, 1)$ distribution instead of an empirical distribution.

Then:

- The median is equal to $\mu$ (because of the symmetry of $\mathcal{N}(0,1)$)

- The quartiles are $\mu - \sigma \Phi^{\leftarrow}(3/4)$ and $\mu + \sigma \Phi^{\leftarrow}(3/4)$.

- The IQR is $2 \sigma  \Phi^{\leftarrow}(3/4) \approx 1.35\times \sigma$


- The default length of the whiskers (under `geom_boxplot`) is
$1.5 \times \text{IQR}$

- For a Gaussian population, $1.5 \text{IQR} = 3 \sigma \Phi^{\leftarrow}(3/4)$

- In a Gaussian population, the probability that a sample point lies outside the whiskers is $2 \times (1 - \Phi(4 \times \Phi^{\leftarrow}(3/4))) \approx 7/1000$


---
class: top, left

### Boxplot of a Gaussian sample



```{r normal-boxplot-label, message=FALSE, warning=FALSE, out.width="70%", fig.show='hide'}
n <- 100
fake_data <- tibble(normal=rnorm(n),t=rt(n, df=3))
title <- stringr::str_c("Standard Gaussian sample, size : ", n)

fake_data %>%
  ggplot(mapping = aes(x=1L, y=normal)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 2, coef=1.5) + #<<
  ggtitle(title) +
  ylab("") +
  ggplot2::theme(
  axis.text.x = element_blank(),
  axis.ticks.x = element_blank()) 

summary(fake_data$normal)
```


.plot-callout[
![gauss-boxplot-label](`r knitr::fig_chunk("normal-boxplot-label", "png")`)
]


???

In order to  understand what boxplots do and how they are tied to Gaussian modelling, let us figure out what boxplot looks like 
when build first on an i.i.d. Gaussian sample and second, on an i.i.d. sample fron a heavy tailed distribution (the `t` distribution with 
3 degrees of freedom)

---

### Boxplot of a Gaussian sample

.fl.w-60.pa2[

![gauss-boxplot-label](`r knitr::fig_chunk("normal-boxplot-label", "png")`)

]

???

Playing with boxplots

- Tune whiskers lengths using `coef`

- Jitter to spot the whole sample 



---

```{r, echo=FALSE, eval=FALSE}
fake_data %>%
  ggplot(mapping = aes(x = 1L, y = normal)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 2, coef = 1.5) + #<<
  geom_jitter(width=.05, height=0, alpha=.25) +
  ggtitle(title) +
  ylab("") + 
  annotate("text", 
            xmin=1.5, 
            y=median(fake_data$normal) ,
            label="median") +
  annotate("text", 
            xmin=1.5, 
            y=quantile(fake_data$normal, .75) ,
            label="third quartile") +
  annotate("text", 
            xmin=1.5, 
            y=quantile(fake_data$normal, .25) ,
            label="first quartile") +
  ggplot2::theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```

---

If we _boxplot_ a Gaussian sample, on average, $4\%$ of the points lie outside the whiskers. Such points are called _extremes_

Deciding which points should be considered as extremes is partly a matter of taste

Boxplots provide a visual assessment of the departure of a sample from normality/Gaussianity

See [`geom_boxplot` documentation](https://ggplot2.tidyverse.org/reference/geom_boxplot.html)

???

Point out the distinction between outliers and extremes


---


### Boxplot of a non-Gaussian sample




The sampling distribution ( $t$-distribution with $3$ degrees of freedom) is _heavy-tailed_

Some sample points lie far away from the whiskers

```{r non-gauss-boxplot-label, fig.show='hide', out.width=.8}
title <- stringr::str_c("Student sample with 3 df, size : ", n)
ggplot(fake_data,
       aes(y=t, x=1L)) +
  geom_boxplot(outlier.shape=2, outlier.colour = 'red', outlier.alpha=.5, coef=1.5) + #<<
  ylab("") +
  ggtitle(title) +
  ggplot2::theme(
  axis.text.x = element_blank(),
  axis.ticks.x = element_blank()
  )
summary(fake_data$t)
```



.plot-callout[

![non-gauss-boxplot-label](`r knitr::fig_chunk("non-gauss-boxplot-label", "png")`)

]

---

### Boxplot of a non-Gaussian sample

.fl.w-60.pa2[

![non-gauss-boxplot-label](`r knitr::fig_chunk("non-gauss-boxplot-label", "png")`)

]

---

### Plotting two boxplots side by side



```{r two-boxplots, out.width=.7, fig.show='hide'}
n <- 1000   #<<

tibble(normal=rnorm(n), 
       t=rt(n, df=3)) %>%
  pivot_longer(cols=c(normal, t),  #<<
               names_to='Distribution') %>%  #<<
  ggplot() + #<<
  aes(x=Distribution, y=value) +   #<<
  geom_boxplot(outlier.alpha = .5,   #<<
               outlier.colour = 'red', # nolint 
               outlier.shape = 2) +
  ggtitle("Gaussian versus heavy-tailed samples")
```




.plot-callout[

![two boxplots](`r knitr::fig_chunk("two-boxplots", "png")`)


]

???

Elaborate on `pivot_longer()` from `tidyr` package 

---

### Plotting two boxplots side by side (cont'd)


![two boxplots](`r knitr::fig_chunk("two-boxplots", "png")`)


---

### Boxplots for normal and heavy-tailed distributions

With usual whiskers length

- In Gaussian samples: 
  + very few sample points outside the whiskers (few extremes/outliers), 
  + extremes lie close to the whiskers endpoints

- In heavy-tailed distributions: 
  + many sample points lie outside the whiskers,  
  + some extremes may lie far far away

---

### Boxplots for delays from `nycflights13`




```{r boxplots-delays, out.width=.7, fig.show='hide'}
flights %>% 
  select(ends_with("_delay")) %>% 
  pivot_longer(
    cols=ends_with("_delay"),
    names_pattern = "(.*)_delay", 
    values_to = "delay"
    ) %>% 
  filter(abs(delay) < 120) %>% 
  ggplot() +
  aes(x=name, y=delay) +   #<<
  geom_boxplot() +    #<<
  ggtitle("Nycflights13: delays")
```






.plot-callout[

![](`r knitr::fig_chunk('boxplots-delays', 'png')`)

]

???

Play with `pivot_longer()` 

What is the result of ?

```
flights %>%
  select(ends_with("_delay"))
```

What is the result of ? 

```
flights %>%
  select(ends_with("_delay")) %>%
  pivot_longer(
    cols = ends_with("_delay"),
    names_pattern = "(.*)_delay",
    values_to = "delay"
  )
```

See [https://tidyr.tidyverse.org/reference/pivot_longer.html](https://tidyr.tidyverse.org/reference/pivot_longer.html)

See [https://r4ds.had.co.nz/tidy-data.html?q=pivot_longer#pivoting](https://r4ds.had.co.nz/tidy-data.html?q=pivot_longer#pivoting)



---


.fl.w-60.pa2[
```{r flip-pivot, eval=FALSE}
flights %>%
  select(ends_with("_delay")) %>%
  pivot_longer(
    cols = ends_with("_delay"),
    names_pattern = "(.*)_delay",
    values_to = "delay"
  ) %>%
  filter(abs(delay) < 120) %>% 
  glimpse(width=50)
```
]


.fl.w-40.pa2[

```{r flip-pivot-out, ref.label="flip-pivot"}

```
]


---

### Pivot at work

`r chunk_reveal("flip-pivot")`

---

### `r fontawesome::fa("plane-departure")` Boxplots for delays from `nycflights13` (cont'd)

![](`r knitr::fig_chunk('boxplots-delays', 'png')`)


???

What if we want the boxplot of departure delays to be on the left side of the plot?

Do you believe that arrival delays are affine transformations of departure delays? 

---
template: inter-slide
name: histograms

## Histograms

---

In many circumstances, the sample is not faithfully described by the sequence of quartiles

Boxplots are not always enough

If the data have been obtained by i.i.d. sampling from some distribution, the *empirical probability 
distribution* defined by the sample provides with a summary of the sample

Describing the empirical distribution as a _summary_ may seem to be far-fetched. Nevertheless, there is less 
information in the empirical distribution than in the sample: we have lost the order in which sample items
were collected 

A (possibly empirical) probability distribution is a complex mathematical object.
Fortunately, probability  distributions on the real line can be characterized by *functions* on the real line: the *cumulative distribution function* (CDF), its (generalized) inverse, the *quantile function*, and, in special circumstances, a *density function*  


---

### `r fontawesome::fa("syringe")` CDFs, densities, empirical distributions

Let $P$ be a probability distribution on the real line

The associated **CDF** is defined by $F(z) = P(-\infty,z]$ for $z \in \mathbb{R}$

The CDF $F$ is *absolutely continuous* iff there exists an (integrable) function $f$ such that 

$$F(z) = \int_{(-\infty, z]} f(x) \mathrm{d}x$$

If the CDF (and the probability $P$) is absolutely continuous, $f$ is said to be a *density* for distribution 
$P$ (a density with respect to Lebesgue measure)

`r fontawesome::fa("exclamation-triangle")` Empirical distributions are not absolutely continuous 

Nevertheless, we start with *histograms*, piecewise constant density functions that are used to portray 
empirical distributions! 







---

### Definition(s)

.fl.w-40.pa2[


`r fontawesome::fa("hand-point-right")` A **histogram** defines a piecewise constant probability density function

A histogram is defined by the number and position of the **breaks** (bins)


A histogram may be **regular** or not 

]

.fl.w-60.pa2[

```{r, echo=FALSE}
spl <- rnorm(1000, mean = 2)
```

```{r histogram-gauss, echo=FALSE}
tibble(x=spl) %>% 
  ggplot() +
  aes(x=x) +
  geom_rug(alpha=.1) +
  geom_histogram(aes(y=..density..), 
                 alpha=.5, 
                 fill="white", 
                 colour="black", 
                 bins=30) +
  stat_function(linetype="dashed", 
                fun=dnorm, 
                args=list(mean=2, sd=1)) +
  annotate("text", x=4, y=.3, 
           label="Gaussian density with mean 2") +
  ggtitle("Histogram for Gaussian sample") +
  labs(caption="Sample size=1000, 30 bins")
```



]

???

The plot on the right is made of three `geom_` 

- the histogram (to be defined)
- the dashed line, that is the density of the Gaussian distribution the sample is drawn from
- the rug, the collection of thin vertical segments below the `x` axis, that sketches the empirical distribution 

Here, we have a regular histogram, with 30 bins (31 breaks)

---



`r chunk_reveal("histogram-gauss")`


???

TODO 

---

A histogram is defined by 

- a sample $x_1, \ldots, x_n$, 

- a collection of *breaks*
$b_0 < b_1 < \ldots < b_k$ with $b_0 \leq \min (x_1, \ldots, x_n)$ and $b_k> \max(x_1, \ldots x_n)$

- a convention about continuity at breaks. Here, we assume right continuity


The histogram is a piecewise constant function $h$ that is zero outside $[b_0, b_k)$
and satisfies 

$$h(z) = \sum_{i=1}^k \left(\frac{\sum_{j=1}^n \mathbb{I}_{[b_{i-1},b_i)}(x_j)}{n(b_{i} -b_{i-1})}\right) \mathbb{I}_{[b_{i-1},b_i)}(z)$$



---

A histogram is said to be *regular* if the breaks are equally spaced ($b_i - b_{i-1}$  does not depend on $i$)

If the histogram is regular, then for every $i \in 1, \ldots, k$, $h(z)$ is proportional 
to $P_n [b_{i-1}, b_i)$ for $b_{i-1} \leq z < b_i$



---

### Construction of histograms

Building the histogram amounts to binning the sample values. 

Building a *useful* histogram  requires a number of decisions:

  + Picking the right (?) number of bins

  + Placing the breaks (regularly or not)
  



---

### Histograms for (trimmed) delay distributions

.fl.w-40.pa2.f6[
```{r  histograms-arr_delay, eval=FALSE}
flights %>% 
  select(
    ends_with("_delay")
    ) %>% 
  pivot_longer(
    cols=ends_with("_delay"),
    names_pattern = "(.*)_delay", 
    values_to = "delay"
    ) %>% 
  filter(
    abs(delay) < 120
    ) %>% 
  ggplot() +
  aes(x=delay, #<<
      y=..density..,  #<<
      fill=name) +   #<<
  geom_histogram(alpha=.3,  #<< 
                 bins=30)  #<<
```
]

.fl.w-60.pa2.f6[

```{r histograms-arr_delay-out, echo=FALSE, ref.label="histograms-arr_delay"}

```

]

???

Overlayed histograms show how the distributions of departure and arrival delays differ

---

### Impact of binwidth on histogram

```{r}
smpl <- runif(1000)
tbl <-  tibble(x=smpl)
```

```{r histos_bins, eval=FALSE}
tbl %>% 
  ggplot() +
  aes(x=x) +
  geom_rug() +
  geom_histogram(aes(y=..density..), 
                 bins=10, alpha=.2) +
  geom_histogram(aes(y=..density..), 
                 bins=30, alpha=.2, linetype=2, color="blue") +
  geom_histogram(aes(y=..density..), 
                 bins=100, alpha=.2, linetype=3, color="red") +
  labs(title="Sample from the uniform distribution",
       caption="1000 points, 10, 30, 100 bins")
  
```

???

The density of the population distribution is constant and equal to $1$ on $[0,1]$

The empirical distribution is sketched by the rug plot below the $x$-axis

Regular histograms with different binwidths are overlayed on the picture. 

When using $100$ bins, the histogram displays suprious peaks. 


---

### Impact of binwidth on histogram (step by step)


`r chunk_reveal("histos_bins")`


???


---

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[


]




---
template: inter-slide
name: ecdf

## Cumulative distribution function

---

### `r fontawesome::fa("syringe")` CDF

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

$P$ be a probability distribution over $\mathbb{R}$ (endowed with the
Borelian $\sigma$-algebra)

The cumulative distribution function (CDF) $F$ is a function from $\mathbb{R}$ to $[0,1]$

$$F(x) = P(-\infty, x)$$

]


---

A real-valued sample defines a probability distribution, called the _empirical distribution_.
Each sample point is assigned a probability equal to its relative frequency.


The cumulative distribution function associated with this empirical distribution
is called the _empirical cumulative distribution function_ (ECDF).

---

Let $x_1, \ldots, x_n$ denote the points from a real sample (they may not be pairwise
distincts), let $P_n$ be the empirical distribution and $F_n$ be the ECDF.

$$\begin{array}{rl}  P_n(A) & = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{x_i \in A}  \\ F_n(z) & = \frac{1}{n} \sum_{i=1}^n \mathbb{I}_{x_i \leq z} \end{array}$$

---

### Computing the ECDF 

Let $x_{1:n} \leq x_{2:n} \leq \ldots \leq x_{n:n}$ denote the _order statistics_
of the sample (that is the non-decreasing rearrangement of sample, with possible
repetitions).


The ECDF is easily computed from the order statistics:


$$F_n(z) = \begin{cases} 0           & \text{if } z < x_{1:n}               \\ \frac{k}{n} & \text{if } x_{k:n} \leq z < x_{k+1:n}\\ 1           & \text{if } x_{n:n} \leq z \end{cases}$$


Once the sample is sorted, computing the ECDF boils down to binary search `r fontawesome::fa("glass-cheers")`


---

Recall that ECDFs are determined  by order statistics (and conversely)


.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

### Proposition

The empirical cumulative distribution function $F_n$ is

- right-continuous over $\mathbb{R}$

- non-decreasing

- takes values in $\{ k/n , k \in \{0, 1, \ldots, n\}\}$

- is constant between consecutive order statistics

]



---

![Geyser](./img/pexels-adeline-man-11877707.jpg)

---

### Example

Any numerical dataset may be used to illustrate ECDFs.


For example,

`faithful`: a data frame with `272` observations on `2` variables concerning waiting time between eruptions and the duration of the eruption for the _Old Faithful_ geyser in Yellowstone National Park, Wyoming, USA.

- `eruptions`:	`numeric`,	Eruption time in minutess

- `wating`:	`numeric`	Waiting time to next eruption (in minutes)

> A closer look at `faithful$eruptions` reveals that these are heavily rounded times originally in seconds, where multiples of 5 are more frequent than expected under non-human measurement.


---

###  A glimpse at the faithfull data 

```{r, echo=TRUE}
data(faithful)
faithful %>%
  glimpse()
```



---



### Plotting an empirical CDF

.fl.w-third.pa2[

Eruptions durations

Under the hood, `ggplot` combines `geom_step`  and `ecdf` statistic


```{r  geyser-label, echo=TRUE, fig.show='hide'}
faithful %>%
  ggplot() +
  aes(x=eruptions) +
  geom_step(stat="ecdf") +   #<<
  ylab("ECDF") +
  xlab("Eruptions duration in minutes") +
  ggtitle("Old Faithful Geyser Data")
```

]

.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk('geyser-label', 'png')`)

]

---
exclude: false

### Computing the ECDF statistic


.fl.w-third.pa2[

ECDF plots may be used to compare two samples or to compare an empirical
cumulative distribution function with the cumulative distribution function
of some well-known probability distribution.

```{r  geyser-label-2, echo=TRUE, fig.show='hide'}
Fn <- ecdf(faithful$eruptions) #<<

x_grid <- seq(min(faithful$eruptions)-1,
              max(faithful$eruptions)+1,
              0.01)

df <- data_frame(x=x_grid,
                 Fn=Fn(x_grid))

df %>%
  ggplot() +
  aes(x=x, y=Fn) +
  geom_step(direction="hv")
```

]


.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk('geyser-label-2', 'png')`)

]


---
template: inter-slide
name: quantile-function

## Quantile function

---
exclude: false


.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

The _quantile function_ of a probability distribution over $\mathbb{R}$ is
the generalized, left-continuous inverse function of the cumulative
distribution function.

For $p \in (0,1)$

$$F_n^{\leftarrow}(p) = \inf \Big\{ z : F_n(z) \geq p\Big\}$$

$$F_n^{\leftarrow}(p) =  X_{k+1:n} \qquad \text{if } \qquad\frac{k}{n} < p \leq \frac{k+1}{n}$$

]


---


.fl.w-third.pa2[

In order to plot the quantile function, it is almost enough to exchange the
role of the axes in the ECDF plot.

```{r  eruptions-label, echo=TRUE, fig.show='hide'}
faithful[1:10,] %>%
  ggplot() +
  aes(x=eruptions) +
  geom_step(stat="ecdf", 
            direction='vh') +
  coord_flip() +   #<< 
  xlab("Quantiles") +
  ylab("Probabilities") +
  ggtitle("Old Faithful Geyser Data")
```

Why is it safe to set the `direction` argument in the call to `geom_step()`

]


.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk("eruptions-label", "png")`)


]


---
exclude: false

### Comparing sample quantiles with distributional quantiles

`r fontawesome::fa("bullhorn")`


When it comes to comparing the empirical distributions defined by two samples,
_quantile-quantile plots_ ( `qqplot` ) are favored.


- $F_n$ and $G_m$ : two ECDFs

- the quantile-quantile function is defined by

$$G_m^\leftarrow \circ F_n(z) = Y_{k+1:m}\qquad \text{if} \qquad \frac{k}{m}< F_n(z) \leq \frac{k+1}{m}$$


---


If $G_m = F_n$ (if the two samples are equal up to a permutation), then

$$F_n^\leftarrow \circ F_n(z) = X_{k+1:n} \text{if} \qquad X_{k:n} < z \leq X_{k+1:n}$$

The quantile-quantile plot lies above line $y=x$ and meets this line at every
unique order statistics.

`r fontawesome::fa("grin")` The departure of the quantile-quantile function from identity reflects
the  differences between   the two empirical distribution functions.

We can for example compare the first and second half of the `faithfull`
dataset.

---

### Defining an empirical quantile function

.pull-left[

Function `equantile()` returns a function

The returned function is _parametrized_ by argument
`sample`

.small[

- A function captures (encloses) the _environment_ in which it is created

- Function `equantile` creates a new _execution environment_ every time it is run. This environment is usually ephemeral, but here it becomes the _enclosing environment_ of the manufactured function

- The _enclosing environment_ of the manufactured function is an _execution environment_ of the function factory

]


]

.pull-right[

#### A quantile function factory

```{r}
equantile <- function(sample){
  sample <- sort(sample)
  m <- length(sample)

  function(p){     #<<
    ks <- pmax(1, ceiling(m * p)) #<<
    sample[ks]  #<<
  }
}


```

See [Advanced R programming](https://adv-r.hadley.nz/function-factories.html)
]

]


---

### QQ plot

.fl.w-40.pa2.f6[

```{r qq_delays, eval=FALSE}
not_cancelled <- flights %>% 
  filter(!is.na(arr_delay)) %>% 
  sample_n(100) %>% 
  arrange(dep_delay) %>% 
  filter(abs(arr_delay) < 120) 
  

nr <- nrow(not_cancelled)


not_cancelled %>% 
  ggplot() +
  aes(x=dep_delay, 
      y=quantile(arr_delay, 
                 seq(1, nr)/nr)
      ) +
  geom_point(size=.1) +
  coord_fixed() +
  geom_abline(slope=1, 
              intercept = 0, 
              alpha=.5) +
  ggtitle("QQ plot clipped arr_delay versus dep_delay") +
  ylab("arr_delay") +
  xlab("dep-delay")
```
]


.fl.w-60.pa2[

```{r qq_delays-out, echo=FALSE, ref.label="qq_delays"}

```


]


---
exclude: true

We can repeat this experiment after sample shuffling.

.fl.w-third.pa2[

```{r shuffle-faithful-label, echo=TRUE, fig.show='hide'}
unfaithful <- faithful[sample(seq(1,nrow(faithful)),
                               size=nrow(faithful),
                               replace = FALSE),
                        ]

first <- unfaithful[1:135,]
second <- unfaithful[-(1:135),]

Fn <- ecdf(first$eruptions)

Qm <- equantile(second$eruptions)

df <- data.frame(x= seq(min(first$eruptions)-1,
                        max(first$eruptions)+1, 0.01)) %>%
  dplyr::mutate(y = Qm(Fn(x)))

df %>%
  ggplot(mapping = aes(x=x, y=y)) +
  geom_step(direction='vh') +
  geom_abline(slope=1,
    intercept = 0,
    linetype='dashed')

```
]

.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk('faithful-shuffle-label','png')`)

]

---
exclude: true 

```{r, faithful-split-label-bis, echo=TRUE, fig.show='hide'}
first <- faithful[1:135,]
second <- faithful[-(1:135),]

Fn <- ecdf(first$eruptions)
Qm <- equantile(second$eruptions)

df <- data.frame(x= seq(min(first$eruptions)-1, max(first$eruptions)+1, 0.01)) %>%
  dplyr::mutate(y = Qm(Fn(x)))


df %>%
  ggplot() +
  aes(x=x, y=y) +
  geom_step(direction='vh') +
  geom_abline(slope=1,
              intercept = 0,
              linetype='dashed')

```


---

```{r child='closing_slide.Rmd'}

```
