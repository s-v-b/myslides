---
title: "Multilinear Regression II: Algorithm(s)"
subtitle: "EDA Master I, MIDS & MFA"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/11/20 (updated: `r Sys.Date()`)"
params:
  curriculum: "Master I MIDS & EDA"
  coursetitle: "Analyse Exploratoire de Données"
  lecturer: "Stéphane Boucheron"
  homepage: "http://stephane-v-boucheron.fr/courses/eda/"
  curriculum_homepage: "https://master.math.univ-paris-diderot.fr/annee/m1-mi/"
output:
  xaringan::moon_reader:
    css: ["header-footer.css",  "xaringan-themer.css", "custom-callout.css"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: inter-slide
class: left, middle, inverse

{{ content }}

---
name: layout-general
layout: true
class: left, middle

```{r setup, child="loaders_fixers.Rmd", echo=FALSE, message=FALSE, warning=FALSE}
```

```{r echo=FALSE}
# xaringanExtra::use_scribble()
```


---
template: inter-slide

# Multilinear Regression II: Formulae and Algorithm(s)


#### [`r params$curriculum`](`r params$homepage_curriculum`)


#### [`r params$coursetitle`](`r params$homepage`)

#### [`r params$lecturer`](http://stephane-v-boucheron.fr)




---
template: inter-slide

## `r fontawesome::fa("map", fill="white")`

### Motivations

### Ordinary Least Squares

### Closed-form Formulae

### `lm`  and `tidyverse`

### QR Factorization

### OLS and Pseudo-inversion

---
template: inter-slide
name: motivations

## Motivations

???


---

### Principled description of OLS

The approach to (multiple) linear regression we are following is called
Ordinary Least Squares (OLS)

Multiple Linear Regression consists of

- Picking a dataframe with `n` rows,  one _response_ variable `Y`, and _explanatory_ variables (also named _covariates_)

- Use the (possibly categorical) covariates columns to build a _design matrix_ `Z` with `n` rows and `p` columns according to a _formula_ `Y ~ ...`

- State the Least Square Problem : find $\beta \in \mathbb{R}^p$ that minimizes

$$\left\| Y - Z \times \beta \right\|^2$$

Call the (possibly chosen) solution $\widehat{\beta}$

- Run _diagnostics_
  + check whether Good Of Fit criteria are trustable
  + spot outliers




---

### Convention

- $\mathcal{M}_{n,p}$ denotes the set of real matrices with $n$ rows and $p$ columns

- Assumption $n > p$ (classical regime)

- Vectors are assumed to be column vectors (matrices with 1 column)

- If $A$  is a matrix, $A^T$ is the transpose of $A$

- The design matrix $Z$ is in  $\mathcal{M}_{n,p}$

- The response vector $Y$ is in $\mathbb{R}^n$ (equivalently $\mathcal{M}_{n,1}$)

- The parameter space is $\mathbb{R}^p$ (equivalently $\mathcal{M}_{p,1}$)


---

### Ordinary Least Squares

Ideally we would like to find $\beta \in \mathbb{R}^p$ such that

$$Y =  Z \times \beta$$

This system of linear equations is usually not solvable

Instead we look for $\beta$ that minimizes the Least Square criterion

$$\left\| Y - Z \times \beta \right\|^2$$


---
template: inter-slide
name: closed-form-formulae

## Closed-form formulae


---

### Solving the OLS problem

- Geometric approach

- Analytic approach

???


---

### Geometric approach

- $\mathcal{L}(Z)$ linear subspace of $\mathbb{R}^n$ generated by the columns of $Z$

- $\Pi$ : $n \times n$ matrix associated with orthogonal projection of $\mathbb{R}^n$ on $\mathcal{L}(Z)$

- $\widehat{Y} = \Pi \times Y$ projection of $Y$ on $\mathcal{L}(Z)$

- $\widehat{\epsilon} = Y - \widehat{Y} = (\mathrm{Id} - \Pi)\times Y$ projection of $Y$ on on $\mathcal{L}(Z)^\bot$

???

Assume for a while that we already know how to compute $\widehat{Y}$

---

### Pythagorean formulae

$$\begin{array}{rl}\left\| Y - Z \beta \right\|^2
& = \left\| Y - \widehat{Y} + \left(\widehat{Y} - Z \beta\right) \right\|^2\\
& = \left\| Y - \widehat{Y}\right\|^2 + 2 \left\langle Y - \widehat{Y}, \widehat{Y} - Z \beta \right\rangle + \left\|\widehat{Y} - Z \beta \right\|^2\\
& = \left\| Y - \widehat{Y}\right\|^2 + \left\|\widehat{Y} - Z \beta \right\|^2\end{array}$$

as

$$Y - \widehat{Y} \in \mathcal{L}(Z)^\bot \quad\text{while}\quad \widehat{Y} - Z \beta \in \mathcal{L}(Z)$$

---

### Exploiting the Pythagorean formula

$$\left\| Y - Z \beta \right\|^2 = \underbrace{\left\| Y - \widehat{Y}\right\|^2}_{\text{depends on }Y, Z} + \underbrace{\left\|\widehat{Y} - Z \beta \right\|^2}_{\text{to be optimized}}$$

OLS boils down to find some $\beta \in \mathbb{R}^p$  that solves

$$\widehat{Y} = Z \beta$$

--

Two cases:

- The columns of $Z$ are linearly independent:  $Z$ has full column rank

- The columns of $Z$ are _not_ linearly independent


---


### Proposition

.bg-light-gray.b--dark-gray.ba.br3.shadow-5.ph4.mt5[


Matrix $Z \in \mathcal{M}_{n,p}$ has full column rank $p$

iff

$Z^T \times Z \in \mathcal{M}_{p,p}$ is invertible

]

---

### Proof

$(\Rightarrow)$ Let $Z$ have full column rank.

Assume $Z^T \times Z$ is not invertible (proof by contradiction)

--

Then there exists a _non-null_ vector $u \in \mathbb{R}^p$ such that

$(Z^T \times Z) \times u = 0$ with $0 \in \mathbb{R}^p$

--

Thus

$0 = \langle u , 0\rangle = u^T \times (Z^T \times Z) \times u = \langle Z\times u, Z \times u\rangle = \left\| Z \times u\right\|^2$

which implies $Z \times u= 0$

--

If $Z$ has full-column rank, $Z \times u = 0$ implies $u=0$, a contradiction


---

### Proof (continued)

$(\Leftarrow)$ Let $Z \in \mathcal{M}_{n,p}$ have column rank $< p$

There exists $u \in \mathbb{R}^p \setminus \{0\}$ such that $Z \times u = 0 \in \mathbb{R}^n$

So

$Z^T \times Z \times u = Z^T \times 0 =  0 \in \mathbb{R}^p$

which implies that that $Z^T \times Z$ is not invertible

`r fontawesome::fa("square")`

---

###  OLS solution


.bg-light-gray.b--dark-gray.ba.br3.shadow-5.ph4.mt5[

$$\begin{array}{rl}
\widehat{Y} =  Z \times \beta
& \Rightarrow Z^T \times \widehat{Y} = \left(Z^T \times Z\right) \times \beta\\
& \Leftrightarrow \left(Z^T \times Z\right)^{-1}\times Z^T \times \widehat{Y} = \beta
\end{array}$$



$$\widehat{\beta} = \left(Z^T \times Z\right)^{-1}\times Z^T \times \widehat{Y}$$

]


---

### Computing the projection

We keep assuming $Z$ has full column rank ( $p$ )

### Definition: Hat matrix

$$H = Z \times \left(Z^T \times Z\right)^{-1} \times Z^T$$

$H \in \mathcal{M}_{n,n}$

???


---

### Proposition

.bg-light-gray.b--dark-gray.ba.br3.shadow-5.ph4.mt5[

Assumption: $Z$ has full column rank

The Hat matrix $H = Z \times \left(Z^T \times Z\right)^{-1} \times Z^T$ coincides with the matrix $\Pi$ associated with the orthogonal projection of $\mathbb{R}^n$ on the linear subspace $\mathcal{L}(Z)$ generated by the  columns of $Z$

]

---

### Proof

It is enough to check that

1. if $u \in \mathcal{L}(Z)^\bot$ $H \times u =0$
1. if $u \in \mathcal{L}(Z) \subseteq \mathbb{R}^n$ $H \times u =u$

--



1.) Assume $u \in \mathcal{L}(Z)^\bot$, then $u$ is orthogonal to any column of $Z$, or equivalently to any row of $Z^T$, which implies $Z^T \times u = 0 \in \mathbb{R}^p$ and $H \times u =  Z \times \left(Z^T \times Z\right)^{-1} \times Z^T u =0$

---

### Proof (continued)

2.) Assume $u \in \mathcal{L}(Z)$

Note that $H \times u$ is a linear combination of the columns of $Z$, hence $v = H \times u \in \mathcal{L}(Z)$

Observe $Z^T\times v = Z^T \times  H \times u =Z \times \left(Z^T \times Z\right)^{-1} \times Z^T u  = Z^T \times u$

Hence $Z^T \times (v- u) = 0 \in \mathbb{R}^p$

$v-u \in \mathcal{L}(Z)$ and $v-u$ is orthogonal to any column of $Z$ (row of $Z^T$ )

This shows $v - u = 0 \in \mathbb{R}^n$.

Hence $H \times u = u$ for $u \in \mathcal{L}(Z)$


`r fontawesome::fa("square")`


---

### Using the Hat matrix

$$\widehat{Y} = H \times Y =   Z\times \left(Z^T \times Z\right)^{-1} \times Z^T \times Y= Z \times \widehat{\beta}$$

$$\widehat{\epsilon} =  Y - Z \times \widehat{\beta}$$


---

### Analytic approach


$$\beta \to f(\beta) = \left\| Y - Z \times \beta \right\|^2$$

is a smooth convex function of $\beta$

The smooth convex function $f$ achieves its minimum where its gradient  $\nabla f$ vanishes

$$f(\beta) = \left\langle Y - Z \times \beta , Y - Z \times \beta \right\rangle= \left\| Y\right\| -2 \left\langle Z^T \times Y,\beta \right\rangle + \left\langle Z \times \beta, Z\times \beta\right\rangle$$

The gradient is

$$\nabla f = -2 Z^T \times Y + 2  \times Z^T \times Z\times \beta$$

The gradient vanishes for

$$\beta = \left( Z^T \times Z \right)^{-1} \times Z^T \times Y$$


---
template: inter-slide

## OLS and `lm` objects

---

`r flipbookr::chunk_reveal("tidy_anscombe", break_type="auto", title="### Tidy Anscombe dataset")`


```{r tidy_anscombe, include=FALSE}
anscombe <- datasets::anscombe

anscombe %>%
  tidyr::pivot_longer(everything(),
    names_to = c(".value", "group"),
    names_pattern = "(.)(.)"
  )  %>%
  rename(X=x, Y=y) %>%
  arrange(group)-> anscombe

anscombe
```

---


`r flipbookr::chunk_reveal("lm_1_anscombe", break_type="auto", title="### First subset ")`


```{r lm_1_anscombe, include=FALSE}
anscombe %>%
  filter(group=="1") %>%
  lm(Y ~ X, data=.) -> lm_1

lm_1
```


---

### `r fontawesome::fa("broom")` Brooming objects of class `lm`

[Broom package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

.panelset[

.panel[.panel-name[`broom::tidy`]

```{r}
lm_1 %>%
  broom::tidy()
```
- Column `Estimate` contains estimated coefficients of $\widehat{\beta}$
- Each row matches a column of design $Z$ ( $Z$ may differ from input dataframe)


]

.panel[.panel-name[`broom::augment`]

```{r}
lm_1 %>%
  broom::augment() %>%
  head()
```
- column `.fitted` contains the projections $\widehat{Y}$
- column `.fitted`  contains the residuals
- column `.hat` cointains the diagonal coefficients of the Hat matrix
- `.cooksd` and `.std.resid` are diagnostic tools

]

.panel[.panel-name[`broom::glance`]

```{r}
lm_1 %>%
  broom::glance()
```

The only contains contains information pertaining to model selection

Column `sigma` contains the standard error of the residuals :
$\|\widehat{\epsilon}\|/\sqrt{n-p}$

`r fontawesome::fa("hand-point-right")` pay attention to normalization

`adj.r.squared` estimates the share of explained variance

`AIC` and `BIC` are information criteria that help assessing the relevance of a model with respect to simpler models

]


]


???

> While model inputs usually require tidy inputs, such attention to detail doesn’t carry over to model outputs. Outputs such as predictions and estimated coefficients aren’t always tidy. This makes it more difficult to combine results from multiple models. For example, in R, the default representation of model coefficients is not tidy because it does not have an explicit variable that records the variable name for each estimate, they are instead recorded as row names. In R, row names must be unique, so combining coefficients from many models (e.g., from bootstrap resamples, or subgroups) requires workarounds to avoid losing important information. This knocks you out of the flow of analysis and makes it harder to combine the results from multiple models. I’m not currently aware of any packages that resolve this problem.


---
template: inter-slide
name: qr-factorization

## Algorithms: QR Factorization

---

### Two flavors

1. Direct methods

2. Iterative methods

---

### A naive attempt

In order to compute the pseudo-inverse $Z^{+} = \left(Z^T \times Z\right)^{-1} \times Z^T$, we  might use the following recipe:

1. Compute $Z^T \times Z$. This is just matrix transposition and matrix multiplication `t(Z) %*% Z`.
1. Compute the Cholesky decomposition of $L \times L^T = Z^T \times Z$ (where $L$ is lower-triangular and invertible if $Z^T \times T$ is): `chol(t(Z) %*% Z)`. If invertible, $L$ is easy to inverse.
1. Invert $L$ and plug inverse in $(Z^T \times Z)^{-1} = (L^{-1})^T \times L^{-1}$
1. $(Z^T \times Z)^{-1} \times Z^T = (L^{-1})^T \times L^{-1} \times Z^T$.


---


This is not what `r fontawesome::fa("r-project")` does.

Rather `r fontawesome::fa("r-project")` relies on the so-called QR-factorization of $Z$:

$$Z = Q \times R$$

where

- $Q \in \mathcal{M}_{n,p}$ has pairwise orthogonal columns
with unit norm  ( $Q^T \times Q = \operatorname{Id}_p$ ) and

- $R$ is upper-triangular with positive
diagonal (assuming again that $Z$ has full column rank).



---

### Using QR factorization

From this factorization we readily obtain
the Cholesky decomposition of $Z^T \times Z$:

$$Z^T \times Z =  \big(R^T \times Q^T \big) \times \big(Q \times R) =  \underbrace{R^T}_{\text{lower triagular}} \times \underbrace{R}_{\text{upper triangular}}$$

and $(Z^T \times Z)^{-1} \times Z^T$  reads as:

$$R^{-1} \times (R^{-1})^T \times R^T \times Q^T = R^{-1} \times Q^T$$

--

$$H = Q \times R \times (R^T \times R)^{-1} \times R^T \times Q^T = Q\times Q^T$$

---

### QR factorization on Anscombe dataset
```{r}
anscombe_1 <- filter(anscombe, group=="1") %>% select(X,Y)
Z <- cbind("I"=rep(1, 11), anscombe_1$X) %>% as.matrix()

n <- nrow(Z); p <- ncol(Z)
```
```{r}
qr.Z <- qr(Z)
Q.Z <- qr.Q(qr.Z) # Extraction of Q
R.Z <- qr.R(qr.Z) # Extraction of R
Ip <- diag(1, p, p)

norm(t(Q.Z) %*% Q.Z - Ip) # Q has orthogonal column

piv.Z <- solve(R.Z, Ip) %*% t(Q.Z)  # pseudo-inverse, see later
```

---

### Computing coefficients

```{r}
piv.Z %*% as.matrix(anscombe_1$Y)
```

### Computing fitted values

```{r}
H <- Q.Z %*% t(Q.Z)

H %*% as.matrix(anscombe_1$Y)

piv.Z %*% H %*% as.matrix(anscombe_1$Y)
```


---
template: inter-slide

## OLS and rank-defficient designs

---

### `r fontawesome::fa("question")` If the design does not have full column rank?

This is equivalent to the fact that there are infinitely many
solutions to the linear system

$$Z \times \beta = \widehat{Y}$$

We choose the solution with minimal Euclidean norm

We solve the following problem:

$$\text{Minimize}\quad \left\| \beta \right\|^2 \quad\text{under constraint}\quad Z \times \beta = \widehat{Y}$$

We minimize a smooth convex function under linear constraint

---
exclude: true
### Moore-Penrose pseudo-inverse of $Z$

The pseudo-inverse $Z^{+}$ is defined by

$$Z^{+} \times Z = \operatorname{Id}_p \qquad\text{and} \qquad Z \times Z^{+} = \Pi_{\mathcal{L}(Z)}$$


???

Existence and unicity ?

---
exclude: true
### QR factorization and pseudo-inversion




---
exclude: true
### OLS (all in one)

- Coefficients
$$\widehat{\beta} =  Z^{+} \times Y$$

- Predictions
$$\widehat{Y} = Z \times Z^{+} \times Y$$

- Residuals
$$\widehat{\epsilon} = \left( \mathrm{Id} - Z \times Z^{+}\right) \times Y$$



---
exclude: true

### Regularized least squares criterion

For each $\lambda>0$, the penalized least square costs is defined by

$$\Big\Vert Y - Z \beta  \Big\Vert^2 + λ \big\|\beta\big\|^2$$

Gradient with respect to $\beta$

$$2\times\big(λ β + Z^T Z β - Z^T Y\big)$$

Unique optimum

$$\big(\lambda \operatorname{Id} + Z^TZ\big)^{-1} Z^T Y$$



---

```{r child="closing_slide.Rmd"}

```
