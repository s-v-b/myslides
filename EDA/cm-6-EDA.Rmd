---
title: "EDA VI : PCA"
subtitle: "Statistiques Master I, MFA et MIDS"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/12/11 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["header-footer.css", "default", "rutgers-fonts", "hygge"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: layout-general
layout: true
class: left, middle

<style>
.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: red;
}
</style>


```{r xaringanExtra, echo=FALSE}
xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons", "logo"))
```


```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons(minified = FALSE)
```

```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_logo(
  image_url = "./img/Universite_Paris_logo_horizontal.jpg",
  position = xaringanExtra::css_position(top = "1em", right = "1em"),
  width = "110px",
  link_url = "http://master.math.univ-paris-diderot.fr/annee/m1-mi/",
  exclude_class = c("hide_logo")
)

xaringanExtra::use_panelset()

xaringanExtra::use_editable(expires = 1)

source("./loaders_fixers.R")

knitr::opts_chunk$set(fig.width = 6,
                      message = FALSE,
                      warning = FALSE,
                      comment = "",
                      cache = F)

pacman::p_load(flipbookr)
pacman::p_load(FactoMineR)
pacman::p_load(FactoInvestigate)
pacman::p_load(tidyverse)
```


---
class: middle, center, inverse



# Exploratory Data Analysis VI: PCA

### `r Sys.Date()`

#### [EDA Master I MIDS et MFA](http://stephane-v-boucheron.fr/courses/eda)

#### [Stéphane Boucheron](http://stephane-v-boucheron.fr)

---
class: middle, inverse

## `r fontawesome::fa("map", fill="white")`

### Approximating a sample matrix

### Bivariate case

### Singular Value Decomposition

### Eckart-Young Theorems

###

---



```{r, echo=FALSE, message=FALSE}
iris <- datasets::iris
```

---

## Approximating a sample matrix

---

### A dimension reduction technique

Principal Component Analysis (PCA) is an ancient dimension reduction technique from statistics that may be motivated from different perspectives:

- maximizing projected variance

- minimizing reconstruction error

---

### The big picture

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

> The central idea of principal component analysis (PCA) is to reduce the
dimensionality of a data set which consists of a large number of interrelated
variables, while retaining as much as possible of the variation present in the
data set.

> This is achieved by transforming to a new set of variables, the
principal components (PCs), which are uncorrelated, and which are ordered
so that the first few retain most of the variation present in all of the original
variables.

.tr[Jolliffe 1986]

]

---

### PCA: under the hood

PCA can be performed using

- _spectral decomposition_ of a data covariance (or correlation) matrix or

- _singular value decomposition_ of a data matrix,

possibly after a normalization step of the initial data.

The main device is a _matrix factorization method_ known as Singular Value Decomposition (SVD)

---

## The bivariate case


---
exclude: true


---
class: center, middle, inverse

## Singular value decomposition

---

The next theorem tells us that any real-valued matrix can be _factorized_
into the product of two orthogonal matrices and a diagonal matrix

This _factorization_ is at the core of many results and methods in computational
matrix analysis.

From a computational viewpoint, Principal Component Analysis (PCA) is
another name for Singular Value Decomposition (SVD)

---



### Théorème: Singular Value Decomposition Theorem

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Let $X$ be a $n \times p$ real matrix.

There exists $U, D, V$

$$X =  U \times D \times V^T \qquad\text{with} \begin{cases}  U \times U^T & = \operatorname{Id}_n \\  V \times V^T & = \operatorname{Id}_p \\  D & \text{non-negative diagonal.}\end{cases}$$

]

--


- The columns of $U$  are called _left singular vectors_.

- The columns of $V$ are called the  _right singular vectors_.

- The diagonal coefficients of $D$ are called the _singular values_.

---

### `r fontawesome::fa("hand-point-right")`

The SVD decomposition is not unique.

$M := \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$  is orthogonal but it is not a rotation

$$\begin{array}{rl}\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} & =  \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \times\begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \times \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}\\ & = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \times \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \times \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix} \\ &  =  \ldots\end{array}$$



---
exclude: true

```{r fancy_svd, echo=FALSE, include=FALSE, eval=FALSE}
M <- matrix(c(0,1,1,0), nrow=2)
knitr::kable(data.frame(list("sing values"=svd(M)$d, "eigenvalues"= eigen(M)$values)))
```

---

### Proposition

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

The number of positive (non-zero) singular values equals the rank of $X$

]

---

### Definition :  Thin SVD

Let $X$ be a $n \times p$ real matrix with rank $r$.

Let  $U, D, V$ be a singular value decomposition of $X$ such that the diagonal
entries of $D$ are non-increasing.

Let $U_r$ (resp. $V_r$) be the $n \times r$ (resp. $p \times r$) matrix
formed by the first $r$ columns of $U$ (resp. $V$)

Then

$$X =  U_r \times D_r \times V_r^T$$

is a _thin Singular Value Decomposition_ of $X$.

---

```{r, echo=FALSE, include=FALSE, eval=FALSE}
A <- matrix(rnorm(12, 1), nrow = 4)

foo <- svd(A)

U <- foo$u

D <- foo$d

V <- foo$v

xtable::xtableMatharray(A)
xtable::xtableMatharray(U)
xtable::xtableMatharray(diag(D))
xtable::xtableMatharray(t(V))
```

---

### Example

.f6[

In `r fontawesome::fa("r-project")`, the SVD of matrix $A$ is obtained by `svd_ <- svd(A)`
with `U <- svd_$u`, `V <- svd_$V` and `D <- diag(svd_$d)`


$$\begin{array}{rl}& \underbrace{\begin{bmatrix}  3.33 & 0.77 & 2.33 \\  0.98 & 1.94 & 1.71 \\
  -0.74 & -0.60 & 1.06 \\  1.65 & 0.17 & 2.90 \\  \end{bmatrix}}_{A} \\ &= \underbrace{\begin{bmatrix}
  -0.71 & 0.28 & -0.48 \\  -0.42 & 0.27 & 0.86 \\
  -0.02 & -0.75 & 0.16 \\  -0.56 & -0.54 & -0.04 \\ \end{bmatrix}}_{U} \times
\underbrace{\begin{bmatrix}  5.66 & 0.00 & 0.00 \\ 0.00 & 1.87 & 0.00 \\ 0.00 & 0.00 & 1.58 \\ \end{bmatrix}}_{D}  \times \underbrace{\begin{bmatrix} -0.65 & -0.25 & -0.71 \\ 0.48 & 0.59 & -0.65 \\  -0.59 & 0.76 & 0.27 \\  \end{bmatrix}}_{V^T}\end{array}$$

]

---

During the proof of the SVD Theorem, we rely on the next matrix norm.


### Definition : norme d'opérateur

$$\|\mathbf{M} \|_{\text{op}} :=  \sup_{u \in \mathbb{R}^n  \|u\|\leq 1}\sup_{v \in \mathbb{R}^p  \|v\|\leq 1} u^T \mathbf{M} v = \sup_{v \in \mathbb{R}^p  \|v\|\leq 1} \| \mathbf{M} v \|$$


---


### Proof of full SVD existence

**Induction on $\max(n,p)$**

If $\max(n,p)=1$ nothing to do. The matrix is its own SVD.


Assume SVD exists for any matrix $n\times p$ with $\max(n,p)\leq m$}

**Convention.**
Let $\mathbf{Z}$  be an $n\times p$ matrix with $\max(n,p)=m+1$ and rank $r\leq \min(n,p)$.

If $\|\mathbf{Z}\|_{\text{op}}=0$,
 $\operatorname{rank}(\mathbf{Z})=0, \mathbf{Z}=\mathbf{0}$ and there is nothing to prove

If $\|\mathbf{Z}\|_{\text{op}}>0$,
 by compactness of unit ball $B^p_2(1)$ in $\mathbb{R}^p$, there exist unit
vectors $\widehat{u} \in \mathbb{R}^n, \widehat{v} \in \mathbb{R}^p$,
$$\|\mathbf{Z}\|_{\text{op}} = \widehat{u}^T \mathbf{Z} \widehat{v} = \sup_{u \in B_2^n(1), v\in B^p_2(1) } u^T \mathbf{Z} v$$

---

### Proof of full SVD existence (continued)

- Let $\sigma_1 = \|\mathbf{Z}\|_{\text{op}}$

- Let $\mathbf{A}$, $\mathbf{B}$  be matrices such that

$$\begin{bmatrix} \widehat{u} & \vdots  & \mathbf{A} \end{bmatrix} \qquad \text{and}\qquad \begin{bmatrix} \widehat{v} & \vdots & \mathbf{B} \end{bmatrix}$$

be *orthognal* matrices with dimensions $n\times n$ and $p\times p$


---

### Proof of full SVD existence (continued)

Decomposing matrices into blocks:

$$\begin{bmatrix}\widehat{u}^T\\ \mathbf{A}^T\end{bmatrix} \times \mathbf{Z} \times \begin{bmatrix} \widehat{v} & \mathbf{B}\end{bmatrix} =  \begin{bmatrix}\sigma_1 & w^T  \\ \mathbf{0} & \mathbf{A}^T \mathbf{Z} \mathbf{B}\end{bmatrix} =: \mathbf{Y}$$

$\mathbf{A}^T \mathbf{Z} \widehat{v} = \mathbf{0}$ since

- $\mathbf{Z} \widehat{v}$ is colinear with $\widehat{u}$ and

- the columns of $\mathbf{A}$ are orthogonal to $\widehat{u}$

---

### Proof of full SVD existence (continued)


Now, we have to check that $w=0$

Because multiplying by orthogonal matrices does not change operator norm $\|\mathbf{Y}\|_{\text{op}}=\|\mathbf{Z}\|_{\text{op}} = \sigma_1$

$$\|\mathbf{Y}\|_{\text{op}} \geq \frac{\left\|\mathbf{Y} \begin{bmatrix}\sigma_1 \\ w \end{bmatrix}\right\|}{\left\|\begin{bmatrix}\sigma_1 \\ w \end{bmatrix}\right\|} \geq \frac{\sigma_1^2 +w^Tw}{\sqrt{\sigma_1^2 +w^Tw}} = \sqrt{\sigma_1^2 +w^Tw}$$

In order to have $\sigma_1 \geq \|\mathbf{Y}\|_{\text{op}}$, we need to have $w=\mathbf{0}$

Matrix $\mathbf{A}^T \mathbf{Z} \mathbf{B}$ (with rank $r-1$)
satisfy the induction hypothesis ($\max(n-1,p-1)\leq m$)

---

### Proof of full SVD existence (continued)

$\rightarrow$ There exist  orthogonal matrices $\mathbf{U}'$  and $\mathbf{V}'$
such that ${\mathbf{U}'}^T \mathbf{A}^T \mathbf{Z} \mathbf{B} \mathbf{V}'$ is equal to a diagonal non-negative matrix $\mathbf{D}'$ with $r-1$ non null-coefficients

This entails

$$\begin{bmatrix} 1 & \mathbf{0} \\ \mathbf{0} &  \mathbf{U'}^T\end{bmatrix} \times \begin{bmatrix}
\widehat{u}^T\\\mathbf{A}^T\end{bmatrix} \times \mathbf{Z} \times \begin{bmatrix} \widehat{v} & \mathbf{B}\end{bmatrix} \times \begin{bmatrix}1 & \mathbf{0} \\ \mathbf{0} &  \mathbf{V}'\end{bmatrix}  = \begin{bmatrix}\sigma_1 & \mathbf{0} \\ \mathbf{0} & \mathbf{D}'\end{bmatrix}$$

Matrices

$$\begin{bmatrix} 1 & \mathbf{0} \\ \mathbf{0} &  \mathbf{U'}^T \end{bmatrix} \times \begin{bmatrix} \widehat{u}^T\\ \mathbf{A}^T \end{bmatrix}\qquad \text{and} \qquad \begin{bmatrix} \widehat{v} & \mathbf{B}\end{bmatrix} \times \begin{bmatrix} 1 & \mathbf{0} \\ \mathbf{0} &  \mathbf{V}'\end{bmatrix}$$

are orthogonal with dimensions $n\times n$ and $p\times p$.

`r fontawesome::fa("square")`

---

### Take-home message  `r fontawesome::fa("bulkhorn")`


The SVD theorem is a factorization theorem:

it allows to break up a matrix into simpler objects.

There is however something special about SVD beyond factorization.

SVD delivers a sequence of best approximations with given rank.

Moreover, these low-rank approximations are simulataneously optimal with respect to two norms.

This is the content of the _Eckart-Young Theorem_

---
class: middle, center, inverse

## Eckart-Young Theorem

---

### Landscape : Two relevant norms on $\mathcal{M}_{n,p}$ the set of $n\times p$ matrices

The first relevant norm is the _operator norm_ defined above

The second norm is the _Hilbert-Schmidt-Frobenius norm_

---

### Definition: Hilbert-Schmidt inner product and norm `r fontawesome::fa("syringe")`


.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

$$\langle \mathbf{A}, \mathbf{B} \rangle_{\text{HS}} :=  \operatorname{Trace}(\mathbf{A} \mathbf{B}^T) \qquad \| \mathbf{A} \|_{\text{HS}}^2 :=  \langle \mathbf{A}, \mathbf{A} \rangle_{\text{HS}} = \sum_{i=1}^n\sum_{j=1}^p \mathbf{A}_{i,j}^2$$

- $\langle \cdot, \cdot \rangle_{\text{HS}}$ is an inner product (symmetric, bilinear, positive)

- $\|\cdot \|_{\text{HS}}$ is a norm (satisfies triangle inequality, positive, scaling property)

]

---

### Theorem: Optimality of Truncated SVD

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

The _truncated SVD_ leads to the best rank-$k$ approximation
with respect to the Hilbert-Schmidt norm and the operator norm

]


---


### A basis for $\mathbb{R}^{n \times p}$



### Proposition

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Let $u_1, \ldots, u_n$ and $v_1,\ldots,v_p$
form two orthonormal bases of $\mathbb{R}^n$ and $\mathbb{R}^p$,
then $(u_i v_j^T)_{i\leq n, j\leq p}$ forms an orthonormal
basis of $\mathbb{R}^{n\times p}$

]

---

### Proof

$$\begin{array}{rcl}\langle u_i v_j^T, u_k v_\ell^T\rangle_{HS} &= &  \operatorname{Trace}\left( u_i v_j^T v_\ell u_k^T\right) \\  &= & \langle v_j, v_\ell\rangle  \operatorname{Trace}\left( u_i  u_k^T\right) \\  &= & \langle v_j, v_\ell\rangle \times \langle u_i, u_k\rangle \end{array}$$

`r fontawesome::fa("square")`


---


### Corollary

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

The left and right singular vectors in the full SVD of an $n\times p$ matrix form an orthonormal basis of $\mathbb{R}^{n\times p}$ endowed with Hilbert-Schmidt norm

]

---

### Proof

**Best approximation in Hilbert-Schmidt norm (proof)**

Assume $n\geq p$ (w.l.o.g.),

From the fat SVD for $\mathbf{Z}$, we have

$$\mathbf{Z} =  \sum_{i=1}^p \sigma_i u_i v_i^T\qquad \text{where } \sigma_1\geq \ldots \geq \sigma_r > \sigma_{r+1} = \ldots = \sigma_p= 0$$

Let

$$\mathbf{W} = \sum_{i\leq n,j\leq p} \mu_{i,j} u_i v_j^T$$

$$\left\| \mathbf{Z} - \mathbf{W} \right\|_{HS}^2 = \sum_{i=1}^p  (\sigma_i^2 - \mu_{i,i})^2+ \sum_{i=1}^n\sum_{j=1}^p \mathbb{I}_{i\neq j}  \mu_{i,j}^2$$

- Any minimizer should satisfy $\mu_{i,j}=0$ for $i\neq j$

- $\sum_{i\leq p} \mu_{i,i} u_iv_i^T$ has rank $k$ iff
exactly $k$ coefficients $\mu_{i,i}$ are non zero

- The squared distance is minimized by picking $\mu_{i,i}=\sigma_i$
for $i\leq k$ and $\mu_{i,i}=0$  for $k+1 \leq i \leq p$

---

**Best approximation in operator norm**

The SVD of $\mathbf{Z}-\mathbf{Z}_k$ is

$$\mathbf{U}\times  \operatorname{diag}\left(\begin{array}{c} 0 \\ \vdots\\ 0 \\ \sigma_{k+1}\\ \vdots \\ \sigma_p \end{array} \right)\times  \mathbf{V}^T$$

The operator norm of the difference is equal to $\sigma_{k+1}$

Now, let $\mathbf{W}$  be of rank $k$. $\operatorname{ker}(\mathbf{W})$ has dimension $p-k$,
is intersects the linear span of the first $k+1$ columns of $\mathbf{V}$.

Let $y$ be a unit vector in this intersection. By definition $\mathbf{W}y=0$.

$$\| \mathbf{Z}y \|^2 = \sum_{i=1}^{k+1} \sigma_i^2 \langle v_i, y_i \rangle^2 \geq \sigma^2_{k+1}$$

which proves that $\| \mathbf{Z} - \mathbf{W}\|_2 \geq \sigma_{k+1}$


`r fontawesome::fa("square")`

---

## Two perspectives on PCA (and SVD)

---

### Projecting the data on a $k$ dimensional space in a faithfull way

Let $X$ be a $n \times p$ matrix, a reasonable task consists in finding
a $k \lll p$-dimensional subspace $E$ of $\mathbb{R}^p$ such that

$$X \times \Pi_E$$

is as close as possible from $X$. By as close as possible, we mean that the sum
of squared Eudlidean distances between the rows of $X$ and
the rows of $X \times \Pi_E$ is as small as possible

Picking $E$ as the subspace generated by the first $k$ right-singular vectors of $X$
solves the problem

---

### Maximizing the projected variance

---
class: inverse, middle, center

## PCA and SVD

---

From the `r fontawesome::fa("r-project")` documentation of  `prcomp`:

> The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using `eigen` on the covariance matrix. This is generally the preferred method for numerical accuracy. The `print` method for these objects prints the results in a nice format and the plot method produces a _scree plot_.


At first glance, performing Principal Component Analysis
consists in applying SVD to the $n  \times p$ data matrix.

---

### Conventions

| PCA vocabulary | SVD vocabulary |
|:---------------:|:----------------:|
| Principal axis | Left singular vector |
| Principal component |                  |
| Factorial axis | Right singular vector |
|                 | Singular value |

---

### Modus operandi

1. Read the data.

2. Choose the active individuals and variables (cherrypick amongs numeric variables)

3. Choose if the variables need to be standardised

4. Choose the number of dimensions you want to keep

5. Analyse the results

6. Automatically describe the dimensions of variability

7. Back to raw data.

---
exclude: true

### `dplyr` programming

.panelset[

.panel[.panel-name[Challenge]

Our goal is to write tidy functions for standardizing datasets

Standardizing consists in transforming numerical columns in a systematic way

Standardizing may consist in mapping the numerical columns on a specific range, says `[0,1]`
or centering and scaling so that the empirical variance is `1`

Standardizing in an interactive way is easy

```{r, eval=FALSE}
df %>%
  mutate(across(where(is.numeric), standardization formula))
```
where standardization formula may be something like
```{r, eval=FALSE}
~ (. - min(.))/(max(.)-min(.))
```

`r fontawesome::fa("frown")` this construct does not work well inside our own functions

]



.panel[.panel-name[Data masking and tidy selection challenge]

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[[

Data masking and tidy selection make interactive data exploration fast and fluid, but they add some new challenges when you attempt to use them indirectly such as in a for loop or a function. This vignette shows you how to overcome those challenges. We’ll first go over the basics of data masking and tidy selection, talk about how to use them indirectly, and then show you a number of recipes to solve common problems. .tr[https://dplyr.tidyverse.org/articles/programming.html]

]
]

.panel[.panel-name[Code]
```{r}
standardize <- function(df)
  mutate(df, across(where(is.numeric),  ~ .x /sd(.x)))

standardize_classic <- function(df)
  sweep(df,
        2,
        STATS = apply(df, 2, sd),
        FUN = "/")

center <- function(df)
  mutate(df, across(where(is.numeric), ~ .x - mean(.x)))

scale <- function(df)
  mutate(df, across(where(is.numeric), ~ (.x - mean(.x))/sd(.x)))

scale(iris)
```
]
]
---

### Microbenchmarking


.panelset[

.panel[.panel-name[Benchmarking]

Thanks to `microbenchmark`, we can figure out the computational efficiency
of both approaches to standardization

]
.panel[.panel-name[Code]

```{r}
pacman::p_load(microbenchmark)

microbenchmark(sweep(iris[,-5], 2, STATS = apply(iris[,-5], 2, sd), FUN = "/")) # sweep based

microbenchmark(mutate(iris, across(where(is.numeric), ~ (. - mean(.))/ sd(.))))  # dplyr based
```

]

]

On the `iris` dataset, the  `sweep` approach is  faster


---

### Impact of standardization and centering

When performing PCA, once the _active variables_ have been chosen,
we wonder whether we should centre and/or standardize the columns

--

Centering does not modify the spectrum

Centering shifts the data and hence the components

--

`r fontawesome::fa("hand-point-right")` PCA investigates the spread of the data not their location

`r fontawesome::fa("question-circle")` To standardize or not to standardize?

There is no universal answer

Centering is performed by default when using `prcomp`

---

### Scaling: back to IRIS

Standardizing the columns slightly spreads out the eigenvalues

.fl.w-50.pa2[

```{r}
iris %>%
  scale()  %>% 
  select(-Species) %>%
  prcomp() %>%
  broom::tidy(matrix="pcs")
```

]

.fl.w-50.pa2[

```{r}
iris %>%
  select(-Species) %>%
  prcomp() %>%
  broom::tidy(matrix="pcs")
```

]

---

###



<!-- TODO: combine the screeplots ... -->

---


## Computing SVD

---

### Proposition

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Computing the SVD is as hard as computing the spectral decomposition of a symmetric matrix

]


---

### Proof

If we feed a symmetric matrix $A$ to an SVD algorithm,
we obtain $U, D, V$ such that $A = U \times D \times V^T$.

Then,  we also have

$$A =  \frac{1}{2} \left((U+V) \times D \times (U+V)^T \right)$$

which defines a spectral decomposition of $A$  `r fontawesome::fa("glass-cheers")`

---

### Proof (converse part)

If we have an eigendecomposition algorithm, we can feed it with

- $A \times A^T$ to get the left singular vectors and
- $A^T \times A$ to get the right singular vectors.

Both spectral decompositions lead to the squared singular values

If we count the number of field operations, both problems are linearly equivalent
`r fontawesome::fa("glass-cheers")`

`r fontawesome::fa("square")`

---
### Power method

We focus on the symmetric eigenvalue problem

---
### Rearrangement methods

---
class: center, middle, inverse

## Visualizing PCA

---

### `r fontawesome::fa("pagelines")` [`iris` flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)

A showcase for Principal Component Analysis

.f6[

> The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula "all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus".

> The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.

.tr[Wikipedia]

]

---

### Inspecting the correlation matrix

```{r}
iris %>%
  dplyr::select_if(is.numeric) %>%
  corrr::correlate() %>%
  corrr::shave() %>%
  corrr::fashion() %>%
  knitr::kable(format="markdown")
```

--

### Look at the singular values/eigenvalues.

```{r, results='asis'}
svd(iris[, 1:4])$d

iris_pca <- iris %>%
  select( -Species ) %>%
  prcomp()

iris_pca %>%
  broom::tidy(matrix="pcs") %>%
  knitr::kable(format="markdown", digits=2)

iris_plus_pca <- cbind(iris, iris_pca$x)

round((100*cumsum(iris_pca$sdev^2)/sum(iris_pca$sdev^2))[1], 2)
```

---

### Screeplot


```{r, out.width="50%", fig.show='hide'}
share_variance <- round((100*(iris_pca$sdev^2)/sum(iris_pca$sdev^2)), 2)

iris_plus_pca %>%
  ggplot(aes(x=PC1, y=PC2, colour=Species)) +
  geom_point() +
  ggtitle("Iris data projected on first two principal axes") +
  xlab(paste("PC1 (", share_variance[1], "%)", sep="")) +
  ylab(paste("PC2 (", share_variance[2], "%)", sep=""))


iris_plus_pca %>%
  ggplot(aes(x=Sepal.Length, y=Petal.Length, colour=Species)) +
  geom_point()
  ggtitle("Iris data projected on Petal and Sepal length")
```

---

### Automatic plotting of PCA

```{r}
# autoplot(prcomp(iris[,-5]), data = iris, colour = 'Species')
```


See [`plot_pca`](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_pca.html)

---

### Overplotting the orginal variables in the factorial plane

```{r, eval=FALSE}
autoplot(prcomp(iris[,-5]),
         data = iris,
         colour = 'Species',
         loadings = TRUE,
         loadings.label = TRUE)
```

---

### Screeplot

.panelset[

.panel[.panel-name[Code]

```{r ns-scree}
zpca <- prcomp(iris[, -5], center = FALSE, scale. = FALSE)

qplot(x=1:4, y=zpca$sdev^2, geom="col") +
  xlab("Principal components") +
  ylab("Variance") +
  ggtitle("Screeplot of non-standardized Iris dataset") -> p
```

]

.panel[.panel-name[Plot]

```{r, echo=FALSE, fig.height=4}
p
```
]

]
---


```{r}
iris[, -5] %>%
  prcomp(retx= TRUE, center=FALSE, scale. = FALSE) -> ir_pca

p <- as.data.frame(ir_pca$x, make.names = paste("PC", 1:4 ,sep=""))  %>%
  rownames_to_column() %>%
  tidyr::pivot_longer(cols = paste("PC", 1:4 ,sep=""),  names_to="Axis", values_to = "Coord") %>%
  ggplot(mapping=aes(x= Axis, y= Coord))

p + geom_boxplot() +
  ggtitle("PCA on Iris: Most of projected variance on the first two principal components")

p + geom_violin() +
  ggtitle("PCA on Iris: Most of projected variance on the first two principal components")

# round(ir_pca$sdev^2/sum(ir_pca$sdev^2), digits = 3)
```
---

### Screeplot


```{r, out.width="30%", fig.show='hide'}
iris[, -5] %>%
  FactoMineR::PCA() %>%
  plot(graph.type="ggplot")
```

---

```{r}

min_max_scaler <- function(df, cols){
  mutate(df, across(cols, .funs = ~ (. - min(.))/(max(.)-min(.))))
}

# min_max <- iris %>%
#   summarise(across(where(is.numeric), c(min, max)))

# t(min_max) %>%
#   as.data.frame() %>%
#   rownames_to_column() %>%
#   mutate(across(var_ = str_split(rowname, "_")[[1]][1],
#          fun_ = str_split(rowname, "_")[[1]][2])

iris <- datasets::iris

min_max_scaler(iris, names(iris)[-5]) %>%
  DT::datatable()
```

---
class: center, middle, inverse

## PCA in action

---
exclude: true

## References


@HorJoh90 and @Bha97 are classics on Matrix Analysis. Their content goes far beyond
Singular Value Decomposition. @elden2019matrix introduces matrix analysis in a data mining/machine
learning perspective. This book shows that many factorizations are relevant to machine learning.
Few of these factorizations are as well understood as SVD.

@MR1417720 is classic on matrix computations.


---

class: middle, center, inverse

background-image: url('./img/pexels-cottonbro-3171837.jpg')
background-size: cover


# The End
