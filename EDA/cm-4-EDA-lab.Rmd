---
title: "Multilinear Regression"
output:
  html_document
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(tidyverse)
```


> Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.

> `Temp`: Purportedly the average outside temperature in degrees Celsius.

> `Gas`: The weekly gas consumption in 1000s of cubic feet.

> Venables, W. N. and Ripley, B. D. (2002) _Modern Applied Statistics with S._ Fourth edition. Springer.


```{r load_whiteside}
whiteside <- MASS::whiteside

whiteside %>%
  dplyr::sample_n(5) %>%
  knitr::kable()
```

### Back to simple linear regression 


We perform again simple linear regression on `Whiteside` data

We look for $\beta_0, \beta_1$  that minimize the quadratic error

$$\sum_{i=1}^n \left( \mathrm{Gas}_i - \beta_0  -\beta_1 \mathrm{Temp}_i\right)^2$$

In the parlance of mathematical statistic, we proceed as if we were
fitting a _Gaussian Linear Model_ with _homoschedastic_ noise:

$$\begin{bmatrix} \mathrm{Gas}_1 \\
  \vdots \\  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix} 1 & \mathrm{Temp}_1 \\   \vdots & \vdots \\ 1 & \mathrm{Temp}_{56} \end{bmatrix} \times \begin{bmatrix}  \beta_0 \\ \beta_1
\end{bmatrix} + \sigma \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{56} \end{bmatrix}$$

where $\epsilon_i \sim_{i.i.d.} \mathcal{N}(0,1)$.

We aim at optimizing $\beta_0, \beta_1, \sigma>0$

### Invoking `lm()`

Evaluating

```{r lm0, eval=FALSE}
lm0 <- lm(Gas ~ Temp,
          data=whiteside)
```
returns an object `lm0` of class `lm` (for linear models)

An object of class `lm` is a _list_ of named objects

Among those objects,  `coefficients`, which can be retrieved either as

- `lm0$coefficients` or
- `coefficients(lm0)`

contains what we call the _Ordinary Least Square_ estimator of
the linear regression of `Gas`  versus `Temp`


The _Intercept_  $\widehat{\beta}_0$  and the _slope_  $\widehat{\beta}_1$ define the simple regression line

```{r}
lm00 <- lm(Gas ~ Temp, data=whiteside)

attributes(lm00)$names %>%
  knitr::kable()
```


Do it from scratch 

```{r}
Y <- whiteside %>% 
  pull(Gas)

X <- whiteside %>% 
  add_column(I=1, .before =  "Temp") %>% 
  select(I, Temp) %>% 
  as.matrix()

A <- t(X) %*% X

B <- solve(A, diag(c(1,1))) 

pinvX <- B  %*% t(X)  

H <- X %*% pinvX 
```

Check that $H$ is the projection onto the linear space spanned by the two columns of $X$. 

```{r}
# H leaves X invariant 
norm(H %*% X - X)
# H is idempotent 
norm(H %*% H - H)
# any vector orthogonal to the columns of X is in the kernel of H

```

```{r}
pinvX %*% Y
```

```{r}
coefficients(lm0)
```


### Simple linear regression of `Gas ~ Temp`

```{r whitesidesimple, echo=TRUE, message=FALSE, warning=FALSE}
p <- whiteside %>%
  ggplot() +
  aes(y=Gas, x=Temp) +
  geom_point(aes(shape=Insul)) +  # +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
 
p + 
  ggtitle("Whiteside data", subtitle = "simple linear regression") 
```

The sign of residuals is determined by `Insul`

This is not what we would expect if the connection between `Gas` consumption  and average outside temperature `Temp` were independent of insulation.

This modeling deserves to be challenged







### Multiple goals

- simple linear regression of `Gas` (weekly consumption) with respect to `Temp` (average external temperature) leaves us with mixed feelings 

- there is a need to incorporate the influence of `Insul` into our model

- there are few a priori reasons to believe that the relationship between gas consumption and external temperature  should be linear

- there is a need for methods to compare different models and ideally to select
one in a principled way `




### Simple linear regression per group

SLR per group is equivalent to fitting
a _multiple linear regression model_, (at least from a statistical viewpoint)

It is equivalent to fitting the next model

$$\begin{bmatrix}\mathrm{Gas}_1 \\ \vdots \\
  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix} 1 & 0 & \mathrm{Temp}_1 &  0\\
  \vdots & \vdots & \vdots & \vdots \\   1 & 0 & \mathrm{Temp}_{26} &  0\\ 0 & 1 & 0 & \mathrm{Temp}_{27} \\  \vdots & \vdots & \vdots & \vdots \\   0 & 1 & 0 & \mathrm{Temp}_{56}
\end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\ \beta_{0,a} \\ \beta_{1, b} \\ \beta_{1, a}
\end{bmatrix} + \sigma_b \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{26}\\ 0 \\ \vdots \\ 0 \end{bmatrix} + \sigma_a \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \epsilon_{27}\\ \vdots \\ \epsilon_{56} \end{bmatrix}$$


Like multiple linear regression with mildly _heteroschedastic_ noise

Noise standard deviation is assumed to be constant within each group

The residuals are now centered within each group

Visually, more appealing than simple linear regression


```{r whitesidesimplebygroup, echo=TRUE, warning=FALSE}
p + 
  aes(color=Insul, linetype=Insul) +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
  ggtitle("Whiteside data",
          subtitle = "simple linear regressions by groups") +
  theme(legend.position = c(0.1, .15))
```


### Subsetting with `lm()`

To perform linear regression of the subset of rows defined by `Insul == After`,
the next expression works:

```{r, echo=TRUE, eval=FALSE}
whiteside %>%
  lm(formula = Gas ~ Temp,
     data= .,
     subset= Insul=='After')
```

We use the pronoun `.` to connect the `data` argument of `lm` with the left-hand-side of the pipe `%>%`

This is necessary since `lm` belongs to base `R` and does not comply with `tidyverse` conventions


### One-hot encoding of `Insul`

We could also use the _design_ defined by formula `Gas ~ Temp * Insul`.

This amounts to use the `treatment` _contrast_ for encoding the two-valued factor `Insul` with `Before` as _control_ and `After` as _treatment_.

In modern machine learning language, this is called _one-hot encoding_ of qualitative variable `Insul`

In this modeling attempt, we envision interaction between `Insul` and `Intercept` and `Insul`  and `Temp`

```{r}
one_hot <- whiteside %>% 
  mutate(OneHot = as.integer(Insul=='After')) %>% 
  pull(OneHot) 
  
X_extended <- cbind(X, Insul=one_hot, `Insul:Temp`=one_hot* X[,"Temp"])

A_extended <-  t(X_extended) %*% X_extended

B_extended <- solve(A_extended, diag(rep(1,4)))

pInvX_extended <- B_extended %*% t(X_extended)

H_extended <- X_extended %*% pInvX_extended 

pInvX_extended %*% Y

coefficients(lm( Gas ~ Temp * Insul, whiteside))
```



### One-hot encoding of `Insul` (continued)

We investigate whether insulation is
 influencing the amount of gas used to make the house comfortable at temperature $0$ and influencing the sensitivity of weekly gas consumption to external temperature

This is at least physically plausible

$$\begin{bmatrix} \mathrm{Gas}_1 \\  \vdots \\  \mathrm{Gas}_{56}\end{bmatrix}  = \begin{bmatrix}  1 & 0 & \mathrm{Temp}_1 &  0\\  \vdots & \vdots & \vdots & \vdots \\  1 & 0 & \mathrm{Temp}_{26} &  0\\  1 & 1 & \mathrm{Temp}_{27} & \mathrm{Temp}_{27} \\  \vdots & \vdots & \vdots & \vdots \\ 1 & 1 & \mathrm{Temp}_{56} & \mathrm{Temp}_{56}\end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\  \beta_{0,a} \\  \beta_{1, b} \\  \beta_{1, a} \end{bmatrix} + \sigma \begin{bmatrix} \epsilon_1 \\ \vdots \\  \epsilon_{56} \end{bmatrix}$$


```{r, eval=FALSE, echo=TRUE}
whiteside %>%
  lm(formula = Gas ~ Temp * Insul, data = .) %>% 
  summary()
```

### Linear regression after one-hot encoding of `Temp`

```{r whitesideonehot,  warning=FALSE}
whiteside %>%
  ggplot(mapping=aes(y=Gas,
                     x=Temp,
                     color=Insul,
                     linetype=Insul)) +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
  geom_point(aes(shape=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "One-hot encoding of Insul") +
  theme(legend.position = c(0.1, .15))
```




### Formulae with arithmetic expressions



By taking into account interactions between `Temp` and `Insul`, we have improved
the appeal of our model. Now we face two questions:

1. can we go further and improve data fitting?

1. can we formally assess the apparent improvement delivered by a richer model?

One question concerns the possibility of describing richer models

The second question is about model assessment/validation/selection

The first question  is about programming, the second takes us in inferential statistics territory



### Building an enriched design


Rather than searching a _linear connection_
between `Gas` and `Temp` we might
as well look for a _polynomial connection_

In the next chunk, we add powers of `Temp` to dataframe `whiteside`

```{r, echo=TRUE}
poly(whiteside$Temp,
     degree=10,
     raw=TRUE) %>%
 as_tibble() -> whiteside_matrix

names(whiteside_matrix) <- paste("Temp",
                                 1:10,
                                 sep="^")

whiteside_10  <- cbind(whiteside,
                       whiteside_matrix)
```



We fit a degree $10$ polynomial to the data

```{r, echo=TRUE}
lm_10 <-
  whiteside_10 %>%
  lm(formula =
     Gas ~ . - Temp - Insul)
```

We are looking for a predictor of `Gas` based on powers of `Temp`


### A paradox


Attempting to fit a degree $10$ polynomial to the `Whiteside` data leads to:

- Goodness of fit is improved

- Plausibility, and thus explanatory power is degraded:
It is hard to envision a non-monotonous relationship between gas consumption and average external temperature


```{r whitesideten-label, echo=TRUE, warning=FALSE}
whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp)) +
  geom_point(aes(shape=Insul)) +
  geom_point(mapping = aes(y=lm_10$fitted.values), shape="+", size=5) +
  geom_smooth(formula= y ~ 1+ poly(x,10, raw=TRUE), method="lm", se=FALSE) +
  ggtitle('Whiteside data: fitting a degree 10 polynomial',
          'Predictions of gas consumption are not monotonous with respect to average external temperature') +
  theme(legend.position = c(0.1, .15))
```


### High order modeling with interaction

We can play that game further by fitting
a high-degree polynomial of `Temperature` with interaction with `Insulation`

Fitting a degree $10$ polynomial with interaction with `Insul` to the `whitesidedata`.

We attempt to fit a model with $22$ coefficients (degrees of freedom) to the `whiteside` data ( $56$ observations)

Goodness of fit is visually improved  but physical plausibility and explanatory power remain bad.


```{r whiteside10insul, echo=FALSE, warning=FALSE}
whiteside_10 %>%
  lm(formula = Gas ~ (. - Temp) * Insul) -> lm_10_insul

whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp, group=Insul)) +
  geom_point(aes(shape=Insul)) +
  geom_point(mapping = aes(y=lm_10_insul$fitted.values),
             shape="+", size=5) +
  geom_smooth(formula= y ~ 1+ poly(x,10, raw=TRUE),
              method="lm",
              se=FALSE,
              aes(linetype=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "Fitting a degree 10 polynomial with interaction with Insul") +
  theme(legend.position = c(0.1, .15))
```


### The best of both worlds




Fit a degree $2$ polynomial with interaction w.r.t. `Insul`

When fitting a degree $2$ polynomial, we face new questions:

- should we prefer quadratic modelling to linear modelling?
- shoud we retain interaction with `Insul` for both subgroups?

Visual inspection does not provides us with clear cut guidelines


```{r  whiteside2Insul, echo=FALSE, warning=FALSE}
lm_2 <- lm(Gas ~ Insul/(Temp + I(Temp^2))-1, whiteside)

whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp, group=Insul)) +
  geom_point(aes(shape=Insul, color=Insul)) +
  geom_point(mapping = aes(y=lm_2$fitted.values), shape="+", size=5) +
  geom_line(mapping = aes(y=lm_2$fitted.values, linetype=Insul)) +
  geom_smooth(formula= y ~ poly(x,2, raw=TRUE),
              method="lm",
              se=FALSE,
              aes(linetype=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "Fitting a degree 2 polynomial per group") +
  theme(legend.position = c(0.1, .15))
```

### GLM interpretation

Figure  corresponds to least-square fitting of the following model

$$\begin{bmatrix}\mathrm{Gas}_1 \\  \vdots \\  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix}  1 & 0 & \mathrm{Temp}_1 &  0 & \mathrm{Temp}^2_1 &  0\\  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\  1 & 0 & \mathrm{Temp}_{26} &  0 & \mathrm{Temp}^2_{26} &  0\\  1 & 1 & \mathrm{Temp}_{27} & \mathrm{Temp}_{27} & \mathrm{Temp}^2_{27} & \mathrm{Temp}^2_{27}  \\  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\   1 & 1 & \mathrm{Temp}_{56} & \mathrm{Temp}_{56} & \mathrm{Temp}^2_{56} & \mathrm{Temp}^2_{56} \end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\  \beta_{0,a} \\  \beta_{1, b} \\  \beta_{1, a} \\  \beta_{2, b} \\  \beta_{2, a}\end{bmatrix} + \sigma \begin{bmatrix}  \epsilon_1 \\  \vdots \\  \epsilon_{56}\end{bmatrix}$$

We still assume _homoschedastic_ noise



