---
title: "Multilinear Regression"
subtitle: "⚔<br/>EDA Master I, MIDS & MFA"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/11/20 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["header-footer.css",  "xaringan-themer.css", "custom-callout.css"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: layout-general
layout: true
class: left, middle

```{r loaders-fixers, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
xaringanExtra::use_animate_css()

xaringanExtra::use_tile_view()

xaringanExtra::use_tachyons(minified = FALSE)

xaringanExtra::use_logo(
  image_url = "./img/UniversiteParis_logo_horizontal_couleur_RVB.jpg",
  position = xaringanExtra::css_position(top = "1em", right = "1em"),
  width = "110px",
  link_url = "http://master.math.univ-paris-diderot.fr/annee/m1-mi/",
  exclude_class = c("hide_logo")
)

xaringanExtra::use_panelset()

xaringanExtra::use_editable(expires = 1)

source("./loaders_fixers.R")

knitr::opts_chunk$set(fig.width = 6,
                      message = FALSE,
                      warning = FALSE,
                      comment = "",
                      cache = F)
library(flipbookr)
```



<style>
.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: red;
}
</style>




---
class: middle, center, inverse

# Multilinear Regression

### `r Sys.Date()`

#### EDA Master I MIDS et MFA


---
class: inverse, middle

## `r fontawesome::fa("map", fill="white")`

### Revisiting simple linear regression

### Simple linear regression per group

### Formulae and model building

### `lm()`  as a statistical pipeline

### Tidy approach to modeling

### ANOVA




---
class: center, middle, inverse

## Revisiting simple linear regression

---

### House Insulation: Whiteside's Data  `r fontawesome::fa("igloo")`

.fl.w-third.pa2[

```{r load_whiteside}
whiteside <- MASS::whiteside

whiteside %>%
  dplyr::sample_n(5) %>%
  knitr::kable()
```
]


.fl.w-two-thirds.pa2[

Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.

`Temp`: Purportedly the average outside temperature in degrees Celsius.

`Gas`: The weekly gas consumption in 1000s of cubic feet.

Venables, W. N. and Ripley, B. D. (2002) _Modern Applied Statistics with S._ Fourth edition. Springer.

]

---

### Multiple goals

- simple linear regression of `Gas` (weekly consumption) with respect to `Temp` (average external temperature) leaves us with mixed feelings `r fontawesome::fa("frown")`

- there is a need to incorporate the influence of `Insul` `r fontawesome::fa("igloo")` into our model

- there are few a priori reasons to believe that the relationship between gas consumption and external temperature `r fontawesome::fa("temperature-low")` should be linear

- there is a need for methods to compare different models and ideally to select
one in a principled way `r fontawesome::fa("vials")`


---

`r fontawesome::fa("syringe")` We perform again simple linear regression on `Whiteside` data

We look for $\beta_0, \beta_1$  that minimize the quadratic error

$$\sum_{i=1}^n \left( \mathrm{Gas}_i - \beta_0  -\beta_1 \mathrm{Temp}_i\right)^2$$

In the parlance of mathematical statistic, we proceed as if we were
fitting a _Gaussian Linear Model_ with _homoschedastic_ noise:

$$\begin{bmatrix} \mathrm{Gas}_1 \\
  \vdots \\  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix} 1 & \mathrm{Temp}_1 \\   \vdots & \vdots \\ 1 & \mathrm{Temp}_{56} \end{bmatrix} \times \begin{bmatrix}  \beta_0 \\ \beta_1
\end{bmatrix} + \sigma \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{56} \end{bmatrix}$$

where $\epsilon_i \sim_{i.i.d.} \mathcal{N}(0,1)$.

We aim at optimizing $\beta_0, \beta_1, \sigma>0$

---

### Invoking `lm()`

Evaluating

```{r lm0, eval=FALSE}
lm0 <- lm(Gas ~ Temp,
          data=whiteside)
```
returns an object `lm0` of class `lm` (for linear models)

An object of class `lm` is a _list_ of named objects

Among those objects,  `coefficients`, which can be retrieved either as

- `lm0$coefficients` or
- `coefficients(lm0)`

contains what we call the _Ordinary Least Square_ estimator of
the linear regression of `Gas`  versus `Temp`

--

The _Intercept_  $\widehat{\beta}_0$  and the _slope_  $\widehat{\beta}_1$ define the simple regression line

```{r, include=FALSE, echo=FALSE}
lm00 <- lm(Gas ~ Temp, data=whiteside)

attributes(lm00)$names %>%
  knitr::kable()
```


---

### Simple linear regression of `Gas ~ Temp`

.fl.w-third.pa2[

`r fontawesome::fa("hand-point-right")` The sign of residuals is determined by `Insul`

This is not what we would expect if the connection between `Gas` consumption  and average outside temperature `Temp` were independent of insulation.

This modeling deserves to be challenged

]

.fl.w-two-thirds.pa2[

```{r whitesidesimple, echo=FALSE, message=FALSE, warning=FALSE}
whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp)) +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
  ggtitle("Whiteside data", subtitle = "simple linear regression") +
  geom_point(aes(shape=Insul))  # +
#  theme(legend_position=c(0.1, .15))
```
]


---

`r chunk_reveal("my_whiteside",
                break_type="auto",
                display_type="code",
                title="### Whiteside step by step")`

```{r my_whiteside, include = FALSE}
whiteside %>%
  ggplot() +
  aes(x = Temp) + #BREAK
  aes(y = Gas) + #BREAK
  geom_smooth(
    method="lm",
    formula = y ~ x,
    se=FALSE
  ) +  #BREAK
  geom_point(
    alpha = .8,
    size = 2,
    aes(color = Insul,
        shape = Insul),
    ) +  #BREAK
  ggtitle("Whiteside data: Simple Linear Regression") +
  labs(subtitle="Residuals should not be so predictable")
```

---

### Simple linear regression per group

SLR per group is equivalent to fitting
a _multiple linear regression model_, (at least from a statistical viewpoint)

It is equivalent to fitting the next model

$$\begin{bmatrix}\mathrm{Gas}_1 \\ \vdots \\
  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix} 1 & 0 & \mathrm{Temp}_1 &  0\\
  \vdots & \vdots & \vdots & \vdots \\   1 & 0 & \mathrm{Temp}_{26} &  0\\ 0 & 1 & 0 & \mathrm{Temp}_{27} \\  \vdots & \vdots & \vdots & \vdots \\   0 & 1 & 0 & \mathrm{Temp}_{56}
\end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\ \beta_{0,a} \\ \beta_{1, b} \\ \beta_{1, a}
\end{bmatrix} + \sigma_b \begin{bmatrix} \epsilon_1 \\ \vdots \\ \epsilon_{26}\\ 0 \\ \vdots \\ 0 \end{bmatrix} + \sigma_a \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \epsilon_{27}\\ \vdots \\ \epsilon_{56} \end{bmatrix}$$

---


### Simple linear regression per  group

.fl.w-third.pa2[

Like multiple linear regression with mildly _heteroschedastic_ noise

Noise standard deviation is assumed to be constant within each group

The residuals are now centered within each group

Visually, more appealing than simple linear regression


```{r whitesidesimplebygroup, echo=FALSE, warning=FALSE, fig.show='hide', fig.height = 5}
whiteside %>%
  ggplot(mapping=aes(y=Gas,
                     x=Temp,
                     color=Insul,
                     linetype=Insul)) +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
  geom_point(aes(shape=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "simple linear regressions by groups") +
  theme(legend.position = c(0.1, .15))
```

]

.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk("whitesidesimplebygroup", "png")`)

]


---

### Subsetting with `lm()`

To perform linear regression of the subset of rows defined by `Insul == After`,
the next expression works:

```{r, echo=TRUE, eval=FALSE}
whiteside %>%
  lm(formula = Gas ~ Temp,
     data= .,
     subset= Insul=='After')
```

`r fontawesome::fa("exclamation")`  We use the pronoun `.` to connect the `data` argument of `lm` with the left-hand-side of the pipe `%>%`

This is necessary since `lm` belongs to base `R` and does not comply with `tidyverse` conventions

---

### One-hot encoding of `Insul`

We could also use the _design_ defined by formula `Gas ~ Temp * Insul`.

This amounts to use the `treatment` _contrast_ for encoding the two-valued factor `Insul` with `Before` as _control_ and `After` as _treatment_.

In modern machine learning language, this is called _one-hot encoding_ of qualitative variable `Insul`

In this modeling attempt, we envision interaction between `Insul` and `Intercept` and `Insul`  and `Temp`

---

### One-hot encoding of `Insul` (continued)

We investigate whether insulation is
 influencing the amount of gas used to make the house comfortable at temperature $0$ and influencing the sensitivity of weekly gas consumption to external temperature

This is at least physically plausible

$$\begin{bmatrix} \mathrm{Gas}_1 \\  \vdots \\  \mathrm{Gas}_{56}\end{bmatrix}  = \begin{bmatrix}  1 & 0 & \mathrm{Temp}_1 &  0\\  \vdots & \vdots & \vdots & \vdots \\  1 & 0 & \mathrm{Temp}_{26} &  0\\  1 & 1 & \mathrm{Temp}_{27} & \mathrm{Temp}_{27} \\  \vdots & \vdots & \vdots & \vdots \\ 1 & 1 & \mathrm{Temp}_{56} & \mathrm{Temp}_{56}\end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\  \beta_{0,a} \\  \beta_{1, b} \\  \beta_{1, a} \end{bmatrix} + \sigma \begin{bmatrix} \epsilon_1 \\ \vdots \\  \epsilon_{56} \end{bmatrix}$$


```{r, eval=FALSE, echo=TRUE}
whiteside %>%
  lm(formula = Gas ~ Temp * Insul, data = .)  #<<
```

---

### Linear regression after one-hot encoding of `Temp`

.fl.w-third.pa2[

TODO: `ggPredict`

```{r whitesideonehot, echo=FALSE, warning=FALSE, fig.show='hide', fig.height = 5}
whiteside %>%
  ggplot(mapping=aes(y=Gas,
                     x=Temp,
                     color=Insul,
                     linetype=Insul)) +
  geom_smooth(method="lm", formula = y ~ x, se=FALSE) +
  geom_point(aes(shape=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "One-hot encoding of Insul") +
  theme(legend.position = c(0.1, .15))
```

]

.fl.w-two-thirds.pa2[

![](`r knitr::fig_chunk("whitesideonehot", "png")`)

]


---
class: inverse, middle, center

## About formulae


---


Objects of class `formula` are used in many corners of `R` environment.

In multiple linear regression, formulae are used to define a design starting from the column of a
dataframe


.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

The models fit by, e.g., the `lm` and `glm` functions are specified in a compact symbolic form

The `~` operator is basic in the formation of such models

An expression of the form `y ~ model` is interpreted as a specification that the response `y` is modelled by a linear predictor specified symbolically by `model`

Such a model consists of a series of _terms_ separated by `+` operators

The terms themselves consist of variable and factor names separated by `:` operators

Such a term is interpreted as the interaction of all the variables and factors appearing in the term

.tr[`r fontawesome::fa("r-project")` documentation]

]
---


.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

In addition to `+` and `:`, a number of other operators are useful in model formulae.

  The `*` operator denotes factor crossing: `a*b` interpreted as `a+b+a:b`.

  The `^` operator indicates crossing to the specified degree.

  For example `(a+b+c)^2` is identical to `(a+b+c)*(a+b+c)` which in turn expands to a formula containing the main effects for `a`, `b` and `c` together with their second-order interactions.

.tr[`r fontawesome::fa("r-project")` documentation]

]

---

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

  The `%in%` operator indicates that the terms on its left are nested within those on the right.

  For example `a + b %in% a` expands to the formula `a + a:b`.

  The `-` operator removes the specified terms, so that `(a+b+c)^2 - a:b` is identical to `a + b + c + b:c + a:c`.

  It can also be  used to remove the intercept term: when fitting a linear model `y ~ x - 1` specifies a line through the origin.

  A model with no intercept can be also specified as `y ~ x + 0` or `y ~ 0 + x`.


.tr[`r fontawesome::fa("r-project")` documentation]

]


---

### Formulae with arithmetic expressions

.bg-light-gray.b--light-gray.ba.bw2.br3.shadow-5.ph4.mt5[

The formula `log(y) ~ a + log(x)` is legal.

When such arithmetic expressions involve operators which are also used symbolically in model formulae, there can be confusion between arithmetic and symbolic operator use.

To avoid this confusion, the function `I()` can be used to bracket those portions of a model formula where the operators are used in their arithmetic sense.

For example, in the formula `y ~ a + I(b+c)`, the term `b+c` is to be interpreted as the sum of `b` and `c`.

Variable names can be quoted by backticks `like this` in formulae, although there is no guarantee that all code using formulae will accept such non-syntactic names

.tr[From [`r fontawesome::fa("r-project")` documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html)]

]

---

.fl.w-third.pa2[

Formulae are so useful when modeling that they have been dedicated
class and even a package: `Formula` (Extended Model Formulas)

]

.fl.w-two-thirds.pa2[

```{r, eval=FALSE}
knitr::include_url("https://rdrr.io/cran/Formula/")
```

]


---


By taking into account interactions between `Temp` and `Insul`, we have improved
the appeal of our model. Now we face two questions:

1. can we go further and improve data fitting?

1. can we formally assess the apparent improvement delivered by a richer model?

One question concerns the possibility of describing richer models

The second question is about model assessment/validation/selection

The first question  is about programming, the second takes us in inferential statistics territory

---
class: center, middle, inverse

## Higher degree polynomials

---

### Building an enriched design

.fl.w-two-thirds.pa2[

Rather than searching a _linear connection_
between `Gas` and `Temp` we might
as well look for a _polynomial connection_

In the next chunk, we add powers of `Temp` to dataframe `whiteside`

```{r, echo=TRUE}
poly(whiteside$Temp,
     degree=10,
     raw=TRUE) %>%
 as_tibble() -> whiteside_matrix

names(whiteside_matrix) <- paste("Temp",
                                 1:10,
                                 sep="^")

whiteside_10  <- cbind(whiteside,
                       whiteside_matrix)
```
]

.fl.w-third.pa2[

We fit a degree $10$ polynomial to the data

```{r, echo=TRUE}
lm_10 <-
  whiteside_10 %>%
  lm(formula =
     Gas ~ . - Temp - Insul)
```

We are looking for a predictor of `Gas` based on powers of `Temp`

]
---

### A paradox

.fl.w-third.pa2[

Attempting to fit a degree $10$ polynomial to the `Whiteside` data leads to:

- Goodness of fit is improved

- Plausibility, and thus explanatory power is degraded:
It is hard to envision a non-monotonous relationship between gas consumption and average external temperature

]

.fl.w-two-thirds.pa2[

```{r whitesideten-label, echo=FALSE, warning=FALSE}
whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp)) +
  geom_point(aes(shape=Insul)) +
  geom_point(mapping = aes(y=lm_10$fitted.values), shape="+", size=5) +
  geom_smooth(formula= y ~ 1+ poly(x,10, raw=TRUE), method="lm", se=FALSE) +
  ggtitle('Whiteside data: fitting a degree 10 polynomial',
          'Predictions of gas consumption are not monotonous with respect to average external temperature') +
  theme(legend.position = c(0.1, .15))
```

]



---

### High order modeling with interaction

.fl.w-third.pa2[

We can play that game further by fitting
a high-degree polynomial of `Temperature` with interaction with `Insulation`

 Fitting a degree $10$ polynomial with interaction with `Insul` to the `whitesidedata`.

 We attempt to fit a model with $22$ coefficients (degrees of freedom) to the `whiteside` data ( $56$ observations)

 Goodness of fit is visually improved  but physical plausibility and explanatory power remain bad.

]

.fl.w-two-thirds.pa2[

```{r whiteside10insul, echo=FALSE, warning=FALSE}
whiteside_10 %>%
  lm(formula = Gas ~ (. - Temp) * Insul) -> lm_10_insul

whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp, group=Insul)) +
  geom_point(aes(shape=Insul)) +
  geom_point(mapping = aes(y=lm_10_insul$fitted.values),
             shape="+", size=5) +
  geom_smooth(formula= y ~ 1+ poly(x,10, raw=TRUE),
              method="lm",
              se=FALSE,
              aes(linetype=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "Fitting a degree 10 polynomial with interaction with Insul") +
  theme(legend.position = c(0.1, .15))
```
]

---

### The best of both worlds


.fl.w-third.pa2[

Fit a degree $2$ polynomial with interaction w.r.t. `Insul`

When fitting a degree $2$ polynomial, we face new questions:

- should we prefer quadratic modelling to linear modelling?
- shoud we retain interaction with `Insul` for both subgroups?

Visual inspection does not provides us with clear cut guidelines

]

.fl.w-two-thirds.pa2[

```{r  whiteside2Insul, echo=FALSE, warning=FALSE}
lm_2 <- lm(Gas ~ Insul/(Temp + I(Temp^2))-1, whiteside)

whiteside %>%
  ggplot(mapping=aes(y=Gas, x=Temp, group=Insul)) +
  geom_point(aes(shape=Insul, color=Insul)) +
  geom_point(mapping = aes(y=lm_2$fitted.values), shape="+", size=5) +
  geom_line(mapping = aes(y=lm_2$fitted.values, linetype=Insul)) +
  geom_smooth(formula= y ~ poly(x,2, raw=TRUE),
              method="lm",
              se=FALSE,
              aes(linetype=Insul)) +
  ggtitle("Whiteside data",
          subtitle = "Fitting a degree 2 polynomial per group") +
  theme(legend.position = c(0.1, .15))
```

]

---

### GLM interpretation

Figure  corresponds to least-square fitting of the following model

$$\begin{bmatrix}\mathrm{Gas}_1 \\  \vdots \\  \mathrm{Gas}_{56} \end{bmatrix}  = \begin{bmatrix}  1 & 0 & \mathrm{Temp}_1 &  0 & \mathrm{Temp}^2_1 &  0\\  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\  1 & 0 & \mathrm{Temp}_{26} &  0 & \mathrm{Temp}^2_{26} &  0\\  1 & 1 & \mathrm{Temp}_{27} & \mathrm{Temp}_{27} & \mathrm{Temp}^2_{27} & \mathrm{Temp}^2_{27}  \\  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\   1 & 1 & \mathrm{Temp}_{56} & \mathrm{Temp}_{56} & \mathrm{Temp}^2_{56} & \mathrm{Temp}^2_{56} \end{bmatrix} \times \begin{bmatrix}  \beta_{0,b} \\  \beta_{0,a} \\  \beta_{1, b} \\  \beta_{1, a} \\  \beta_{2, b} \\  \beta_{2, a}\end{bmatrix} + \sigma \begin{bmatrix}  \epsilon_1 \\  \vdots \\  \epsilon_{56}\end{bmatrix}$$

We still assume _homoschedastic_ noise

---
exclude: true

```{r}
require("parsnip")
require(Formula)
```


```{r}
formule <- Formula(Gas ~ Temp * Insul)

lm_mod <-
  linear_reg() %>%
  set_engine("lm")

lm_fit <-
    lm_mod %>%
    fit(formule,
        data = whiteside)
```

```{r}
# model.matrix(lm_2) %>% View()
```

---
exclude:true

```{r }
pacman::p_load(ggfortify)
```

```{r}
autoplot(lm_2, which = 6)
```


---
class: middle, center, inverse

## ANOVA


---

[ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance) stands for ANalysis
Of VAriance.

Its provides us with a method for choosing and assessing a model at
least if we adhere to the _Gaussian Linear Modeling_ credo.


ANOVA is just the tip of a large iceberg.

It has been extended to more general models and
has inspired _model selection methods_ in many areas of statistics and machine learning.


---
class: middle, center, inverse

## Variable selection in Gaussian Linear Models

---



In the sequel of this section, we use simulated data to introduce and motivate
the ANOVA method.

We generate

- a design matrix $Z \in \mathcal{M}_{n,p}$
with $n=1000$ and $p=100$,
- a sparse random vector $\beta \in \mathbb{R}^p$
- a vector of observation $Y = Z \times \beta + \sigma \epsilon$ where
$\sigma=1$ and $\epsilon$ is a standard Gaussian vector of dimension $p$.

---

`r chunk_reveal("my_simul_anova", break_type="auto", display_type="code", title="### Preparing ANOVA")`

```{r my_simul_anova, include = FALSE}
set.seed(1515)   # for reproducibility

p <- 100    # dimension of sample points
n <- 1000   # sample size

sigma <- 1  # noise standard deviation

betas <- matrix(rbinom(p, 1, 0.1) * rnorm(p, mean=1),
                nrow=p, ncol=1)  # true coefficient vector

important_coeffs <- which(abs(betas) >.01) # sparse support

Z <- matrix(rnorm(n*p, 5), ncol = p, nrow=n)  #  design

Y <- Z %*% betas  + matrix(rnorm(n, 0, sd=sigma), nrow=n)

df <- as.data.frame(cbind(Z, Y), )

names(df)  <- c(paste("x", 1:p, sep="_"), "y")
```

---

### True model versus full model

Dataframe `df` fits Gaussian linear modeling assumptions

We may feed it to `lm()`

This leads to what we call the _full model_

We may also take advantage of the fact that we know which columns of $Z$ matters when we want to predict $Y$.

Hence, we can compare the linear model obtained by brute force `lm(y ~ . -1, df)` ( $-1$ because we need no intercept) with the linear model obtained by using columns of `df` corresponding
to non-null coefficients of `betas`

We call the former the _true model_

---

### Building formulae

```{r}
formula_full <- formula("y ~ . - 1")

formula_true <- formula(str_c("y",
                              paste(str_c("x",
                                          important_coeffs,
                                          sep = "_"),
                                     collapse =" + "),
                              sep=" ~ "))

formulae <-  c(formula_full, formula_true)

lm_df <- map(formulae,
             lm,
             data=df)

names(lm_df) <- c("full", "true")
```

---

### The true model

```{r}
formula_true
```

```{r, echo=FALSE}
rsss <- map_dbl(lm_df,
                ~ round(sum(residuals(.)^2), 1))
```

---

### Comparing reconstruction errors

We can compare the true and full models using the quadratic error criterion.

The residuals can be gathered from both models using function `residuals()`.

The Residual Sum of Squares (RSS)  are respectively `r rsss['full']` for the full model and `r rsss['true']`.

At first glance, the full model achieves a better fit to the data than the true model

---

### Comparing estimation errors

`r fontawesome::fa("exclamation-triangle")` This comparison is not fair

The full model is obtained by optimizing the quadratic error over a set which is much larger  than the set used
for the true model

As we are handling simulated data, we can compare the coefficient
estimates in the two models and see whether one comes closer to
the coefficients used to generate the data

The squared $\ell_2$ distances to $\beta$ are substantially different:

$$\| \beta - \widehat{\beta}^{\text{true}}\|^2  = 1.31 \times 10^{-2}$$

while

$$\| \beta - \widehat{\beta}^{\text{full}}\|^2  = 9.75 \times 10^{-2}$$

---

### Overfitting

The fact that the full model outperforms the small model with
respect to the Residual Sum of Squares criterion is righteously called _overfitting_.

```{r, echo=FALSE, include=FALSE}
full_distance = norm(coefficients(lm_df[['full']]) - betas, type = "2")^2

small_distance = norm(coefficients(lm_df[['true']]) - betas[important_coeffs], type = "2")^2
```

---

### Diagnosing the full model

.panelset[

.panel[.panel-name[Code]

```{r qplot-residuals-label, fig.show='hide', fig.height=3.5}
xx <- residuals(lm_df[['full']])
yy <- fitted.values(lm_df[['full']])
qplot(y=yy,
      x=xx,
      alpha=I(.25)) +
  xlab("fitted values") +
  ylab("residuals") +
  ggtitle("Gaussian linear model",
  subtitle = "n=1000, p=100, full model")
```

]

.panel[.panel-name[Residuals versus Fitted plot]

![](`r knitr::fig_chunk("qplot-residuals-label","png")`)

]

]

---

###  


```{r}
head((anova(lm_df[['full']]))  %>%
       broom::tidy(.n), 10)
```

---

```{r, echo=FALSE}
tib <-
  tibble(residuals= residuals(lm_df[['full']]))
m <- mean(tib$residuals)
s <- sd(tib$residuals)
tit <- "Kernel density estimate of residuals distribution (full model)"
```

`r chunk_reveal("kde_residuals", break_type="auto", title="### Residual distribution (full model)")`



```{r kde_residuals, include=FALSE, fig.width=6, cache=F, comment = ""}
tib %>%
  ggplot(aes(x=residuals)) +
  geom_histogram(aes(y=..density..),
                 alpha=.3) +
  stat_density(kernel = "epanechnikov",
               bw = .15,
               alpha=.0,
               col=1) +
  stat_function(fun = dnorm,
                args=list(mean=m,
                          sd=s),
                linetype=2) +
  ggtitle(tit,
          "Dashed curve: Gaussian fit")
```

---

### Assessing residuals

```{r}
shapiro.test(residuals(lm_df[['full']]))
```


```{r}
# (coefficients(lm.1)[1])^2 + norm(betas - coefficients(lm.1)[-1])^2

# norm(betas - lm.0$coefficients)^2
```

---
class: middle, center, inverse

## Using Fisher $F$ statistic

---

### Computing the Fisher $F$ statistic for comparing model 0 and model 1

As we are dealing with simulated data, we can behave as if a benevolent _oracle_
had whispered that all but `97` coefficients are null, and even told us the index
of the non-null coefficients.

We fit this exact and minimal model to the data.

Under the Gaussian linear model assumption, the empirical distribution of residuals is the empirical distribution of the projection of a standard Gaussian vector onto the linear subspace orthogonal to the subspace generated by the design columns.

As such the distribution of residuals is not the empirical distribution of an i.i.d Gaussian sample. Nevertheless, if the linear subspace is not too correlated with the canonical basis, the residuals empirical distribution is not too far from a Gaussian distribution.

---
exclude: true

```{r residdistrib, echo=FALSE, warning=FALSE, fig.cap='(ref:residdistrib)'}
residuals(lm_df[['true']]) %>%
  tibble(residuals= .) %>%
  ggplot(aes(x=residuals)) +
  stat_density(kernel = "epanechnikov", bw = .15, alpha=.0, col=1) +
  stat_function(fun = dnorm,
                mean=mean(residuals(lm_df[['true']])),
                sd=sd(residuals(lm_df[['true']])),
                linetype=2) +
  ggtitle("True model: Kernel density estimate of residuals distribution (true model)",
          "Dashed curve: Gaussian fit")
```

---

### Comparing true model and full model

We are facing a _binary hypotheses problem_.

- Null hypothesis: all coefficients but `x_2 , x_9 , x_22 , x_27 , x_41 , x_49 , x_52 , x_53 , x_59, x_60 , x_77 , x_88 , x_93` are null

- Alternative hypothesis: the vector of coefficients doesnot satisfy the $13$ linear equations defining the null hypothesis

Note that both the null and alternative hypotheses are composite.

---

The null hypothesis asserts that the true vector of coefficients belong to a linear subspace of dimension `13` of $\mathbb{R}^{100}$. 

If we stick to Gaussian linear modelling
we may test this null hypothesis by performing a _Fisher test_.

Under the null hypothesis,

$$\frac{\|\widehat{Y} - \widehat{Y}^\circ\|^2/(p-p^\circ)}{\|{Y} - \widehat{Y}\|^2/(n-p)} \sim F_{p-p^\circ, n-p}$$

where $F_{p-p^\circ, n-p}$ stands for the Fisher distribution with
$p-p^\circ$ and $n-p$ degrees of freedom.

Function `anova`  does this.

---
exclude: true

```{r, tidy.opts=''}
anova(lm_df[['true']], lm_df[['full']]) %>%
  broom::tidy() %>%
  dplyr::mutate_if(is.numeric, round, 2)
```

On the second row, column `statistic` contains the Fisher statistic. The `p.value` tells us the probability that a random variable distributed according to Fisher distribution with $87$ and $900$ degrees of freedom ($F_{87,900}$) exceeds the statistic's value $0.89$. The fact that `p.value` (`pf(.89, 87, 900, lower.tail = FALSE)`) is $0.75$ does not prompt us to reject the null hypothesis.

---

### Handling $p$-values

The rule of thumb when using the `p.value` reported by a test function is the following: 

> If, _under the null hypothesis_, you  accept to be wrong  with probability $\alpha$, 

> then 

> you should reject when the `p.value` is less than $\alpha$

In words: if you are looking for a _level_ (probability of an error of the first kind) equal to $\alpha$, you should reject the null hypothesis when the $p$-value is less than $\alpha$

We can decompose the computations performed by ANOVA

---
exclude: true

```{r, eval=FALSE}
setdiff(important_coeffs, # 2  9 22 27 41 49 52 53 59 60 77 88 93
        which(
(df %>%
  select(starts_with("x")) %>%
  purrr::map_dbl(.f = ~ cor(., df$y)) %>%
  abs() ) > .05)
)
```

```{r, eval=FALSE}
lm.0 %>% summary()
(svd(t(Z) %*% (Z))$d)^(-1)
   # Inverse of covariance matrix of coefficients in lm.0
```

---
exclude: true
class: middle, center, inverse

## Iterative methods for OLS

---
exclude: true
class: middle, center, inverse

## ...

---
exclude: true
class: middle, center, inverse

## Ridge regression


---
exclude: true
### Regularized least squares criterion

For each $\lambda>0$, the penalized least square costs is defined by

$$\Big\Vert Y - Z \beta  \Big\Vert^2 + λ \big\|\beta\big\|^2$$

Gradient with respect to $\beta$

$$2\times\big(λ β + Z^T Z β - Z^T Y\big)$$

Unique optimum

$$\big(\lambda \operatorname{Id} + Z^TZ\big)^{-1} Z^T Y$$

---
exclude: true
### Solution



---
exclude: true
class: middle, center.inverse

## Split/Apply/Combine and (Ridge) regression

---
exclude: true


---
exclude: true
class: middle, center, inverse

## Feature engineering

---
exclude: true
class: middle, center, inverse

##

---
exclude: true
class: middle, center, inverse

##

---
class: center, middle, inverse
background-image: url(img/pexels-cottonbro-3171837.jpg)
background-size: cover

## The End

