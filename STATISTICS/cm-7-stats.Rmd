---
title: "Statistique VII: Tests de type Chi-deux"
subtitle: "⚔<br/>EDA Master I, MIDS & MFA"
author: "Stéphane Boucheron"
institute: "Université de Paris"
date: "2020/01/29 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["header-footer.css",  "xaringan-themer.css", "custom-callout.css"]
    lib_dir: libs
    seal: false
    includes:
      in_header:
        - 'toc.html'
    nature:
      nature:
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
name: layout-general
layout: true
class: left, middle

```{r loaders-fixers, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
xaringanExtra::use_animate_css()

xaringanExtra::use_tile_view()

xaringanExtra::use_tachyons(minified = FALSE)

xaringanExtra::use_logo(
  image_url = "./img/UniversiteParis_logo_horizontal_couleur_RVB.jpg",
  position = xaringanExtra::css_position(top = "1em", right = "1em"),
  width = "110px",
  link_url = "http://master.math.univ-paris-diderot.fr/annee/m1-mi/",
  exclude_class = c("hide_logo")
)

xaringanExtra::use_panelset()

xaringanExtra::use_editable(expires = 1)

source("./loaders_fixers.R")

knitr::opts_chunk$set(fig.width = 6,
                      message = FALSE,
                      warning = FALSE,
                      comment = "",
                      cache = F)
library(flipbookr)
```



<style>
.remark-slide-number {
  position: inherit;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: red;
}
</style>




---
class: middle, center, inverse

# Statistique VII: Tests de type $\chi^2$

### `r Sys.Date()`


#### [Statistique Fondamentale Master I MIDS et MFA](http://stephane-v-boucheron.fr/courses/statistics-paris/)

#### [Stéphane Boucheron](http://stephane-v-boucheron.fr)

---
class: inverse, middle

## `r fontawesome::fa("map", fill="white")`

### [Notations](#notations)


### [Problèmes](#problemes)

### [Chi-deux d'adéquation](#adequation)

### [Puissance du Chi-deux d'adéquation](#)

### [Hypothèse nulle composite](#h0composite)

### [Test d'indépendance](#independance)


---


Dans la leçon  précédente, nous  avons adopté un point de vue abstrait et général sur les tests. Ici, nous revenons sur des constructions très classiques, qui remontent aux débuts de la statistique, à la seconde moitié du $\text{XIX}^{\text{eme}}$ siècle. Il s'agit de répondre à des questions simples: Un échantillon de valeurs issues d'un ensemble fini (les modalités) peut-il raisonnablement avoir été produit par des tirages indépendants selon une loi donnée à l'avance? C'est le problème de l'adéquation _(goodness of fit)_ à une loi fixée. D'une manière plus générale, on veut tester l'adéquation à une loi issue d'un modèle réputé régulier. Cela recouvre en particulier les tests d'indépendance, très utilisés dans les tentatives d'analyse causale. Tous les tests que nous verrons dans cette leçon relèvent de la famille des tests de type chi-deux. Ce sont des tests dont le niveau asymptotique
est garanti. Leur construction passe par la construction d'une suite de vecteurs aléatoires dont la distribution limite est pivotale sous l'hypothèse nulle. Techniquement, tout repose sur le théorème central limite multivarié et la méthode Delta   de Cramer.


---
class: center, middle, inverse
name: notations

## Notations

---

### Modèles multinomiaux

Pour  $p\geq 1$, le modèle multinomial de _dimension_ $p$ est formé par les lois sur $\{0,\ldots,p\}$

Paramétrisation fournie par $\theta \in ]0,1[^p$  vérifiant $\sum_{\alpha =1}^p \theta[\alpha] < 1$ avec les conventions

1. $\mathbf{p}_ \theta(\alpha) = \theta[\alpha] \text{ pour } \alpha>0 \text{ et } \mathbf{p}_ \theta(0) = 1 - \sum_{\alpha>0} \theta[\alpha]$

2. $\theta[0] = 1 - \sum_{\alpha=1}^p \theta[\alpha]$

Un modèle multinomial est un modèle exponentiel minimal

La paramétrisation canonique  serait

$$\left( \log \frac{\theta[\alpha]}{\theta[0]}\right)_{\alpha \in \{1,\ldots, p\}}$$

Pour $\alpha \in \{0, \ldots, p\}$,  $N^n(\alpha)$ désigne le nombre d'occurrences de la _modalité_ $\alpha$ dans l'échantillon de taille $n$

???

Des modèles exponentiels particulièrement simples.

Ces tests sont historiquement importants.

Faciles à calculer, ils ont longtemps constitué la technique de choix pour étudier les données de comptage.

---

### Log-vraisemblance dans les modèles multinomiaux

La log-vraisemblance s'écrit

$$\begin{array}{rl}\ell_n(\theta) & = \sum_{\alpha=0}^p N^n(\alpha) \log \theta[\alpha]\\
& =  \sum_{\alpha=1}^p N^n(\alpha) \log \theta[\alpha] + \left(n -\sum_{\alpha=1}^p N^n(\alpha)\right) \log \left(1 - \sum_{\alpha=1}^p \theta[\alpha] \right)\end{array}$$

La _fonction score_ est  le gradient de la log-vraisemblance par rapport à $\theta$.

On vérifie  que, sous $\mathbf{p}_ \theta$,

$$\frac{1}{n}\operatorname{cov}(N^n(\alpha),N^n(\beta)) = \begin{cases}\mathbf{p}_ \theta(\alpha) -\mathbf{p}_ \theta(\alpha)^2 & \text{si}\qquad \alpha= \beta \\- \mathbf{p}_ \theta(\alpha)\mathbf{p}_ \theta(\beta) & \text{sinon.}\end{cases}$$

---

### Notation ...

Nous noterons $\text{diag}\left( \tfrac{1}{\sqrt{n \mathbf{p}_ \theta}} \right)$ la matrice diagonale
dont les coefficients diagonaux sont les
${1}/{\sqrt{n \mathbf{p}_ \theta(\alpha)}}$ pour $\alpha \in \{0, \ldots p\}$.


$$\frac{1}{n}\operatorname{cov}(N^n) = \operatorname{diag} \left( \mathbf{p}_ \theta \right) - \begin{pmatrix}\mathbf{p}_ \theta(0)\\\vdots \\\mathbf{p}_ \theta(p) \end{pmatrix} \times \begin{pmatrix}\mathbf{p}_ \theta(0),\ldots ,\mathbf{p}_ \theta(p)\end{pmatrix}$$

En multipliant  le vecteur aléatoire
$(N^n- n \mathbf{p}_ \theta)/\sqrt{n}$ par $\text{diag}(1/\sqrt{\mathbf{p}_ \theta})$
on obtient un vecteur aléatoire de matrice de covariance

$$\operatorname{cov}\left(\left(\frac{N^n(\alpha)- n \mathbf{p}_ \theta (\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha \leq p} \right)   = \text{Id}_{p+1} - \begin{pmatrix}
\sqrt{ \mathbf{p}_ \theta(0)}\\\vdots \\
\sqrt{     \mathbf{p}_ \theta(p)}\end{pmatrix} \times \begin{pmatrix}\sqrt{ \mathbf{p}_ \theta(0)},
\ldots ,\sqrt{     \mathbf{p}_ \theta(p)}\end{pmatrix}$$

---

On note

$$\Gamma(\theta) = \text{Id}_{p+1} - \begin{pmatrix} \sqrt{ \mathbf{p}_ \theta(0)}\\\vdots \\ \sqrt{     \mathbf{p}_ \theta(p)}\end{pmatrix} \times \begin{pmatrix}\sqrt{ \mathbf{p}_ \theta(0)},
\ldots ,\sqrt{     \mathbf{p}_ \theta(p)}\end{pmatrix}$$

Il s'agit de la matrice de projection sur le sous-espace orthogonal
à la droite engendrée par

$$\left(\sqrt{ \mathbf{p}_ \theta(\alpha)}\right)_{0 \leq \alpha \leq p}$$

---

### Matrice d'information de Fisher

La matrice
d'_information de Fisher_  est  la _matrice de covariance de la fonction score_ évaluée en $\theta$ sous $\mathbf{p}_ \theta$

La matrice d'_information de Fisher_ est définie par:

$$\begin{array}{rl}I (\theta) & = \mathbb{E}_{\theta} \left[ \nabla \log \mathbf {p}_{\theta} \times \nabla^{t} \log \mathbf {p}_{\theta} \right] \\ & = \left[ \mathbb{E}_{\theta} \left[ \frac{\partial_\alpha
p_{\theta}}{p_{\theta}}  \frac{\partial_{\alpha'} p_{\theta}}{p_{\theta}} \right] \right]_{1\leq \alpha, \alpha' \leq  p}\\ & = \sum_{\alpha=0}^p \frac{\nabla \mathbf {p}_{\theta}(\alpha)}{\sqrt{\mathbf {p}_{\theta}(\alpha)}}\frac{\nabla \mathbf {p}_{\theta}(\alpha)^t}{\sqrt{\mathbf {p}_{\theta}(\alpha)}}\end{array}$$

---

La matrice d'information de Fisher en $\theta$ s'écrit

$$I(\theta) =  \text{diag}  \begin{pmatrix}\frac{1}{\theta[1]} \\ \vdots \\ \frac{1}{\theta[p]} \end{pmatrix} + \frac{1}{\theta[0]} \begin{pmatrix}1 \\\vdots \\ 1 \end{pmatrix} \times \begin{pmatrix} 1 &\vdots & 1 \end{pmatrix}$$

---

La relation entre la matrice d'information et la matrice de covariance est facilement vérifiée

### Formule de  Shermann-Morrison

Soit $\mathbf{A}$  une matrice inversible et $u,v$ deux vecteurs  colonnes tels que

$$1+v^t \mathbf{A}^{-1}u \neq 0$$

Alors

$$(\mathbf{A}+ uv^t)^{-1} = \mathbf{A}^{-1} -  \frac{\mathbf{A}^{-1}u v^t \mathbf{A}^{-1}}{1+ v^t \mathbf{A}^{-1}u}$$

???

Si une perturbation de rang un d'une matrice  inversible est  inversible, alors l'inverse de la  perturbation est une perturbation de rang un de l'inverse

---

La matrice d'information est de rang $p$ et

$$I(\theta)^{-1} =  \frac{1}{n}\operatorname{cov}\left( \left(N^n(\alpha)\right)_{1\leq \alpha\leq p} \right)$$





---
class: center, middle, inverse
name: problemes


## Problèmes

---

### Plusieurs sortes de tests de type $\chi^2$.

- Adéquation à une hypothèse simple

- Homogénéité

- Indépendace

- Symétrie

---
### Définition : Problème d'_adéquation à une hypothèse simple_

Soient

- $P$ une loi de probabilités sur un ensemble fini $\Omega$ et

- $(x_1, \ldots, x_n)$ un échantillon de $n$ points de $\Omega$

`r fontawesome::fa("brain")` L'échantillon $(x_1, \ldots, x_n)$ a-t-il été engendré par $n$
tirages indépendants selon $P$ ?

---


### Exemple: données Geissler

En 1889, le docteur  Geissler étudia les registres  des hôpitaux de
Saxe et nota  le nombre  de garçons dans $6115$ familles de $12$ enfants

.f7[

| Nombre de garçons/12 | Nombre de familles |
|:---------------------:|-------------------:|
|0 | 3|
|1 | 24|
|2 | 104 |
|3 | 286|
|4 | 670 |
|5 | 1033 |
|6 | 1343|
|7 | 1112|
|8 | 829|
|9 | 478 |
|10 | 181 |
|11 | 45|
|12 | 7 |

]


---
exclude: true 

```{r}

```
---

Si les sexes des enfants dans une même  famille sont
le résultat d'épreuves aléatoires indépendantes et identiquement
distribuées,

alors

la loi du nombre de garçons dans une
famille de 12 enfants est  une loi binomiale de paramètres $12$ et $\theta$ où
$\theta \in (0,1)$

Les démographes nous disent que la fréquence des naissances masculines est $52\%$

On peut tester l'adéquation de la distribution du nombre de
garçons à la loi binomiale de paramètres $12$ et $.52$ en utilisant
un test de type chi-deux

???

Pour les trois autres problèmes, l'hypothèse nulle et l'alternative sont
composées.

---

### Définition : _Problème  d'homogénéité_

Soient  $(x_1, \ldots, x_n)$ et $(y_1, \ldots, y_n)$ deux échantillons de $n$ points de $\Omega$

`r fontawesome::fa("brain")` Les deux échantillons ont-ils été engendrés par $n$ tirages
indépendants selon une même loi (inconnue)?


---

### Exemple  `r fontawesome::fa("boat")`

La table  `Titanic`  est une table de contingence, c'est-à-dire
un tableau à entrées multiples (4 ici: `Age, Sex, Class, Survived`) compilé
à partir des archives du célèbre voyage.

Pour chaque personne à bord (2201), on a noté
son `Age` (`Child/Adult`), sa `Class` (`1st,2nd,3rd,Crew`), son `Sex`
(`Male, Female`), son destin (`Survived`: `No, Yes`)

L'archive est un  `data.frame` où  les quatre variables sont catégorielles (ce sont des `factors`)

La table de contingence indique pour chaque combinaison des modalités des variables l'effectif correspondant.

```{r, echo=TRUE, eval=TRUE}
data(Titanic)
Titanic[c("3rd"), c("Male","Female"), "Child",]
```

---

### Exemple  (suite)

Un effort de modélisation (discutable) nous conduit à concevoir le destin des personnes à bord comme la réalisation de tirages pile/face indépendants (c'est naïf compte tenu de  la capacité insuffisante des canaux de sauvetage).

On peut définir une quantité de sous-populations, selon la `Class`, l'`Age`, etc, et se demander
si ces différentes sous-populations ont subi le même aléa. Nous sommes conduits à mener un test d'homogénéité.

---


Le problème du test d'indépendance est le plus commun des tests d'hypothèse composites.

### Définition : _Problème  d'indépendance_

Soit $((x_1, y_1), \ldots, (x_n, y_n))$ un échantillon de $n$ points de $\Omega \times \Omega'$.

`r fontawesome::fa("brain")` L'échantillon a-t-il été engendré par $n$ tirages indépendants
selon une loi produit  $P \otimes P'$ sur $\Omega \times \Omega'$?

---




### Définition : _Problème de symétrie_

Soit $((x_1, y_1), \ldots, (x_n, y_n))$ un
échantillon de $n$ points de $\Omega \times \Omega$.

`r fontawesome::fa("boat")` L'échantillon a-t-il été engendré par $n$ tirages indépendants
selon une loi    $P$ sur $\Omega \times \Omega$,
telle que $P\{A \times B\}= P\{B \times A\}$ pour tous $A, B \subseteq \Omega$ ?

---
name: adequation
class: center, middle, inverse

## Test du $\chi^2$ d'adéquation

---

### Rappel des conventions

Comme dans le reste de cette leçon:

$\Omega$ est un ensemble fini de taille $p+1$ (identifié à $\{0, \ldots, p\}$)

L'hypothèse nulle est définie par $\left(\mathbf{p}_ \theta(\alpha)\right)_{\alpha \leq p}$

La paramétrisation naturelle est fournie par $\theta \in ]0,1[^p$
qui vérifie $\sum_{\alpha =1}^p \theta[\alpha] < 1$

???

On pourrait de fait aborder le problème du test d'adéquation à l'hypothèse simple
définie par $(\theta[\alpha])_{\alpha>0}$ avec les outils développés en général pour les modèles exponentiels,
notamment avec les régions de confiance inspirées par le phénomène de Wilks.


Nous allons reprendre l'approche suivie par Pearson au début du vingtième siècle.

---

### Définition :statistique du $\chi^2$  de Pearson

Pour $\alpha \in \{0, \ldots, p\}$, on note $N^n(\alpha)$ le nombre d'occurrences de la modalité $\alpha$
dans l'échantillon de taille $n$.

La statistique de Pearson est définie par

$$\sum_{\alpha =0}^{p} \frac{(N^n (\alpha) - n \mathbf{p}_\theta(\alpha))^2}{n \mathbf{p}_\theta(\alpha)} =  \left\|  \left(\frac{N^n(\alpha)- n \mathbf{p}_ \theta (\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha \leq p}\right\|^2$$

On note cette statistique $C_n(\theta)$.

---

### Interprétation


Si on note

$O_ \alpha:= N^n(\alpha)$ (O comme _observed_),

et

$E_ \alpha:= n \mathbf{p}_\theta(\alpha)$ (E comme _expected_),

cette statistique se présente comme le carré de la norme du vecteur aléatoire

$$\left(\frac{O_ \alpha- E_ \alpha}{\sqrt{E_ \alpha}} \right)_{\alpha \in \Omega}$$

une forme que nous reverrons à propos des $\chi^2$ d'adéquation à des hypothèses composites.

---

### Statistique de Pearson comme divergence d'information


Nous avons déjà mentionné

$$\chi^2(P \mid Q):= \int \frac{\mathrm{d} Q}{\mathrm{d} \nu} \left(\frac{\mathrm{d} P}{\mathrm{d} Q}  - 1 \right) ^2 \mathrm{d} \nu  =  \int \frac{\mathrm{d} P}{\mathrm{d} \nu} \left(\frac{\mathrm{d} P}{\mathrm{d} Q}  -1 \right) \mathrm{d} \nu$$

Si on note $P_n$ la loi empirique définie par l'échantillon et $P$
la loi définie par $\theta$, la statistique de Pearson s'écrit aussi
$$n \chi^2 \left( P_n \mid P \right)$$

---

### `r fontawesome::fa("brain")`

Vérifier que

$$D(P \Vert Q) \leq \chi^2(P \mid Q)$$

Montrer que pour certains couples de lois de probabilité,
la divergence du $\chi^2$ peut étre infinie alors que l'entropie
relative est finie.

---

### TCL en action (décor)

$$\frac{1}{n}\operatorname{cov}(N^n,N^n) = \operatorname{diag} \left( \mathbf{p}_ \theta \right) - \begin{pmatrix} \mathbf{p}_ \theta(0)\\\vdots \\ \mathbf{p}_ \theta(p)\end{pmatrix} \times \begin{pmatrix}\mathbf{p}_ \theta(0),\ldots ,\mathbf{p}_ \theta(p)\end{pmatrix} = \Gamma(\theta)$$

Le vecteur aléatoire $N^n$ est une somme de vecteurs aléatoires indépendants et identiquement distribués:

Notons $X_i$ la variable aléatoire à valeur dans $\{0, \ldots, p\}$
qui vaut $\alpha$ si le $i^{\text{eme}}$ élément de
l'échantillon vaut $\alpha$, pour chaque modalité $\alpha \in \{1, \ldots, p\}$

$$N^n(\alpha) = \sum_{i=1}^n \mathbb{I}_{X_i = \alpha}$$

---

### TCL en action

Le théorème central limite vectoriel nous indique alors que sous l'hypothèse nulle

$$\left(\frac{N^n(\alpha)- n \mathbf{p}_ \theta (\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha \leq p} \rightsquigarrow \mathcal{N}\left( 0, \Gamma(\theta)\right)$$

où  $\Gamma(\theta)$  est définie dans la section d'introduction.

---

### TCL en action (suite)

Comme la matrice $\Gamma(\theta)$  est la matrice de projection sur le sous-espace orthogonal
à la droite engendrée par $(\sqrt{ \mathbf{p}_ \theta(\alpha)})_{0 \leq \alpha \leq p}$, on peut conclure avec

- le théorème de Cochran

- le principe de l'image continue

que

> sous l'hypothèse nulle, la loi limite de la statistique de Pearson est $\chi^2_p$

---

### Une méthode de test de niveau asymptotique garanti

Pour atteindre le niveau $\eta \in ]0,1[$,

on compare la statistique de Pearson $C_n(\theta)$ au quantile $q_{p,1-\eta}$ d'ordre $1- \eta$ de la loi $\chi^2_p.$

Si $C_n(\theta)>q_{p, 1-\eta}$, on rejette l'hypothèse nulle

---

### Exemple : retour sur les données Geissler

On effectue un test d'adéquation à une hypothèse simple:  la probabilité
d'observer une famille comportant $\alpha$ garçons est

$$\mathbf{p}_ \theta(\alpha) =   \binom{12}{\alpha}  \theta^{\alpha} (1-\theta)^{12- \alpha}$$

pour $\alpha \in \{0,\ldots,12\}$

On note $O_ \alpha$  le nombre de familles comportant $\alpha$ garçons
parmi 12 recensées par le  docteur Geissler.

On note $E_ \alpha =n \mathbf{p}_ \theta(\alpha)$ l'espérance du nombre de familles c
omportant $\alpha$ garçons parmi 12 dans un échantillon i.i.d. de $n=6115$
observations.

---

### Exemple : retour sur les données Geissler (suite)

La statistique de Pearson est

$$\sum_{\alpha=0}^{12} \frac{(O_ \alpha-E_ \alpha)^2}{E_ \alpha}$$

```
chisq.test(saxony[,2],p=dbinom(c(0:12),12,prob=.52))

Chi-squared test for given probabilities

data:  saxony[, 2]
X-squared = 110.53, df = 12, p-value < 2.2e-16

Warning message:
In chisq.test(saxony[, 2], p = dbinom(c(0:12), 12, prob = 0.52)):
Chi-squared approximation may be incorrect
```

Nous reviendrons plus loin sur l'avertissement.

---

### Exemple : retour sur les données Geissler (suite)

La comparaison de la statistique de Pearson `X-squared`
aux quantiles de la loi $\chi^2_{12}$ (chi-deux à `df` $=12$ degrés de liberté)

conduit à

_rejeter l'hypothèse nulle_

> nombre de garçons dans une famille de 12 enfants distribué selon une binomiale de paramètres $12$ et $.52$

si on a choisi un _niveau_ supérieur à $2.2 \times 10^{-16}$.

---

L'usage pratique des tests passe par la compréhension de la notion de $p$-valeur.

### Définition : $p$-value, $p$-valeur, Degré de signification atteint

Soit un problème de tests binaires où l'hypothèse nulle est simple et définie par la loi $P$

On considère les  tests qui comparent une  statistique $S$ à un seuil et  rejettent  l'hypothèse nulle lorsque ce seuil est dépassé (le seuil définit le niveau)

La $p$-valeur associée à une réalisation $s$ de $S$  est
définie par  $\overline{F}(s) = 1 - F(s)$ où $F$ est la
_fonction de répartition de la loi de la statistique_ $S$ _sous l'hypothèse nulle_

---

### Proposition


.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Sous une hypothèse nulle simple,  si la loi de la statistique $S$ est diffuse,

alors,

la loi de la $p$-valeur  est  uniforme sur $[0,1]$

]



---

### Preuve

La proposition est le corollaire immédiat de résultat classique de calcul des probabilités:

Si $X \sim F$ avec $F$ continue, alors $F(X)$ est uniformément distribuée sur $[0,1]$

`r fontawesome::fa("square")`




---

### Remarquue

Lorsqu'on utilise une fonction de test dans un environment de calcul statistique,
le niveau n'est en général pas décidé d'avance.

Le fait de rapporter systématiquement la valeur de la statistique de test et la $p$-valeur associée, permet de reculer la décision.

Lorsqu'on veut tester au niveau $\eta$, il n'est meme pas nécéssaire de comparer la statistique de test au quantile d'ordre $1-\eta$ de la loi de la statistique sous l'hypthèse nulle, il suffit de comparer la $p$-valeur à $\eta$.

Si cette $p$-valeur est plus petite que $\eta$, on rejette l'hypothèse nulle.

Cela garantit un test de niveau $\eta$.



---
class: center, middle, inverse

## Puissance du test du $\chi^2$

---

Dans la mesure où  l'alternative comprend toutes les lois sur $\Omega$, l'étude de la puissance du
test du $\chi^2$  d'adéquation peut sembler un défi. L'étude asymptotique la rend relativement simple.


---

### Alternative fixe

Si on choisit $\theta'\neq \theta$, pour un $\alpha \in \{0, \ldots, p\}$ au moins
$\mathbf{p}_ \theta(\alpha)\neq \mathbf{p}_ {\theta'}(\alpha)$.

Sous la loi définie par
$\mathbf{p}_ {\theta'}$, $\frac{N^n(\alpha)}{n}$  converge presque sûrement vers $\mathbf{p}_ {\theta'}(\alpha)$
et donc

$$\left( \frac{N^n(\alpha)- n \mathbf{p}_ \theta(\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}}\right)^2 = n \left( \frac{N^n(\alpha)/n-  \mathbf{p}_ \theta(\alpha)}{\sqrt{ \mathbf{p}_ \theta(\alpha)}}\right)^2\stackrel{P}{\longrightarrow}  + \infty$$

Quelque soit le niveau fixé, quelque soit le point de l'alternative, la probabilité d'erreur de seconde espèce tend vers $0$.

Cette observation est rassurante mais pas très informative.

---

### Suite d'alternatives contigue


Pour apprécier les performances de la statistique du $\chi^2$ d'adéquation,
il est bon de considérer non pas une alternative fixe, mais une suite d'alternatives qui se rapprochent de l'hypothèse nulle.

Dans la suite $h \in \mathbb{R}^{p+1}$ vérifie $\sum_{\alpha=0}^p h[\alpha]=0$. Pour $n$ assez grand

$$\mathbf{p}_{\theta_n}:=   \mathbf{p}_\theta + \frac{h}{\sqrt{n}}$$

définit bien une probabilité $P_{\theta_n}$ sur $\{0, \ldots, p\}$.

La divergence du $\chi^2$ entre $P_{\theta_n}$ et $P_  \theta$ vaut

$$\chi^2(P_{\theta_n}\mid P_  \theta) =  \frac{1}{n}\sum_{\alpha=0}^p \frac{h[ \alpha]^2}{\theta[\alpha]}
=  \frac{1}{n}  h^t I(\theta) h$$

---

### Abus de notations  `r fontawesome::fa("exclamation-triangle")`

Dans la dernière expression nous abusons des notations:

- $I(\theta)$ est une matrice de dimension $p$ par $p$,

- $h$ est introduit comme un vecteur colonne indicé entre $0$ et $p$

Il faut comprendre

$$h^t I(\theta) h = \sum_{1\leq \alpha, \beta\leq p} I(\theta)[\alpha, \beta] h(\alpha)h(\beta)$$

---

### Quantifier la vitesse d'approximation de $P_  \theta$ par  $P_{\theta_n}$

La divergence du $\chi^2$ permet de quantifier la vitesse à laquelle $P_{\theta_n}$ s'approche de  $P_  \theta$ dans la direction définie par $h$

On peut aussi calculer un équivalent de la distance de Hellinger entre $P_{\theta_n}$ et $P_  \theta$:

$$\lim_{n \to \infty} n  H(P_ {\theta_n}, P_\theta)^2  = \frac{1}{4} \lim_n n \chi^2(P_{\theta_n}\mid P_  \theta)$$

???

La limite des lois  images de $P_{\theta_n}^{\otimes n}$ par les statistiques de Pearson
$n \chi^2(P_{\theta_n} \mid P_ \theta)$ se détermine.

---

### Loi du $\chi^2$ décentrée



---

### Proposition

---

### Domination stochastique


---

### Proposition


.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Sous $(P^{\otimes n}_{\theta_n})_n$,  la limite des lois de la statistique de Pearson $C_n(\theta)$
est la loi du $\chi^2$ à $p$ degrés de liberté décentrée,  de paramètre de décentrage

$$h^t I(\theta) h =  \sum_{\alpha=0}^{p} h^2_\alpha/\theta_\alpha = n \chi^2(P_{\theta_n} \mid P_\theta)$$

Cette loi est notée $\chi^2_p(h^t I(\theta)h)$

]

---

### Preuve

Comme

$$\left(\frac{N^n(\alpha) -n \mathbf{p}_ \theta(\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha}=  \left(\frac{N^n(\alpha) -n \mathbf{p}_ {\theta_n} (\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha} + \left( \frac{h(\alpha)}{\sqrt{\mathbf{p}_ \theta(\alpha)}}\right)_{\alpha}$$

pour établir le résultat, il suffit d'établir que sous la suite $(P_{\theta_n}^{\otimes n})$,

$$\left(\frac{N^n(\alpha) -n \mathbf{p}_ {\theta_n} (\alpha)}{\sqrt{n \mathbf{p}_ \theta(\alpha)}} \right)_{\alpha} \rightsquigarrow \mathcal{N}(0, \Gamma(\theta))$$

---

### Preuve (suite)

Le mécanisme de Cramer-Wold et le lemme de Slutsky
nous indiquent qu'il suffit de vérifier cette convergence pour les formes linéaires appliquées aux vecteurs aléatoires

$$\frac{N^n -n \mathbf{p}_{\theta_n}}{\sqrt{n \mathbf{p}_ {\theta_n}}}:=  \left(\frac{N^n(\alpha) -n \mathbf{p}_ {\theta_n} (\alpha)}{\sqrt{n \mathbf{p}_ {\theta_n}(\alpha)}} \right)_{\alpha}$$

Soit $\lambda \in \mathbb{R}^p$, on note $\sigma_n^2:=  \lambda^t \Gamma(\theta_n) \lambda$ et  $\sigma^2:= \lambda^t \Gamma(\theta) \lambda$, soit $\lim_n \sigma^2_n = \sigma$.

---

### Preuve (suite)

On définit

$$S_n:=  \left\langle \lambda , \frac{N^n -n \mathbf{p}_{\theta_n}}{\sqrt{n \mathbf{p}_ {\theta_n}}}  \right\rangle$$

Pour chaque $n$, $S_n$  est une somme de variables aléatoires indépendantes identiquement distribuées, centrées. La variance de $S_n$  est $\sigma^2_n$. La distribution des variables aléatoires
dont la somme centrée et normalisée donne $S_n$  dépend de $n.$ On dit qu'on a affaire à un __tableau triangulaire__

Le théorème Central Limite de Lindeberg-Feller est justement valable pour les tableaux triangulaires.

---

### Preuve (suite)


On  vérifie facilement que les conditions sont remplies dans le cas des vecteurs multinomiaux


Par le théorème de Berry-Esseen:
$$\sup_{x \in \mathbb{R}} \left| \mathbb{P} \left\{S_n/\sigma_n \leq x \right\} - \Phi(x)\right| \leq \frac{\mathbb{E}[|X_1|^3]}{\sqrt{n} \sigma_n^3}$$
ou encore

$$\sup_{x \in \mathbb{R}} \left| \mathbb{P} \left\{S_n \leq x \right\} - \Phi(x/\sigma_n)\right| \leq \frac{\mathbb{E}[|X_1|^3]}{\sqrt{n} \sigma_n^3}$$

Comme
$$\sup_{x \in \mathbb{R}} \left| \Phi(x/\sigma) - \Phi(x/\sigma_n)\right| \leq \left(\frac{\sigma\vee \sigma_n}{\sigma \wedge \sigma_n} -1\right) M$$
pour une constante universelle $M \leq \sup_{x>0} x \phi(x)$, on peut conclure que
$$\lim_n  \sup_{x \in \mathbb{R}} \left| \mathbb{P} \left\{S_n \leq x \right\} - \Phi(x/\sigma)\right| =0$$
soit la convergence en loi désirée.

`r fontawesome::fa("square")`


---

Lors de l'étude du test de Fisher en régression linéaire gaussienne, nous avons vu que la loi du $\chi^2$  décentrée domine stochastiquement la loi du $\chi^2$ centrée. Nous pouvons donc conclure que sous la suite $P_{\theta_n}$
le test du $\chi^2$ de niveau $\alpha$  est sous la suite d'alternatives contigue $(\theta_n)_n$,  de puissance asymptotique supérieure ou égale à $\alpha$ (on dit que le test est _sans biais_).

On peut regarder cette étude de la puissance asymptotique du test du $\chi^2$ sur une suite l'alternatives contigue dans la perspective du théorème.

Ce théorème nous indique que pour tout seuil $\tau>0$

$$\max\left(P_ \theta^{\otimes n}\{ C(n,\mathbf{p}_ \theta) \geq \tau\}, P_ {\theta_n}^{\otimes n}\{ C(n,\mathbf{p}_ \theta) < \tau\} \right) \geq \frac{1}{4} - \frac{1}{2} n H(P_ \theta, P_{\theta_n})^2$$

---

Si $F_p$ (resp. $F_{p,\delta}$) désigne la fonction de répartition de la loi $\chi^2_p$ (resp. $\chi^2_p(\delta)$), en choisissant

$$\delta = \sum_{\alpha=0}^p h(\alpha)^2/\mathbf{p}_\theta(\alpha) =  h^t I(\theta) h$$

(on choisit bien sûr $h$  de façon à avoir $\delta<2$),

la limite du membre gauche est

$$\max(\eta, F_{p,\delta}(F_p^{\leftarrow}(1- \eta)))$$

alors que celle du membre droit est $\tfrac{1}{4}-\tfrac{\delta}{8}$.

Pour $\delta=1$, $p=12$, le minimum du membre gauche et atteint autour de $\eta\approx .45$  et il est proche de $.45$.

L'écart avec la minoration ($1/8$) ne permet pas de déclarer que le test du $\chi^2$ est asymptotiquement optimal.

Ce calcul suscite une question:  peut-on concevoir un test d'adéquation à $P_ \theta$
qui soit asymptotiquement de niveau $\eta$ et pour toute suite $(\theta_n)_n$ d'alternatives contigue
vérifiant $n \chi^2(P_{\theta_n}\mid P_{\theta})\to \delta$,
de niveau asymptotique supérieur à $F_{p,\delta}(F_p^{\leftarrow}(1- \eta)))$ ?

???

% Pour essayer de prouver que au moins asymptotiquement, le test du $\chi^2$ est optimal lorsqu'on considère une suite d'alternatives contigue, il faudrait  prouver que les tests du $\chi^2$ de niveau $\alpha$ se comportent asymptotiquement comme les tests de rapport de vraisemblance de même niveau.

---
class: center, middle, inverse
name: h0composite

## Hypothèse nulle composite



---

### Modifications de la statistique de Pearson

Notons  $\Theta_0$ un ouvert de $\mathbb{R}^r$ (avec $r < p$), on
supposera dans la suite qu'il existe une fonction deux fois (continuement)
différentiable de $\Theta_0$ dans l'ensemble des fonctions de masse de probabilité sur
$\{0, \ldots, p \}$. On notera encore $\mathbf {p}_{\theta}$ la (fonction de masse de)  probabilité associée
à $\theta \in \Theta_0$.

On peut généraliser le problème du test
d'ajustement à la situation suivante: si
l'échantillon $x_1, \ldots, x_n$ a été tiré selon une loi \ $P$ inconnue sur
$\{0, \ldots, p \}$, tester

-   $H_0$:   $\exists \theta \in \Theta_0$ tel que la loi  $P$  est définie par $\mathbf {p}_{\theta}$

-  $H_1$:   $\not{\exists} \theta \in \Theta_0,  P \text{ est définie par } \mathbf {p}_{\theta}$.

---


On pourra vérifier que les trois situations données en introduction
(homogénéité, indépendance, symétrie) rentrent dans ce cadre.
Si on considère $\Theta_0$ comme un sous-modèle et qu'on dispose
d'un (d'une suite d') estimateur(s) $\widehat{\theta}$ pour ce modèle, on peut
aussi procéder de la façon suivante.

---

### Définition

On note $N^n(\alpha)$
le nombre d'occurrences de $\alpha \in \{0, \ldots, p\}$ \ dans
l'échantillon ( $N^n_i \equiv \sum_{t = 1}^n \mathbb{1}_{x_t = i}$ )

Le test du $\chi^2$ d'adéquation à $\Theta_0 \subset \mathbb{R}^r$ consiste
à comparer la statistique de Pearson modifiée:

$$C (n, \mathbf {p}_{\widehat{\theta}}) \equiv \sum_{\alpha = 0}^p \left( \frac{N^n(\alpha)- n\mathbf {p}_{\widehat{\theta}}
(\alpha)}{\sqrt{n\mathbf {p}_{\widehat{\theta}} (\alpha)}}\right)^2$$

où $\widehat{\theta}$ est un estimateur pour le modèle
$\Theta_0$,  à un seuil $\tau > 0$, à rejeter si $C (n, \Theta_0)$
est supérieur à ce seuil.

---



### Exemple : retour sur les données Geissler


Le nombre moyen de garçons dans les familles de l'échantillon $0.519$ est à peine
différent de celui qu'on apprend dans les livres ( $.52$ ), l'écart
est en tous cas plus petit que l'écart-type de la moyenne empirique de
$73 380$ épreuves de Bernoulli de paramètre  $.52$.

Si on soumet les données `saxony` au test d'adéquation du $\chi^2$ calculé par la fonction
`chisq.test`, on obtient le résultat suivant.

```
chisq.test(saxony[,2],p=dbinom(c(0:12),12,prob=hattheta))

Chi-squared test for given probabilities

data:  saxony[, 2]
X-squared = 110.5, df = 12, p-value < 2.2e-16

Warning message:
In chisq.test(saxony[, 2], p = dbinom(c(0:12), 12, prob = hattheta)):
Chi-squared approximation may be incorrect
```

---

La statistique `X-squared`  est bien la statistique de Pearson modifiée, puisque
le paramètre `p`  est défini à partir de $\widehat{\theta}_n\approx .519$ calculé sur les données.

Cette statistique n'est pas comparée au quantile de la loi $\chi^2_{11}$ mais au quantile de la loi $\chi^2_{12}$.

La fonction `chisq.test` ne peut tenir compte du fait que les paramètres sont estimés.


Le message d'avertissement est lié au fait que pour $\alpha\leq 1$ et $\alpha\geq 11,$ les effectifs
attendus
$E_ \alpha$ sont inférieurs à $5,$ dans ce cas
$$(N^n_ \alpha - n \mathbf{p}_{\widehat{\theta}_n}(\alpha))/\sqrt{n \mathbf{p}_{\widehat{\theta}_n}}$$
n'est pas approximativement normale et le raisonnement asymptotique qui justifie
la comparaison de la statistique de Pearson aux quantiles de la loi $\chi^2_{11}$ n'est pas tenable

---

On regroupe les familles comportant
$0$ et $1$ garçons, ainsi que les familles
comportant $11$ et $12$ garçons. On compare à nouveau les fréquences
observées et les probabilités attendues en utilisant la loi
binomiale de paramètres $12$ et $.5192...$

```
chisq.test(Saxonygrp,p=dbingrp)

Chi-squared test for given probabilities

data:  Saxonygrp
X-squared = 105.8463, df = 10, p-value < 2.2e-16
```

---


Si l'hypothèse binomiale est correcte, asymptotiquement,
cette statistique se comporte selon une loi   $\chi^2_{9}$.
La probabilité qu'une variable $\chi^2_9$ distribuée atteigne la valeur $105.8$ est
inférieure à la précision de la machine.

Si on a choisi un niveau supérieur à $10^{-16}$, on est conduit à rejeter
l'hypothèse nulle (dans une même famille,
les sexes des enfants sont indépendamment et
identiquement distribués selon une loi de Bernoulli dont la probabilité est un propriété de l'espèce).

---

### Spéculations

.panelset[

.panel[.panel-name[Légende]

Les triangles  représentent les proportions observées dans les données du docteur Geissler, les disques représentent les proportions attendues dans un échantillon de  réalisations indépendantes de la loi binomiale de paramètres $12$ et $.519$ (ajusté).  On note que les familles équilibrées sont sous-représentées dans l'échantillon du Dr. Geissler.

Cette visualisation complète l'interprétation de la statistique du $\chi^2$.

L'hypothèse d'une distribution binomiale du nombre de garçons dans la progéniture est rejetée. On pourrait tester d'autre hypothèses.

L'une des plus simples postule que nous avons affaire à un _mélange_ de deux types de famille.

Dans chaque type de famille, le nombre de garçons est distribué selon une binomiale, et le paramètre de succès de la binomiale dépend du type de la famille.

Ce modèle est défini par trois paramètres:  les paramètres de succès des deux binomiales et les proportions du mélange.

Ce n'est pas un modèle exponentiel.
]

.panel[.panel-name[Graphique]


 <!-- \begin{figure}
 \centering
 \includegraphics[width=.9\textwidth]{TP-CM7_files/figure-latex/RplotBinomAjuste.png}
  \caption{

 }
 \end{figure} -->

]
]

---

## Loi limite de la statistique de Pearson modifiée

---

Dans cette section, nous caractérisons la loi limite de
$C (n,\mathbf {p}_{\widehat{\theta}})$ sous l'hypothèse nulle lorsque $\Theta_0$ est
un modèle   régulier et $\widehat{\theta}$ un estimateur  du
maximum de vraisemblance

Pour établir que cette loi limite est une loi du
$\chi^2$ dont le nombre de degrés de liberté est égal à la
différence entre $p$ et la  _dimension_  de $\Theta_0$ (soit $r$ si
$\Theta_0$ est un ouvert de $\mathbb{R}^r$),

Nous allons rappelerquelques outils probabilistes

---

### Hypothèses techniques: modèles multinomiaux réguliers

On suppose que:

- L'espace des paramètres $\Theta \subset \mathbb{R}^r$ est ouvert,

- la fonction de $\Theta_0 \to ]0,1[^{p+1}$,
$\theta \mapsto \mathbf {p}_{\theta}$ est deux fois continument différentiable en $\theta$.

- la matrice d'information de Fisher  est encore définie par:
$$I (\theta) \equiv \mathbb{E}_{\theta} \left[ \nabla \log \mathbf {p}_{\theta} \times \nabla^{t} \log \mathbf {p}_{\theta} \right] = \left[ \mathbb{E}_{\theta} \left[ \frac{\partial_i p_{\theta}}{p_{\theta}}  \frac{\partial_{i'} p_{\theta}}{p_{\theta}} \right] \right]_{i, i' \leq  r} = \sum_{\alpha=0}^p \frac{\nabla \mathbf {p}_{\theta}(\alpha)}{\sqrt{\mathbf {p}_{\theta}(\alpha)}}\frac{\nabla \mathbf {p}_{\theta}(\alpha)^t}{\sqrt{\mathbf {p}_{\theta}(\alpha)}}$$

Elle est supposée inversible, donc  de rang $r$,  en tout $\theta \in \Theta_0$

- La fonction $\theta \mapsto I(\theta)$ est supposée continue


---

On peut définir une matrice $B (\theta)$  de dimensions $r \times (p+1)$ et
factoriser  la matrice d'information $I (\theta)$:

$$B (\theta):=  \left(\begin{array}{c} \frac{\partial_j \mathbf {p}_{\theta} (\alpha)}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}} \end{array}\right)_{\scriptsize {\begin{array}{l}1 \leq  j \leq  r\\ 0 \leq  \alpha \leq  p \end{array}}}   \text{ et }  I (\theta):= B(\theta) \times B(\theta)^t$$

La colonne de $B(\theta)$ indexée par $\alpha$
est  $\tfrac{1}{\sqrt{\mathbf{p}_\theta(\alpha)}} \nabla \mathbf{p}_{\theta}(\alpha)$.

On utilisera plus tard le fait que
$$B(\theta) \times \begin{pmatrix} \sqrt{\mathbf{p}_\theta}(0)\\\vdots \\ \sqrt{\mathbf{p}_\theta}(p) \end{pmatrix} =  \left( \sum_{\alpha=0}^p \partial_j \mathbf {p}_{\theta} (\alpha) \right)_{1 \leq j \leq r} =0$$

---

### Remarque

On a aussi:

$$I (\theta) = \left(\begin{array}{c} - \mathbb{E}_{\theta} \left[ \partial_{j, j'} \log \mathbf{p}_{\theta} \right] \end{array}\right)_{j, j' \leq  r}    =  - \mathbb{E}\left[ \nabla^2 \log \mathbf{p}_\theta \right]$$

Les coefficients de la matrice d'information peuvent en effet  s'écrire

$$- \sum_{\alpha= 0}^p \frac{\partial_{j, j'} \mathbf{p}_{\theta} \{\alpha\} \times \mathbf{p}_{\theta} \{\alpha\}- \partial_j \mathbf{p}_{\theta} \{\alpha\} \times \partial_{j'} \mathbf{p}_{\theta}\{\alpha\}}{\mathbf{p}_{\theta} \{\alpha\}} \,   \quad \text{ pour } 1 \leq  j, j'\leq  r$$

La possibilité d'écrire l'information de Fisher comme l'espérance du Hessien de la log-vraisemblance est
une propriété de certains modèles statistiques, pas de tous, elle est notamment vérifiée dans les modèles exponentiels.

L'information de Fisher est définie comme la covariance sous $P_ \theta$ du vecteur score évalué en $\theta$.


---

Rappelons que la matrice $(p+1 )\times (p+1)$, $\Gamma (\theta)$ est définie par

$$\Gamma (\theta):= \text{Id}_{p+1} - \sqrt{\mathbf {p}_{\theta}} \sqrt{\mathbf {p}_{\theta}}^t$$

C'est la matrice de la projection orthogonale sur le sous-espace orthogonal
à la droite engendrée par le vecteur unitaire $\sqrt{\mathbf {p}_{\theta}}$.

Nous notons $\text{diag}\left( \tfrac{1}{\sqrt{n \mathbf{p}_ \theta}} \right)$ la matrice diagonale
dont les coefficients diagonaux sont les $\tfrac{1}{\sqrt{n \mathbf{p}_ \theta(\alpha)}}$ pour $\alpha \in \{0, \ldots p\}$.

---

On note

$$Z^n:= \left( \frac{N^n(\alpha) - n\mathbf {p}_{\theta} \{\alpha\}}{\sqrt{n\mathbf {p}_{\theta} \{\alpha\}}} \right)_{\alpha \leq  p} =  \text{diag}\left( \tfrac{1}{\sqrt{n \mathbf{p}_ \theta}} \right) \times \left(N^n - n \mathbf {p}_{\theta}\right)$$

Dans la suite $\ell_n (\theta')$ désigne la log-vraisemblance de
l'échantillon en $\theta' \in \Theta_0$

$$\ell_n (\theta') \equiv \sum_{l = 1}^n \log \mathbf {p}_{\theta'} \{x_l \}$$

pour chaque $j \leq  r$,

$$\partial_j \ell_n (\theta') = \sum_{l = 1}^n  \frac{\partial_j\mathbf {p}_{\theta'} \{x_l \}}{\mathbf {p}_{\theta'} \{x_l \}}$$

La fonction score est définie comme le gradient de la log-vraisemblance:

$$\nabla \ell_n (\theta') = (\partial_j \ell_n (\theta'))_{j \leq  r}$$

c'est un vecteur colonne.

---

On définit la matrice symétrique $A(\theta)$  par

$$A (\theta):= \left(\begin{array}{ccc}\Gamma (\theta) & \vdots & B(\theta)^{t } I^{- 1} (\theta)\\\ldots & \ldots & \ldots\\I^{- 1} (\theta) B (\theta) & \vdots & I^{- 1} (\theta)\end{array}\right)$$

où $\Gamma (\theta)$ est défini par ... et $B (\theta)$ et $I (\theta)$ sont
définis par (\ref{def:B}).

On note la factorisation

$$A(\theta) = \left(\begin{array}{c}\text{Id}_{p+1}\\\ldots\\I^{- 1} (\theta) B (\theta)\end{array}\right) \times \Gamma(\theta) \times \begin{pmatrix}\text{Id}_{p+1} &\vdots &B (\theta)^t I^{- 1} (\theta)\end{pmatrix}$$

qui se vérifie en n'oubliant pas que 
$\Gamma(\theta) B(\theta)^t I^{-1}(\theta) = B(\theta)^t I^{-1}(\theta)$

---

En effet

$$\begin{array}{rl}\Gamma(\theta) B(\theta)^t & = B(\theta)^t -  \begin{pmatrix} \sqrt{ \mathbf{p}_ \theta(0)}\\ \vdots \\ \sqrt{     \mathbf{p}_ \theta(p)}\end{pmatrix} \times \begin{pmatrix} \sqrt{ \mathbf{p}_ \theta(0)}, \ldots , \sqrt{     \mathbf{p}_ \theta(p)} \end{pmatrix} \times B(\theta)^t \\ & = B(\theta)^t\end{array}$$

---

Le résultat essentiel sur le comportement asymptotique de la statistique
de Pearson (étendue) est donné par

### Théorème

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Si $\Theta_0 \subset \mathbb{R}^r$ définit un
modèle multinomial régulier et si $\hat{\theta}$ est un estimateur
(consistant) au maximum de vraisemblance

alors

pour tout $\theta \in \Theta_0$, sous $\mathbf {p}_{\theta}$

$$C (n, \mathbf {p}_{\hat{\theta}}) \rightsquigarrow X \hspace{1em}  \text{où}\quad  X \sim \chi^2_{p - r}$$

]

---

La preuve du Théorème s'appuie sur le lemme suivant qui implique au
passage la normalité asymptotique de l'estimateur du maximum de
vraisemblance dans le modèle $\Theta_0$.


---

### Lemme

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Sous les conditions du théorème \ref{the:lim:chi2},
pour tout $\theta \in \Theta_0$, sous $\mathbf {p}_{\theta}$,

$$I (\theta) \sqrt{n} ( \widehat{\theta}_n - \theta) - \frac{1}{\sqrt{n}} \nabla \ell_n (\theta) \stackrel{\mathbf{p}_{\theta}}{\longrightarrow}0$$

]

---

### ...

 Ce couplage asymptotique entre $I(\theta)^{-1} \frac{1}{\sqrt{n}}\nabla \ell_n(\theta)$
 et $\sqrt{n} ( \widehat{\theta}_n - \theta)$ n'est pas une propriété spécifique des modèles
 multinomiaux réguliers mais des modèles dits réguliers en général.

 Dans le cadre des modèles exponentiels, cette convergence en probabilité a déjà été mise à profit pour étudier les méthodes d'estimation _à un pas_.

 Ce couplage  peut être établi
 dans des conditions beaucoup moins exigeantes que celles énoncées ici.
---

### Lemme (intermédiaire)

.bg-light-gray.b--dark-gray.ba.bw1.br3.shadow-5.ph4.mt5[

Sous les conditions du théorème ...
pour tout $\theta \in \Theta_0$, sous $\mathbf {p}_{\theta}$,
la suite de vecteurs aléatoires de dimension $p +  1 + r$

$$X^n:= \begin{pmatrix} Z^n  \\  \ldots \\  \sqrt{n}  \left(\hat{\theta}_n - \theta \right)\end{pmatrix}$$

converge en loi vers un vecteur gaussien centré de matrice de covariance  $A(\theta)$.

]

---

La démonstration du Lemme  commence par vérifier que le score
(le gradient de la log-vraisemblance $\ell_n (.)$) en $\theta$ est une
transformation linéaire de $Z^n$.


---

### Preuve (lemme intermédiaire)

Le théorème central limite vectoriel  nous assure que la suite des vecteurs $(Z_n)$
converge en loi vers
$\mathcal{N}(0,\Gamma (\theta))$

On vérifie d'abord que la fonction score est une transformation linéaire de $Z_n$.

$$\begin{array}{rl} \frac{1}{\sqrt{n}} \nabla \ell_n (\theta) & =  \frac{1}{\sqrt{n}} \sum_{\alpha = 0}^p \,  N^n(\alpha)  \frac{\nabla \mathbf {p}_{\theta} \{\alpha\}}{\mathbf {p}_{\theta} \{\alpha\}}\\ &  \qquad \text{comme } 0 = \sum_{\alpha = 0}^{p} \nabla \, \mathbf {p}_{\theta} \{\alpha\},\\ & =  \sum_{\alpha = 0}^p  \frac{1}{\sqrt{n} }  \frac{(N^n(\alpha) -
n\mathbf {p}_{\theta} \{\alpha\})}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}} \times \frac{\nabla \mathbf {p}_{\theta} \{\alpha\}}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}}  \,  \\ & =  B (\theta) \times Z^n \end{array}$$

---

### Preuve  (lemme intermédiaire)

Passons au  _couplage_   

Par définition du maximum de
vraisemblance
$\nabla \ell_n ( \hat{\theta}) = 0$,
soit $0 = \partial_i \ell_n (\hat{\theta})$ pour $i \leq r$.

On utilise la différentiabilité
de la log-vraisemblance et un développement autour de $\theta$ par la formule de Taylor avec reste intégral.

---

### Preuve  (lemme intermédiaire)
Pour tout $i \leq  r$,

$$\begin{array}{rl} 0 & =  \partial_i \ell_n ( \widehat{\theta}_n) = \partial_i \ell_n (\theta) +
\sum_{j = 1}^r ( \widehat{\theta}_n [j] - \theta [j]) \int_0^1 \,  \,\partial_{j, i} \ell_n (\theta + u ( \widehat{\theta}_n - \theta)) \mathrm{d} u\\&   \qquad \text{soit en divisant les 2 membres par } 1 / \sqrt{n}:\\ 0 & =  \sum_{\alpha = 0}^p \frac{\partial_i \mathbf {p}_{\theta} \{\alpha \}}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}}  \frac{(N^n(\alpha) - n\mathbf {p}_{\theta} \{\alpha\})}{\sqrt{n\mathbf {p}_{\theta} \{\alpha\}}} + \sum_{j = 1}^r \sqrt{n} ( \widehat{\theta}_n [j] - \theta [j]) \frac{1}{n} \int_0^1 \,  \,  \,  \partial_{j, i} \ell_n (\theta + u (\widehat{\theta}_n - \theta)) \mathrm{d} u\end{array}$$

---

### Preuve  (lemme intermédiaire)
Vérifions maintenant que la matrice aléatoire de terme général

$$\left( \frac{1}{n} \int_0^1 \,  \,  \,  \partial_{i, j}\ell_n (\theta + u ( \widehat{\theta}_n - \theta)) \mathrm{d} u \right)$$

converge en probabilité vers $- I (\theta)$ dont le terme général est
aussi la limite en probabilité de

$$\frac{1}{n} \partial_{i, j} \ell_n (\theta) = \frac{1}{n} \int_0^1\,  \,  \,  \partial_{i, j} \ell_n (\theta) \mathrm{d} u$$

Nous voulons établir

$$\frac{1}{n} \int_0^1 \,  \,  \,  \left[ \partial_{i, j} \ell_n (\theta + u ( \widehat{\theta}_n - \theta)) - \partial_{i, j} \ell_n (\theta) \right] \mathrm{d} u \stackrel{P}{\longrightarrow}0$$

---
### Preuve  (lemme intermédiaire)
Notons

$$w (\delta) \equiv \sup_{\theta': \theta - \theta' \| \leq \delta} \sup_{i, j \leq  r} \sup_{\alpha} | \partial_{i, j} \log \mathbf {p}_{\theta'} \{\alpha\}- \partial_{i, j} \log \mathbf {p}_{\theta} \{\alpha\}|.$$

Cette quantité est une fonction continue de $\delta$ et tend vers
$0$ lorsque $\delta$ tend vers $0.$

On a par ailleurs

$$\left| \frac{1}{n} \int_0^1 \,  \,  \,  \left[\partial_{i, j} \ell_n (\theta + u ( \widehat{\theta}_n - \theta)) - \partial_{i, j} \ell_n (\theta) \right] \mathrm{d} u \right| \leq  w\left( \| \theta - \widehat{\theta}_n \| \right)$$

---

### Preuve  (lemme intermédiaire)

L'hypothèse de consistance de l'estimateur $\widehat{\theta}_n$
et l'hypothèse de continuité de la matrice d'information de Fisher  permettent  alors
d'établir la convergence en probabilité recherchée.

Par ailleurs la loi des grands nombres permet de conclure que

$$\frac{1}{n} \partial_{i, j} \ell_n (\theta) \begin{array}{c} P\\ \longrightarrow\end{array} \mathbb{E}_{\theta} \left[ \partial_{i, j} \log\mathbf {p}_{\theta} \right]$$

`r fontawesome::fa("square")`




---

### Preuve

D'après le lemme intermédiaire, la limite en loi de 
la suite $(X^n)_{n \in \mathbb{N}}$ coïncide  avec la limite en loi
des images des vecteurs $Z^n$ par la transformation linéaire définie par

$$\left(\begin{array}{c} \text{Id}_{p+1}\\\ldots\\ I^{- 1} (\theta) B (\theta) \end{array}\right)$$

La limite est   un vecteur gaussien centré. La  covariance de cette limite est donnée par

$$\left(\begin{array}{c}\text{Id}_{p+1}\\\ldots\\ I^{- 1} (\theta) B (\theta)\end{array}\right) \times \Gamma (\theta) \times \left(\begin{array}{ccc} \text{Id}_{p+1}  & \vdots & B(\theta)^t I^{- 1} (\theta) \end{array}\right) =  A(\theta)$$


`r fontawesome::fa("square")`




Nous pouvons passer à la démonstration du théorème.

---

### Preuve

En $\theta$, la différentielle de l'application

$$\theta' \mapsto \left(\frac{\mathbf {p}_{\theta'} \{\alpha\}}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}}\right)_{\alpha \leq  p}$$

vaut exactement $B (\theta)$

---

### Preuve (suite)

.f6[
En invoquant à nouveau le principe du Delta et le lemme 

$$\sqrt{n} \left( \frac{\frac{N^n(\alpha)}{n} -\mathbf {p}_{\theta} \{\alpha\}}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}}, \frac{(\mathbf {p}_{\theta} \{\alpha\}-\mathbf {p}_{\widehat{\theta}_n} \{\alpha\})}{\sqrt{\mathbf {p}_{\theta} \{\alpha\}}} \right)_{\alpha \leq  p}$$

converge en loi vers un vecteur Gaussien de matrice de covariance

$$\left(\begin{array}{cccc} \Gamma (\theta) & \vdots & B^{t } (\theta) I^{- 1} (\theta) B^{} (\theta) & \\ \ldots &  & \ldots & \\ B(\theta)^t I^{- 1} (\theta) B^{} (\theta) & \vdots & B(\theta)^t I^{- 1} (\theta) B^{} (\theta) & \end{array}\right) \\    = \left(\begin{array}{ccc} \text{Id}_k & \vdots & B^{t } (\theta) \end{array}\right) \left(\begin{array}{cccc} \Gamma (\theta) & \vdots & B(\theta)^t I^{- 1} (\theta) & \\ \ldots &  & \ldots & \\ I^{- 1} (\theta) B^{} (\theta) & \vdots & I^{- 1} (\theta) & \end{array}\right) \left(\begin{array}{c}\text{Id}_k\\  \ldots\\ B^{} (\theta)\end{array}\right)$$
]
---

### Preuve (suite)

.f6[
On déduit que la suite de vecteurs aléatoires $(Y^n)_{n \in \mathbb{N}}$

$$Y^n:= \sqrt{n} \left( \frac{\frac{N^n(\alpha)}{n} -\mathbf{p}_{\theta} \{\alpha\}}{\sqrt{\mathbf{p}_{\theta} \{\alpha\}}} - \frac{(\mathbf{p}_{\theta} \{\alpha\}-\mathbf{p}_{\widehat{\theta}_n} \{\alpha\})}{\sqrt{\mathbf{p}_{\theta} \{\alpha\}}} \right)_{\alpha \leq p}$$

converge en loi vers un vecteur Gaussien de matrice de covariance

$$\begin{array}{rl} & \Gamma (\theta) - B(\theta)^t I^{- 1} (\theta) B^{} (\theta)\\ & = \mathrm{Id} - \left(\begin{array}{c}\sqrt{p_1}\\\sqrt{p_2}\\\vdots\\ \sqrt{p_k}\end{array}\right) \left(\begin{array}{cccc} \sqrt{p_1} & \sqrt{p_2} & \ldots & \sqrt{p_k}\end{array}\right) - B(\theta)^t \left( B (\theta) \times B(\theta)^t \right)^{- 1} B^{} (\theta)\end{array}$$
]


---

### Preuve (suite)

La matrice symétrique

$$D (\theta) \equiv B(\theta)^t \left( B (\theta) \times B(\theta)^t \right)^{- 1} B^{} (\theta) = B(\theta)^t I^{- 1} (\theta) B^{} (\theta)$$

est idempotente.

C'est une matrice de projection orthogonale de rang $r$.

De plus, le vecteur

$$\left(\begin{array}{cccc}\sqrt{p_{\theta} \{1\}} & \sqrt{p_{\theta} \{2\}} & \ldots & \sqrt{p_{\theta} \{k\}} \end{array}\right)^{t }$$

appartient à son noyau.

---

### Preuve (suite)

Sous $\mathbf{p}_{\theta}$, les limites en loi de

$$C (n, \mathbf{p}_{\hat{\theta}}) \equiv \sum_{\alpha = 0}^p  \left(\frac{\sqrt{n} (N^n(\alpha) / n -\mathbf{p}_{\widehat{\theta}_n}\{\alpha\})}{\sqrt{\mathbf{p}_{\widehat{\theta}_n} \{\alpha\}}} \,  \right)^2$$

et de

$$\sum_{\alpha = 0}^p  \left( \frac{\sqrt{n} (N^n(\alpha) / n -\mathbf{p}_{\widehat{\theta}_n} \{\alpha\})}{\sqrt{\mathbf{p}_{\theta} \{\alpha\}}}\right)^2$$

sont identiques.

La dernière quantité s'écrit

$$\sum_{\alpha = 0}^p \,  \left( \frac{\sqrt{n} (N^n(\alpha) / n -\mathbf{p}_{\theta} \{\alpha\})}{\sqrt{\mathbf{p}_{\theta} \{\alpha\}}} \,  - \frac{\sqrt{n} (\mathbf{p}_{\theta}\{\alpha\}-\mathbf{p}_{\widehat{\theta}_n} \{\alpha\})}{\sqrt{\mathbf{p}_{\theta}\{\alpha\}}} \right)^2,$$

c'est le carré de la norme euclidienne de $Y^n$.

---

### Preuve (suite)


Du principe de l'image continue, on déduit que la suite $(\left\| Y^n \right\|_2^2)_n$ converge
en loi vers la loi du carré de la norme du vecteur Gaussien de matrice de
covariance $\Gamma(\theta)- D (\theta)$.

D'après le Théorème de Cochran, le carré de
cette norme suit la loi d'une somme pondérées de variables distribuées
selon la loi $\chi^2_1 .$

Les valeurs propres de $\Gamma(\theta) - D (\theta)$ sont $1$ (multiplicité $p - r$) et $0$ (multiplicité $r+1$)

`r fontawesome::fa("square")`


---
class: center, middle, inverse
name: independance

## Le test d'indépendance


---

On dispose d'un échantillon de couples d'éléments de

$\Omega' \times \Omega''$ (deux ensembles finis de cardinalités $k'+1$ et $k''+1$):

$( \omega'_1, \omega''_1), \ldots, ( \omega'_n, \omega''_n)$.

Cet échantillon a été obtenu en réalisant $n$ tirages indépendants selon une loi $P$ inconnue sur $\Omega' \times \Omega''$. L'hypothèse d'indépendance s'écrit:  _la loi jointe est le produit de ses deux marginales_

L'ensemble des lois qui sont le produit de leur marginales est un petit sous-ensemble de l'ensemble des lois sur
$\Omega' \times \Omega''$. Ce sous-ensemble peut être paramétrisé par un sous-ensemble de dimension $k'+k''$ alors que pour décrire toutes les lois sur $\Omega' \times\Omega''$, on doit utiliser un ensemble de dimension $(k'+1)\times(k''+1) - 1$.

On vérifie simplement que les conditions qui précèdent l'énoncé du Théorème \ref{the:lim:chi2} sont
remplies.  Dans le cas du test d'indépendance,
l'espace $\Theta_0$ \ est formé par le produit d'un ouvert de
$\mathbb{R}^{k'}$ par un ouvert de $\mathbb{R}^{k''}$.

---

 Si on note

$$\theta:=( \theta', \theta'' ) \text{ avec } \theta' \in \mathbb{R}^{k'}, \theta'' \in \mathbb{R}^{k''}$$

un élément de
$\Theta_0$ on a
$$\begin{array}{rl} \mathbf{p}_{\theta} \{\langle \alpha', \alpha'' \rangle\}= \left\{ \begin{array}{l}\theta' [\alpha'] \times \theta'' [\alpha''] \quad \text{ si } \quad 0 < \alpha' \leq k' \text{ et } 0 <\alpha'' \leq  k''\\ \theta' [\alpha'] \times \left( 1 - \sum_{\beta''=1}^{k''} \theta^{''} [\beta''] \,\right) \quad \text{ si } \quad 0 <\alpha' \leq  k' \text{ et } \alpha'' = 0\\\left( 1 - \sum_{\beta'=1}^{ k'} \theta' [\beta']  \right) \times \theta'' [\alpha''] \quad \text{ si } \quad \alpha' = 0 \text{ et } 0 < \alpha'' \leq  k''\\\left( 1 - \sum_{\beta'=1}^{k'} \theta' [\beta']  \right) \times \left( 1 -\sum_{\beta''=1}^{k''} \theta'' [\beta'']  \right) \quad \text{ si } \alpha' = 0
\text{ et } \alpha'' = 0\end{array} \right. \quad &  &\end{array}$$

---

L'estimateur au maximum de vraisemblance est  simple.

On note

- $N_{\alpha', \alpha''}$ le nombre d'occurrences du couple
$(\alpha', \alpha'') \in \Omega \times \Omega'$ dans l'échantillon
- $N_{\alpha', \cdot} = \sum_{\alpha'' \leq  k''} N_{\alpha',\alpha''}$
- $N_{\cdot, \alpha''} = \sum_{\alpha' \leq  k'} N_{\alpha', \alpha''}$

Le maximum de vraisemblance dans le sous-modèle $\Theta_0$ est atteint en
$()\hat{\theta}', \hat{\theta}'' )$, où $\mathbf{p}_{\hat{\theta}'} [\alpha'] = N_{\alpha', \cdot}/ n$
et $\mathbf{p}_{\hat{\theta}''} [\alpha''] = N_{\cdot, \alpha''} / n$

La statistique de Pearson modifiée s'écrit alors

$$\sum_{\alpha'= 0}^{k'}\sum_{\alpha'' = 0}^{k''}  \frac{\left( N_{\alpha', \alpha''} - N_{\alpha', \cdot} N_{\cdot, \alpha''} / n
\right)^2}{N_{\alpha', \cdot} N_{\cdot, \alpha''} / n}$$

Le théorème \ref{the:lim:chi2} implique que sous l'hypothèse nulle, la statistique du $\chi^2$ d'indépendance
converge en distribution vers une loi du $\chi^2$  à $((k'+1) (k''+1) -1 )- (k'  + k'') = k' \times k''$ degrés
de liberté.



---
class: center, middle, inverse
background-image: url(img/pexels-cottonbro-3171837.jpg)
background-size: cover

## The End
